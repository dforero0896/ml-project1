{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-26T09:36:43.706674Z",
     "start_time": "2019-10-26T09:36:43.362056Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "from proj1_helpers import *\n",
    "from implementations import *\n",
    "from tests import *\n",
    "from preprocessing import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline of the pipeline\n",
    "## Import data\n",
    "+ Import raw data. Split original train data into out test and train sets.\n",
    "## Preprocessing\n",
    "+ The file `preprocessing.py` is imported and contains functions to clean (impute with mean), remove columns, standardize and do PCA.\n",
    "+ Preprocess train and test data separately (you can define the number of principal components used with the max_comp parameter. Defaults to 30).\n",
    "## Apply Model\n",
    "+ Apply your preferred model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-26T09:37:13.868059Z",
     "start_time": "2019-10-26T09:36:46.863694Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read train_y=LABELS, train_x=FEATURES and train_id=EVENT_IDS from dataset.\n",
    "subsamp = False\n",
    "y, x, id_ = load_csv_data('../data/train.csv', sub_sample=subsamp)\n",
    "y_out_test, x_out_test, id_out_test = load_csv_data('../data/test.csv', sub_sample=subsamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-26T17:36:52.883215Z",
     "start_time": "2019-10-26T17:36:48.855392Z"
    }
   },
   "outputs": [],
   "source": [
    "features = np.loadtxt('../data/train.csv', dtype=str, delimiter=',')[0,2:]\n",
    "def show_PC_explicit(features):\n",
    "    import sympy as sy\n",
    "    features_sym = [sy.symbols(f, real=True, positive=True) for f in features]\n",
    "    display(np.array(features_sym).dot(np.around(transform_train, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-26T17:37:16.248561Z",
     "start_time": "2019-10-26T17:37:15.919016Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 30) (200000, 30)\n",
      "['DER_deltaeta_jet_jet' 'DER_mass_jet_jet' 'DER_prodeta_jet_jet'\n",
      " 'DER_lep_eta_centrality' 'PRI_jet_subleading_pt' 'PRI_jet_subleading_eta'\n",
      " 'PRI_jet_subleading_phi']\n"
     ]
    }
   ],
   "source": [
    "clean = True\n",
    "dopca = False\n",
    "remove_cols = False\n",
    "cols = (4, 5, 6, 12, 26, 27, 28)\n",
    "#cols=(0, 4, 5, 6, 12, 23, 24, 25, 26, 27, 28)\n",
    "max_comp = 30  # For cleaning, and no removing cols\n",
    "\n",
    "x_train, y_train, x_test, y_test = split_data(x, y, ratio=0.80, seed=42)\n",
    "y_train, x_train, x_train_mean, x_train_var, transform_train, eigenvals_train = preprocess(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    clean=clean,\n",
    "    dopca=dopca,\n",
    "    max_comp=max_comp,\n",
    "    remove_cols=remove_cols,\n",
    "    cols=cols)\n",
    "y_test, x_test, x_test_mean, x_test_var, transform_test, eigenvals_test = preprocess(\n",
    "    x_test,\n",
    "    y_test,\n",
    "    clean=clean,\n",
    "    dopca=dopca,\n",
    "    max_comp=max_comp,\n",
    "    remove_cols=remove_cols,\n",
    "    cols=cols)\n",
    "print(x_test.shape, x_train.shape)\n",
    "print(features[list(cols)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-26T13:01:02.377823Z",
     "start_time": "2019-10-26T13:00:53.002915Z"
    }
   },
   "outputs": [],
   "source": [
    "degree = 10\n",
    "# Build data matrix with feature expansion\n",
    "tx_train = build_poly(x_train, degree)\n",
    "tx_test = build_poly(x_test, degree)\n",
    "tx_train.shape, tx_test.shape\n",
    "tx_train[:,1:], _, _ = standardize_features(tx_train[:,1:])\n",
    "tx_test[:,1:], _, _ = standardize_features(tx_test[:,1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-26T12:05:11.315942Z",
     "start_time": "2019-10-26T12:02:41.845465Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD (0/4999): loss=0.5\n",
      "GD (100/4999): loss=0.442184935171644\n",
      "GD (200/4999): loss=0.4115490059588939\n",
      "GD (300/4999): loss=0.3919753385151225\n",
      "GD (400/4999): loss=0.37818181437789616\n",
      "GD (500/4999): loss=0.367904721804925\n",
      "GD (600/4999): loss=0.3599855858896922\n",
      "GD (700/4999): loss=0.35375000664114153\n",
      "GD (800/4999): loss=0.34876542821949175\n",
      "GD (900/4999): loss=0.34473459550394064\n",
      "GD (1000/4999): loss=0.3414435679355703\n",
      "GD (1100/4999): loss=0.3387336326074574\n",
      "GD (1200/4999): loss=0.336484633077038\n",
      "GD (1300/4999): loss=0.334604284110886\n",
      "GD (1400/4999): loss=0.3330209257506864\n",
      "GD (1500/4999): loss=0.3316784146817885\n",
      "GD (1600/4999): loss=0.3305324257396797\n",
      "GD (1700/4999): loss=0.3295477241393855\n",
      "GD (1800/4999): loss=0.3286961258209597\n",
      "GD (1900/4999): loss=0.3279549557166036\n",
      "GD (2000/4999): loss=0.3273058717984965\n",
      "GD (2100/4999): loss=0.3267339610171813\n",
      "GD (2200/4999): loss=0.32622703931531677\n",
      "GD (2300/4999): loss=0.3257751061109137\n",
      "GD (2400/4999): loss=0.3253699165865967\n",
      "GD (2500/4999): loss=0.3250046444454194\n",
      "GD (2600/4999): loss=0.3246736145846888\n",
      "GD (2700/4999): loss=0.32437209013143337\n",
      "GD (2800/4999): loss=0.32409610198326266\n",
      "GD (2900/4999): loss=0.32384231176140266\n",
      "GD (3000/4999): loss=0.3236079011602948\n",
      "GD (3100/4999): loss=0.32339048225056594\n",
      "GD (3200/4999): loss=0.32318802448958484\n",
      "GD (3300/4999): loss=0.3229987951110255\n",
      "GD (3400/4999): loss=0.3228213102713424\n",
      "GD (3500/4999): loss=0.32265429487816577\n",
      "GD (3600/4999): loss=0.3224966494514439\n",
      "GD (3700/4999): loss=0.32234742270120953\n",
      "GD (3800/4999): loss=0.3222057887675323\n",
      "GD (3900/4999): loss=0.3220710282747506\n",
      "GD (4000/4999): loss=0.3219425125157545\n",
      "GD (4100/4999): loss=0.32181969021233303\n",
      "GD (4200/4999): loss=0.3217020764016165\n",
      "GD (4300/4999): loss=0.3215892430820294\n",
      "GD (4400/4999): loss=0.32148081131923556\n",
      "GD (4500/4999): loss=0.32137644456666664\n",
      "GD (4600/4999): loss=0.3212758429990345\n",
      "GD (4700/4999): loss=0.3211787386927782\n",
      "GD (4800/4999): loss=0.3210848915163441\n",
      "GD (4900/4999): loss=0.3209940856168225\n",
      "Accuracy ratio = 0.768\n",
      "Test loss = 0.320\n",
      "Train loss = 0.321\n"
     ]
    }
   ],
   "source": [
    "w_init = np.array([0] * tx_train.shape[1])\n",
    "max_iter_gd = 5000\n",
    "gamma_gd = 1e-3\n",
    "w_gd, loss_gd = least_squares_GD(y_train,\n",
    "                                 tx_train,\n",
    "                                 w_init,\n",
    "                                 max_iter_gd,\n",
    "                                 gamma_gd,\n",
    "                                 pr=True,\n",
    "                                 adapt_gamma=False,\n",
    "                                 kind='mse',\n",
    "                                accel=False)\n",
    "gd_prediction = predict_labels(w_gd, tx_test)\n",
    "acc_gd = accuracy_ratio(gd_prediction, y_test)\n",
    "print('Accuracy ratio = %.3f' % acc_gd)\n",
    "print('Test loss = %.3f' % compute_loss(y_test, tx_test, w_gd))\n",
    "print('Train loss = %.3f' % loss_gd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T12:37:03.486835Z",
     "start_time": "2019-10-17T12:37:03.480662Z"
    },
    "hidden": true
   },
   "source": [
    "np.savetxt('../data/w_gd_acc.dat', w_gd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T11:00:36.111308Z",
     "start_time": "2019-10-25T10:57:02.385613Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy ratio = 0.64\n",
      "Test loss = 4.88e-01\n",
      "Train loss = 4.89e-01\n"
     ]
    }
   ],
   "source": [
    "w_init = np.array([0] * tx_train.shape[1])\n",
    "max_iter_sgd = 500\n",
    "gamma_sgd = 1e-5\n",
    "batch_size = 1\n",
    "\n",
    "w_sgd, loss_sgd = least_squares_SGD(y_train,\n",
    "                                    tx_train,\n",
    "                                    w_init,\n",
    "                                    batch_size,\n",
    "                                    max_iter_sgd,\n",
    "                                    gamma_sgd,\n",
    "                                    pr=False,\n",
    "                                    adapt_gamma=False,\n",
    "                                    choose_best=True)\n",
    "sgd_prediction = predict_labels(w_sgd, tx_test)\n",
    "acc_sgd = accuracy_ratio(sgd_prediction, y_test)\n",
    "print('Accuracy ratio = %.2f' % acc_sgd)\n",
    "print('Test loss = %.2e' % compute_loss(y_test, tx_test, w_sgd))\n",
    "print('Train loss = %.2e' % loss_sgd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-26T12:59:19.383028Z",
     "start_time": "2019-10-26T12:59:18.841458Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy ratio = 0.43\n",
      "Train loss = 0.28\n",
      "Test loss = 4.04e+08\n"
     ]
    }
   ],
   "source": [
    "w_lsq, loss_lsq = least_squares(y_train, tx_train)\n",
    "lsq_prediction = predict_labels(w_lsq, tx_test)\n",
    "acc_lsq = accuracy_ratio(lsq_prediction, y_test)\n",
    "print('Accuracy ratio = %.2f' % acc_lsq)\n",
    "print('Train loss = %.2f' % loss_lsq)\n",
    "print('Test loss = %.2e' % compute_loss(y_test, tx_test, w_lsq))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T12:45:30.271461Z",
     "start_time": "2019-10-17T12:45:30.267909Z"
    },
    "hidden": true
   },
   "source": [
    "np.savetxt('../data/w_lsq.dat', w_lsq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-26T13:01:26.829390Z",
     "start_time": "2019-10-26T13:01:26.275323Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy ratio = 0.784\n",
      "Test loss = 0.315\n",
      "Train loss = 0.300\n"
     ]
    }
   ],
   "source": [
    "lambda_rr = 2.7e-3\n",
    "w_rr, loss_rr = ridge_regression(y_train, tx_train, lambda_rr)\n",
    "rr_prediction = predict_labels(w_rr, tx_test)\n",
    "acc_rr = accuracy_ratio(rr_prediction, y_test)\n",
    "print('Accuracy ratio = %.3f'%acc_rr)\n",
    "print('Test loss = %.3f'%compute_loss(y_test, tx_test, w_rr))\n",
    "print('Train loss = %.3f'%loss_rr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T15:18:22.322364Z",
     "start_time": "2019-10-18T15:18:22.309384Z"
    }
   },
   "source": [
    "np.savetxt('../data/w_rr.dat', w_rr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-26T11:56:38.822096Z",
     "start_time": "2019-10-26T11:54:59.065654Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy ratio = 0.710\n",
      "Test loss = 28862.970\n",
      "Train loss = 115577.099\n"
     ]
    }
   ],
   "source": [
    "y_train_log = minus_one_2_zero(y_train)\n",
    "y_test_log = minus_one_2_zero(y_test)\n",
    "\n",
    "\n",
    "w_init = np.array([0] * tx_train.shape[1])\n",
    "max_iter_lrgd = 500\n",
    "gamma_lrgd = 1e-8\n",
    "w_lrgd, loss_lrgd = logistic_regression(y_train_log,\n",
    "                                        tx_train,\n",
    "                                        w_init,\n",
    "                                        max_iter_lrgd,\n",
    "                                        gamma_lrgd,\n",
    "                                        pr=False,\n",
    "                                        adapt_gamma=False,\n",
    "                                       accel=False)\n",
    "\n",
    "lrgd_prediction = predict_labels(w_lrgd, tx_test)\n",
    "acc_lrgd = accuracy_ratio(lrgd_prediction, y_test)\n",
    "\n",
    "print('Accuracy ratio = %.3f' % acc_lrgd)\n",
    "print('Test loss = %.3f' % compute_loss_logistic(y_test_log, tx_test, w_lrgd))\n",
    "print('Train loss = %.3f' % loss_lrgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-26T11:59:41.181605Z",
     "start_time": "2019-10-26T11:57:50.778667Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Regularized Logistic Regression GD (0/499): loss=138629.43611198905\n",
      " Regularized Logistic Regression GD (100/499): loss=127908.06070553078\n",
      " Regularized Logistic Regression GD (200/499): loss=123004.92086239882\n",
      " Regularized Logistic Regression GD (300/499): loss=120019.13028727792\n",
      " Regularized Logistic Regression GD (400/499): loss=117907.72498582836\n",
      "Accuracy ratio = 0.710\n",
      "Test loss = 28927.939\n",
      "Train loss = 116290.899\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "from implementations import *\n",
    "lambda_rlrgd = 1e4\n",
    "gamma_rlrgd = 1e-8\n",
    "max_iter_rlrgd = 500\n",
    "\n",
    "w_rlrgd, loss_rlrgd = reg_logistic_regression(y_train_log,\n",
    "                                              tx_train,\n",
    "                                              lambda_rlrgd,\n",
    "                                              w_init,\n",
    "                                              max_iter_rlrgd,\n",
    "                                              gamma_rlrgd,\n",
    "                                              pr=True,\n",
    "                                              adapt_gamma=False, \n",
    "                                              accel=False)\n",
    "rlrgd_prediction = predict_labels(w_rlrgd, tx_test)\n",
    "acc_rlrgd = accuracy_ratio(rlrgd_prediction, y_test)\n",
    "print('Accuracy ratio = %.3f' % acc_rlrgd)\n",
    "print('Test loss = %.3f' % compute_loss_logistic(y_test_log, tx_test, w_rlrgd))\n",
    "print('Train loss = %.3f' % loss_rlrgd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-26T17:14:58.887285Z",
     "start_time": "2019-10-26T17:12:05.237035Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using method ridge_regression\n",
      "Using lambda = 1.0e-04\n",
      "Using lambda = 3.6e-04\n",
      "Using lambda = 1.3e-03\n",
      "Using lambda = 4.6e-03\n",
      "Using lambda = 1.7e-02\n",
      "Using lambda = 6.0e-02\n",
      "Using lambda = 2.2e-01\n",
      "Using lambda = 7.7e-01\n",
      "Using lambda = 2.8e+00\n",
      "Using lambda = 1.0e+01\n",
      "Best lambda from error: 2.15e-01\n",
      "Best lambda from accuracy: 1.67e-02\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "from tests import *\n",
    "from implementations import *\n",
    "clean = True\n",
    "dopca = False\n",
    "remove_cols = False\n",
    "stdafter = False\n",
    "cols = (4, 5, 6, 12, 26, 27, 28)\n",
    "cols=range(16, 30)\n",
    "max_comp = 30  # For cleaning, and no removing cols\n",
    "cross_validation_demo(x,\n",
    "                      y,\n",
    "                      ridge_regression,\n",
    "                      args_rr,\n",
    "                      k_fold=4,\n",
    "                      degree=10,\n",
    "                      clean=clean,\n",
    "                      dopca=dopca,\n",
    "                      remove_cols=remove_cols,\n",
    "                      lambda_min = -4,\n",
    "                      lambda_max = 1,\n",
    "                      stdafter=stdafter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-26T15:11:31.355165Z",
     "start_time": "2019-10-26T15:10:33.065349Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(568238, 30) (250000, 30)\n",
      "(250000, 301) (568238, 301)\n",
      "Train loss = 0.276\n"
     ]
    }
   ],
   "source": [
    "subsamp = False\n",
    "y, x, id_ = load_csv_data('../data/train.csv', sub_sample=subsamp)\n",
    "y_out_test, x_out_test, id_out_test = load_csv_data('../data/test.csv', sub_sample=subsamp)\n",
    "clean = True\n",
    "dopca = False\n",
    "remove_cols = False\n",
    "stdafter=False\n",
    "cols = (4, 5, 6, 12, 26, 27, 28)\n",
    "#cols=(0, 4, 5, 6, 12, 23, 24, 25, 26, 27, 28)\n",
    "max_comp = 30  # For cleaning, and no removing cols\n",
    "\n",
    "y_train, x_train, x_train_mean, x_train_var, transform_train, eigenvals_train = preprocess(\n",
    "    x,\n",
    "    y,\n",
    "    clean=clean,\n",
    "    dopca=dopca,\n",
    "    max_comp=max_comp,\n",
    "    remove_cols=remove_cols,\n",
    "    cols=cols)\n",
    "y_test, x_test, x_test_mean, x_test_var, transform_test, eigenvals_test = preprocess(\n",
    "    x_out_test,\n",
    "    y_out_test,\n",
    "    clean=clean,\n",
    "    dopca=dopca,\n",
    "    max_comp=max_comp,\n",
    "    remove_cols=remove_cols,\n",
    "    cols=cols)\n",
    "print(x_test.shape, x_train.shape)\n",
    "degree = 10\n",
    "# Build data matrix with feature expansion\n",
    "tx_train = build_poly(x_train, degree)\n",
    "tx_test = build_poly(x_test, degree)\n",
    "print(tx_train.shape, tx_test.shape)\n",
    "if stdafter:\n",
    "    tx_train[:,1:], _, _ = standardize_features(tx_train[:,1:])\n",
    "    tx_test[:,1:], _, _ = standardize_features(tx_test[:,1:])\n",
    "lambda_rr = 1e-4\n",
    "w_rr, loss_rr = ridge_regression(y_train, tx_train, lambda_rr)\n",
    "rr_prediction = predict_labels(w_rr, tx_test)\n",
    "print('Train loss = %.3f'%loss_rr)\n",
    "create_csv_submission(id_out_test, predict_labels(w_rr, tx_test) , '../results/rr_pred_deg10_cl1_pc0_rmcol0_stdafter0.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
