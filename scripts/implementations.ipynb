{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-27T09:58:16.315242Z",
     "start_time": "2019-10-27T09:58:15.677337Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "from proj1_helpers import *\n",
    "from implementations import *\n",
    "from tests import *\n",
    "from preprocessing import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline of the pipeline\n",
    "## Import data\n",
    "+ Import raw data. Split original train data into out test and train sets.\n",
    "## Preprocessing\n",
    "+ The file `preprocessing.py` is imported and contains functions to clean (impute with mean), remove columns, standardize and do PCA.\n",
    "+ Preprocess train and test data separately (you can define the number of principal components used with the max_comp parameter. Defaults to 30).\n",
    "## Apply Model\n",
    "+ Apply your preferred model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-27T09:58:45.899433Z",
     "start_time": "2019-10-27T09:58:18.793404Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read train_y=LABELS, train_x=FEATURES and train_id=EVENT_IDS from dataset.\n",
    "subsamp = False\n",
    "y, x, id_ = load_csv_data('../data/train.csv', sub_sample=subsamp)\n",
    "y_out_test, x_out_test, id_out_test = load_csv_data('../data/test.csv', sub_sample=subsamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-27T09:58:52.148912Z",
     "start_time": "2019-10-27T09:58:45.903695Z"
    }
   },
   "outputs": [],
   "source": [
    "features = np.loadtxt('../data/train.csv', dtype=str, delimiter=',')[0,2:]\n",
    "def show_PC_explicit(features):\n",
    "    import sympy as sy\n",
    "    features_sym = [sy.symbols(f, real=True, positive=True) for f in features]\n",
    "    display(np.array(features_sym).dot(np.around(transform_train, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-27T09:58:52.768342Z",
     "start_time": "2019-10-27T09:58:52.150973Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 30) (200000, 30)\n",
      "['DER_deltaeta_jet_jet' 'DER_mass_jet_jet' 'DER_prodeta_jet_jet'\n",
      " 'DER_lep_eta_centrality' 'PRI_jet_subleading_pt' 'PRI_jet_subleading_eta'\n",
      " 'PRI_jet_subleading_phi']\n"
     ]
    }
   ],
   "source": [
    "clean = True\n",
    "dopca = False\n",
    "remove_cols = False\n",
    "cols = (4, 5, 6, 12, 26, 27, 28)\n",
    "#cols=(0, 4, 5, 6, 12, 23, 24, 25, 26, 27, 28)\n",
    "max_comp = 30  # For cleaning, and no removing cols\n",
    "\n",
    "x_train, y_train, x_test, y_test = split_data(x, y, ratio=0.80, seed=42)\n",
    "y_train, x_train, x_train_mean, x_train_var, transform_train, eigenvals_train = preprocess(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    clean=clean,\n",
    "    dopca=dopca,\n",
    "    max_comp=max_comp,\n",
    "    remove_cols=remove_cols,\n",
    "    cols=cols)\n",
    "y_test, x_test, x_test_mean, x_test_var, transform_test, eigenvals_test = preprocess(\n",
    "    x_test,\n",
    "    y_test,\n",
    "    clean=clean,\n",
    "    dopca=dopca,\n",
    "    max_comp=max_comp,\n",
    "    remove_cols=remove_cols,\n",
    "    cols=cols)\n",
    "print(x_test.shape, x_train.shape)\n",
    "print(features[list(cols)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-27T09:59:41.230009Z",
     "start_time": "2019-10-27T09:59:32.407639Z"
    }
   },
   "outputs": [],
   "source": [
    "degree = 10\n",
    "# Build data matrix with feature expansion\n",
    "tx_train = build_poly(x_train, degree)\n",
    "tx_test = build_poly(x_test, degree)\n",
    "tx_train.shape, tx_test.shape\n",
    "tx_train[:,1:], _, _ = standardize_features(tx_train[:,1:])\n",
    "tx_test[:,1:], _, _ = standardize_features(tx_test[:,1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-26T12:05:11.315942Z",
     "start_time": "2019-10-26T12:02:41.845465Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD (0/4999): loss=0.5\n",
      "GD (100/4999): loss=0.442184935171644\n",
      "GD (200/4999): loss=0.4115490059588939\n",
      "GD (300/4999): loss=0.3919753385151225\n",
      "GD (400/4999): loss=0.37818181437789616\n",
      "GD (500/4999): loss=0.367904721804925\n",
      "GD (600/4999): loss=0.3599855858896922\n",
      "GD (700/4999): loss=0.35375000664114153\n",
      "GD (800/4999): loss=0.34876542821949175\n",
      "GD (900/4999): loss=0.34473459550394064\n",
      "GD (1000/4999): loss=0.3414435679355703\n",
      "GD (1100/4999): loss=0.3387336326074574\n",
      "GD (1200/4999): loss=0.336484633077038\n",
      "GD (1300/4999): loss=0.334604284110886\n",
      "GD (1400/4999): loss=0.3330209257506864\n",
      "GD (1500/4999): loss=0.3316784146817885\n",
      "GD (1600/4999): loss=0.3305324257396797\n",
      "GD (1700/4999): loss=0.3295477241393855\n",
      "GD (1800/4999): loss=0.3286961258209597\n",
      "GD (1900/4999): loss=0.3279549557166036\n",
      "GD (2000/4999): loss=0.3273058717984965\n",
      "GD (2100/4999): loss=0.3267339610171813\n",
      "GD (2200/4999): loss=0.32622703931531677\n",
      "GD (2300/4999): loss=0.3257751061109137\n",
      "GD (2400/4999): loss=0.3253699165865967\n",
      "GD (2500/4999): loss=0.3250046444454194\n",
      "GD (2600/4999): loss=0.3246736145846888\n",
      "GD (2700/4999): loss=0.32437209013143337\n",
      "GD (2800/4999): loss=0.32409610198326266\n",
      "GD (2900/4999): loss=0.32384231176140266\n",
      "GD (3000/4999): loss=0.3236079011602948\n",
      "GD (3100/4999): loss=0.32339048225056594\n",
      "GD (3200/4999): loss=0.32318802448958484\n",
      "GD (3300/4999): loss=0.3229987951110255\n",
      "GD (3400/4999): loss=0.3228213102713424\n",
      "GD (3500/4999): loss=0.32265429487816577\n",
      "GD (3600/4999): loss=0.3224966494514439\n",
      "GD (3700/4999): loss=0.32234742270120953\n",
      "GD (3800/4999): loss=0.3222057887675323\n",
      "GD (3900/4999): loss=0.3220710282747506\n",
      "GD (4000/4999): loss=0.3219425125157545\n",
      "GD (4100/4999): loss=0.32181969021233303\n",
      "GD (4200/4999): loss=0.3217020764016165\n",
      "GD (4300/4999): loss=0.3215892430820294\n",
      "GD (4400/4999): loss=0.32148081131923556\n",
      "GD (4500/4999): loss=0.32137644456666664\n",
      "GD (4600/4999): loss=0.3212758429990345\n",
      "GD (4700/4999): loss=0.3211787386927782\n",
      "GD (4800/4999): loss=0.3210848915163441\n",
      "GD (4900/4999): loss=0.3209940856168225\n",
      "Accuracy ratio = 0.768\n",
      "Test loss = 0.320\n",
      "Train loss = 0.321\n"
     ]
    }
   ],
   "source": [
    "w_init = np.array([0] * tx_train.shape[1])\n",
    "max_iter_gd = 5000\n",
    "gamma_gd = 1e-3\n",
    "w_gd, loss_gd = least_squares_GD(y_train,\n",
    "                                 tx_train,\n",
    "                                 w_init,\n",
    "                                 max_iter_gd,\n",
    "                                 gamma_gd,\n",
    "                                 pr=True,\n",
    "                                 adapt_gamma=False,\n",
    "                                 kind='mse',\n",
    "                                accel=False)\n",
    "gd_prediction = predict_labels(w_gd, tx_test)\n",
    "acc_gd = accuracy_ratio(gd_prediction, y_test)\n",
    "print('Accuracy ratio = %.3f' % acc_gd)\n",
    "print('Test loss = %.3f' % compute_loss(y_test, tx_test, w_gd))\n",
    "print('Train loss = %.3f' % loss_gd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T12:37:03.486835Z",
     "start_time": "2019-10-17T12:37:03.480662Z"
    },
    "hidden": true
   },
   "source": [
    "np.savetxt('../data/w_gd_acc.dat', w_gd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T11:00:36.111308Z",
     "start_time": "2019-10-25T10:57:02.385613Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy ratio = 0.64\n",
      "Test loss = 4.88e-01\n",
      "Train loss = 4.89e-01\n"
     ]
    }
   ],
   "source": [
    "w_init = np.array([0] * tx_train.shape[1])\n",
    "max_iter_sgd = 500\n",
    "gamma_sgd = 1e-5\n",
    "batch_size = 1\n",
    "\n",
    "w_sgd, loss_sgd = least_squares_SGD(y_train,\n",
    "                                    tx_train,\n",
    "                                    w_init,\n",
    "                                    batch_size,\n",
    "                                    max_iter_sgd,\n",
    "                                    gamma_sgd,\n",
    "                                    pr=False,\n",
    "                                    adapt_gamma=False,\n",
    "                                    choose_best=True)\n",
    "sgd_prediction = predict_labels(w_sgd, tx_test)\n",
    "acc_sgd = accuracy_ratio(sgd_prediction, y_test)\n",
    "print('Accuracy ratio = %.2f' % acc_sgd)\n",
    "print('Test loss = %.2e' % compute_loss(y_test, tx_test, w_sgd))\n",
    "print('Train loss = %.2e' % loss_sgd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-26T12:59:19.383028Z",
     "start_time": "2019-10-26T12:59:18.841458Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy ratio = 0.43\n",
      "Train loss = 0.28\n",
      "Test loss = 4.04e+08\n"
     ]
    }
   ],
   "source": [
    "w_lsq, loss_lsq = least_squares(y_train, tx_train)\n",
    "lsq_prediction = predict_labels(w_lsq, tx_test)\n",
    "acc_lsq = accuracy_ratio(lsq_prediction, y_test)\n",
    "print('Accuracy ratio = %.2f' % acc_lsq)\n",
    "print('Train loss = %.2f' % loss_lsq)\n",
    "print('Test loss = %.2e' % compute_loss(y_test, tx_test, w_lsq))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T12:45:30.271461Z",
     "start_time": "2019-10-17T12:45:30.267909Z"
    },
    "hidden": true
   },
   "source": [
    "np.savetxt('../data/w_lsq.dat', w_lsq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-27T10:03:20.051958Z",
     "start_time": "2019-10-27T10:03:19.571658Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy ratio = 0.784\n",
      "Test loss = 0.315\n",
      "Train loss = 0.300\n"
     ]
    }
   ],
   "source": [
    "lambda_rr = 2.7e-3\n",
    "w_rr, loss_rr = ridge_regression(y_train, tx_train, lambda_rr)\n",
    "rr_prediction = predict_labels(w_rr, tx_test)\n",
    "acc_rr = accuracy_ratio(rr_prediction, y_test)\n",
    "print('Accuracy ratio = %.3f'%acc_rr)\n",
    "print('Test loss = %.3f'%compute_loss(y_test, tx_test, w_rr))\n",
    "print('Train loss = %.3f'%loss_rr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T15:18:22.322364Z",
     "start_time": "2019-10-18T15:18:22.309384Z"
    }
   },
   "source": [
    "np.savetxt('../data/w_rr.dat', w_rr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-27T12:28:24.664593Z",
     "start_time": "2019-10-27T12:12:16.106070Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression GD (0/4999): loss=138629.43611198905\n",
      "Logistic Regression GD (100/4999): loss=93238.78320807862\n",
      "Logistic Regression GD (200/4999): loss=90741.30364242186\n",
      "Logistic Regression GD (300/4999): loss=89548.78185686289\n",
      "Logistic Regression GD (400/4999): loss=88790.06707491117\n",
      "Logistic Regression GD (500/4999): loss=88249.09943494303\n",
      "Logistic Regression GD (600/4999): loss=87839.05559566483\n",
      "Logistic Regression GD (700/4999): loss=87515.22787293412\n",
      "Logistic Regression GD (800/4999): loss=87250.93851234576\n",
      "Logistic Regression GD (900/4999): loss=87029.76908116811\n",
      "Logistic Regression GD (1000/4999): loss=86840.94824060913\n",
      "Logistic Regression GD (1100/4999): loss=86677.06876001206\n",
      "Logistic Regression GD (1200/4999): loss=86532.8324551642\n",
      "Logistic Regression GD (1300/4999): loss=86404.96579106138\n",
      "Logistic Regression GD (1400/4999): loss=86290.8533741525\n",
      "Logistic Regression GD (1500/4999): loss=86187.38885925693\n",
      "Logistic Regression GD (1600/4999): loss=86092.72445980992\n",
      "Logistic Regression GD (1700/4999): loss=86005.402470316\n",
      "Logistic Regression GD (1800/4999): loss=85924.2621247991\n",
      "Logistic Regression GD (1900/4999): loss=85848.37273461401\n",
      "Logistic Regression GD (2000/4999): loss=85776.9846873061\n",
      "Logistic Regression GD (2100/4999): loss=85709.49430109875\n",
      "Logistic Regression GD (2200/4999): loss=85645.42078034236\n",
      "Logistic Regression GD (2300/4999): loss=85584.39126659078\n",
      "Logistic Regression GD (2400/4999): loss=85526.12282017177\n",
      "Logistic Regression GD (2500/4999): loss=85470.39127214643\n",
      "Logistic Regression GD (2600/4999): loss=85417.00141347051\n",
      "Logistic Regression GD (2700/4999): loss=85365.7794346273\n",
      "Logistic Regression GD (2800/4999): loss=85316.57595610601\n",
      "Logistic Regression GD (2900/4999): loss=85269.31390078353\n",
      "Logistic Regression GD (3000/4999): loss=85224.63505569921\n",
      "Logistic Regression GD (3100/4999): loss=85183.54106146852\n",
      "Logistic Regression GD (3200/4999): loss=85144.5034117256\n",
      "Logistic Regression GD (3300/4999): loss=85106.98007221232\n",
      "Logistic Regression GD (3400/4999): loss=85070.85056167224\n",
      "Logistic Regression GD (3500/4999): loss=85036.01716426498\n",
      "Logistic Regression GD (3600/4999): loss=85002.41654631056\n",
      "Logistic Regression GD (3700/4999): loss=84970.04923478862\n",
      "Logistic Regression GD (3800/4999): loss=84938.97851435008\n",
      "Logistic Regression GD (3900/4999): loss=84909.24161048446\n",
      "Logistic Regression GD (4000/4999): loss=84880.77075087477\n",
      "Logistic Regression GD (4100/4999): loss=84853.42345433752\n",
      "Logistic Regression GD (4200/4999): loss=84827.04777816549\n",
      "Logistic Regression GD (4300/4999): loss=84801.51557186109\n",
      "Logistic Regression GD (4400/4999): loss=84776.72837440333\n",
      "Logistic Regression GD (4500/4999): loss=84752.61392584635\n",
      "Logistic Regression GD (4600/4999): loss=84729.1216952638\n",
      "Logistic Regression GD (4700/4999): loss=84706.22048024499\n",
      "Logistic Regression GD (4800/4999): loss=84683.90094661916\n",
      "Logistic Regression GD (4900/4999): loss=84662.18387833235\n",
      "Accuracy ratio = 0.804\n",
      "Test loss = 22088.799\n",
      "Train loss = 84641.323\n"
     ]
    }
   ],
   "source": [
    "y_train_log = minus_one_2_zero(y_train)\n",
    "y_test_log = minus_one_2_zero(y_test)\n",
    "\n",
    "\n",
    "w_init = np.array([0] * tx_train.shape[1])\n",
    "max_iter_lrgd = 5000\n",
    "gamma_lrgd = 1e-6\n",
    "w_lrgd, loss_lrgd = logistic_regression(y_train_log,\n",
    "                                        tx_train,\n",
    "                                        w_init,\n",
    "                                        max_iter_lrgd,\n",
    "                                        gamma_lrgd,\n",
    "                                        pr=True,\n",
    "                                        adapt_gamma=False,\n",
    "                                       accel=False)\n",
    "\n",
    "lrgd_prediction = predict_labels(w_lrgd, tx_test)\n",
    "acc_lrgd = accuracy_ratio(lrgd_prediction, y_test)\n",
    "\n",
    "print('Accuracy ratio = %.3f' % acc_lrgd)\n",
    "print('Test loss = %.3f' % compute_loss_logistic(y_test_log, tx_test, w_lrgd))\n",
    "print('Train loss = %.3f' % loss_lrgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-27T12:47:05.472838Z",
     "start_time": "2019-10-27T12:29:55.813945Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Regularized Logistic Regression GD (0/4999): loss=138629.43611198905\n",
      " Regularized Logistic Regression GD (100/4999): loss=93240.1935979139\n",
      " Regularized Logistic Regression GD (200/4999): loss=90743.57200283137\n",
      " Regularized Logistic Regression GD (300/4999): loss=89551.7584977208\n",
      " Regularized Logistic Regression GD (400/4999): loss=88793.67690119703\n",
      " Regularized Logistic Regression GD (500/4999): loss=88253.28982347847\n",
      " Regularized Logistic Regression GD (600/4999): loss=87843.78386950732\n",
      " Regularized Logistic Regression GD (700/4999): loss=87520.45605496898\n",
      " Regularized Logistic Regression GD (800/4999): loss=87256.63476167026\n",
      " Regularized Logistic Regression GD (900/4999): loss=87035.90631510803\n",
      " Regularized Logistic Regression GD (1000/4999): loss=86847.50300860114\n",
      " Regularized Logistic Regression GD (1100/4999): loss=86684.02052046584\n",
      " Regularized Logistic Regression GD (1200/4999): loss=86540.16323838657\n",
      " Regularized Logistic Regression GD (1300/4999): loss=86412.66001537253\n",
      " Regularized Logistic Regression GD (1400/4999): loss=86298.89524136996\n",
      " Regularized Logistic Regression GD (1500/4999): loss=86195.76628362374\n",
      " Regularized Logistic Regression GD (1600/4999): loss=86101.42710233695\n",
      " Regularized Logistic Regression GD (1700/4999): loss=86014.42154285692\n",
      " Regularized Logistic Regression GD (1800/4999): loss=85933.59021403546\n",
      " Regularized Logistic Regression GD (1900/4999): loss=85858.00364508187\n",
      " Regularized Logistic Regression GD (2000/4999): loss=85786.91330859489\n",
      " Regularized Logistic Regression GD (2100/4999): loss=85719.71648635413\n",
      " Regularized Logistic Regression GD (2200/4999): loss=85655.93319340111\n",
      " Regularized Logistic Regression GD (2300/4999): loss=85595.19115175692\n",
      " Regularized Logistic Regression GD (2400/4999): loss=85537.20774439641\n",
      " Regularized Logistic Regression GD (2500/4999): loss=85481.75900703363\n",
      " Regularized Logistic Regression GD (2600/4999): loss=85428.64995039925\n",
      " Regularized Logistic Regression GD (2700/4999): loss=85377.70693253957\n",
      " Regularized Logistic Regression GD (2800/4999): loss=85328.78061310633\n",
      " Regularized Logistic Regression GD (2900/4999): loss=85281.79184020305\n",
      " Regularized Logistic Regression GD (3000/4999): loss=85237.36731559006\n",
      " Regularized Logistic Regression GD (3100/4999): loss=85196.52054502183\n",
      " Regularized Logistic Regression GD (3200/4999): loss=85157.73904251188\n",
      " Regularized Logistic Regression GD (3300/4999): loss=85120.46989701157\n",
      " Regularized Logistic Regression GD (3400/4999): loss=85084.59238019807\n",
      " Regularized Logistic Regression GD (3500/4999): loss=85050.00872335731\n",
      " Regularized Logistic Regression GD (3600/4999): loss=85016.65504186666\n",
      " Regularized Logistic Regression GD (3700/4999): loss=84984.53032563224\n",
      " Regularized Logistic Regression GD (3800/4999): loss=84953.69628625696\n",
      " Regularized Logistic Regression GD (3900/4999): loss=84924.19080569965\n",
      " Regularized Logistic Regression GD (4000/4999): loss=84895.9485207167\n",
      " Regularized Logistic Regression GD (4100/4999): loss=84868.82896409511\n",
      " Regularized Logistic Regression GD (4200/4999): loss=84842.68116129305\n",
      " Regularized Logistic Regression GD (4300/4999): loss=84817.37723643731\n",
      " Regularized Logistic Regression GD (4400/4999): loss=84792.81863349793\n",
      " Regularized Logistic Regression GD (4500/4999): loss=84768.93276024373\n",
      " Regularized Logistic Regression GD (4600/4999): loss=84745.66855693341\n",
      " Regularized Logistic Regression GD (4700/4999): loss=84722.99401167959\n",
      " Regularized Logistic Regression GD (4800/4999): loss=84700.89833328403\n",
      " Regularized Logistic Regression GD (4900/4999): loss=84679.39979830124\n",
      "Accuracy ratio = 0.804\n",
      "Test loss = 22087.373\n",
      "Train loss = 84658.748\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "from implementations import *\n",
    "lambda_rlrgd = 1e0\n",
    "gamma_rlrgd = 1e-6\n",
    "max_iter_rlrgd = 5000\n",
    "\n",
    "w_rlrgd, loss_rlrgd = reg_logistic_regression(y_train_log,\n",
    "                                              tx_train,\n",
    "                                              lambda_rlrgd,\n",
    "                                              w_init,\n",
    "                                              max_iter_rlrgd,\n",
    "                                              gamma_rlrgd,\n",
    "                                              pr=True,\n",
    "                                              adapt_gamma=False, \n",
    "                                              accel=False)\n",
    "rlrgd_prediction = predict_labels(w_rlrgd, tx_test)\n",
    "acc_rlrgd = accuracy_ratio(rlrgd_prediction, y_test)\n",
    "print('Accuracy ratio = %.3f' % acc_rlrgd)\n",
    "print('Test loss = %.3f' % compute_loss_logistic(y_test_log, tx_test, w_rlrgd))\n",
    "print('Train loss = %.3f' % loss_rlrgd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-26T17:14:58.887285Z",
     "start_time": "2019-10-26T17:12:05.237035Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using method ridge_regression\n",
      "Using lambda = 1.0e-04\n",
      "Using lambda = 3.6e-04\n",
      "Using lambda = 1.3e-03\n",
      "Using lambda = 4.6e-03\n",
      "Using lambda = 1.7e-02\n",
      "Using lambda = 6.0e-02\n",
      "Using lambda = 2.2e-01\n",
      "Using lambda = 7.7e-01\n",
      "Using lambda = 2.8e+00\n",
      "Using lambda = 1.0e+01\n",
      "Best lambda from error: 2.15e-01\n",
      "Best lambda from accuracy: 1.67e-02\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "from tests import *\n",
    "from implementations import *\n",
    "clean = True\n",
    "dopca = False\n",
    "remove_cols = False\n",
    "stdafter = False\n",
    "cols = (4, 5, 6, 12, 26, 27, 28)\n",
    "cols=range(16, 30)\n",
    "max_comp = 30  # For cleaning, and no removing cols\n",
    "args_rlrgd['pr']=True\n",
    "\n",
    "cross_validation_demo(x,\n",
    "                      y,\n",
    "                      ridge_regression,\n",
    "                      args_rr,\n",
    "                      k_fold=4,\n",
    "                      degree=10,\n",
    "                      clean=clean,\n",
    "                      dopca=dopca,\n",
    "                      remove_cols=remove_cols,\n",
    "                      lambda_min = -4,\n",
    "                      lambda_max = 1,\n",
    "                      stdafter=stdafter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-27T13:52:48.566Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using method least_squares_GD\n",
      "GD (0/4999): loss=0.5\n",
      "GD (100/4999): loss=0.4560206925074313\n",
      "GD (200/4999): loss=0.4292641703423632\n",
      "GD (300/4999): loss=0.41143348633198323\n",
      "GD (400/4999): loss=0.3988199946404141\n",
      "GD (500/4999): loss=0.3895291216690483\n",
      "GD (600/4999): loss=0.3824820250968874\n",
      "GD (700/4999): loss=0.37701397046542406\n",
      "GD (800/4999): loss=0.3726918727226744\n",
      "GD (900/4999): loss=0.36922175961020354\n",
      "GD (1000/4999): loss=0.3663976140627746\n",
      "GD (1100/4999): loss=0.3640712535796154\n",
      "GD (1200/4999): loss=0.36213375251302676\n",
      "GD (1300/4999): loss=0.360503559310501\n",
      "GD (1400/4999): loss=0.3591186609426928\n",
      "GD (1500/4999): loss=0.35793127625122767\n",
      "GD (1600/4999): loss=0.3569041751151248\n",
      "GD (1700/4999): loss=0.35600806997153933\n",
      "GD (1800/4999): loss=0.35521973149457475\n",
      "GD (1900/4999): loss=0.354520604026057\n",
      "GD (2000/4999): loss=0.3538957728207588\n",
      "GD (2100/4999): loss=0.35333318348106557\n",
      "GD (2200/4999): loss=0.35282304514278673\n",
      "GD (2300/4999): loss=0.3523573695228966\n",
      "GD (2400/4999): loss=0.3519296117458467\n",
      "GD (2500/4999): loss=0.35153438831155304\n",
      "GD (2600/4999): loss=0.35116725414278577\n",
      "GD (2700/4999): loss=0.35082452529837943\n",
      "GD (2800/4999): loss=0.35050313727395976\n",
      "GD (2900/4999): loss=0.3502005312373307\n",
      "GD (3000/4999): loss=0.34991456233161145\n",
      "GD (3100/4999): loss=0.34964342550946553\n",
      "GD (3200/4999): loss=0.34938559536319724\n",
      "GD (3300/4999): loss=0.3491397771767617\n",
      "GD (3400/4999): loss=0.3489048670096503\n",
      "GD (3500/4999): loss=0.3486799190741707\n",
      "GD (3600/4999): loss=0.34846411901943564\n",
      "GD (3700/4999): loss=0.3482567620112856\n",
      "GD (3800/4999): loss=0.34805723471507793\n",
      "GD (3900/4999): loss=0.347865000460968\n",
      "GD (4000/4999): loss=0.3476795870089496\n",
      "GD (4100/4999): loss=0.34750057644109233\n",
      "GD (4200/4999): loss=0.347327596796911\n",
      "GD (4300/4999): loss=0.3471603151391289\n",
      "GD (4400/4999): loss=0.34699843179474205\n",
      "GD (4500/4999): loss=0.3468416755629976\n",
      "GD (4600/4999): loss=0.34668979971982083\n",
      "GD (4700/4999): loss=0.34654257867907473\n",
      "GD (4800/4999): loss=0.3463998051961754\n",
      "GD (4900/4999): loss=0.34626128802009526\n",
      "GD (0/4999): loss=0.5\n",
      "GD (100/4999): loss=0.45589818117496533\n",
      "GD (200/4999): loss=0.42906694900299064\n",
      "GD (300/4999): loss=0.41119574480580656\n",
      "GD (400/4999): loss=0.3985592623190196\n",
      "GD (500/4999): loss=0.38925340532888714\n",
      "GD (600/4999): loss=0.3821947667702171\n",
      "GD (700/4999): loss=0.3767165943067351\n",
      "GD (800/4999): loss=0.37238497188494946\n",
      "GD (900/4999): loss=0.3689056256223369\n",
      "GD (1000/4999): loss=0.36607246473456917\n",
      "GD (1100/4999): loss=0.3637373252381547\n",
      "GD (1200/4999): loss=0.3617913317967602\n",
      "GD (1300/4999): loss=0.360152988527078\n",
      "GD (1400/4999): loss=0.35876033240539734\n",
      "GD (1500/4999): loss=0.3575656225820386\n",
      "GD (1600/4999): loss=0.35653165865270636\n",
      "GD (1700/4999): loss=0.35562917265432664\n",
      "GD (1800/4999): loss=0.3548349457761334\n",
      "GD (1900/4999): loss=0.35413042500701825\n",
      "GD (2000/4999): loss=0.3535006916115205\n",
      "GD (2100/4999): loss=0.3529336817325872\n",
      "GD (2200/4999): loss=0.3524195906490239\n",
      "GD (2300/4999): loss=0.35195041278495987\n",
      "GD (2400/4999): loss=0.3515195833845922\n",
      "GD (2500/4999): loss=0.3511216972173594\n",
      "GD (2600/4999): loss=0.35075228625651217\n",
      "GD (2700/4999): loss=0.35040764292471116\n",
      "GD (2800/4999): loss=0.3500846788367645\n",
      "GD (2900/4999): loss=0.34978081139569817\n",
      "GD (3000/4999): loss=0.349493872384492\n",
      "GD (3100/4999): loss=0.34922203402591795\n",
      "GD (3200/4999): loss=0.3489637489839434\n",
      "GD (3300/4999): loss=0.3487177015408457\n",
      "GD (3400/4999): loss=0.348482767767409\n",
      "GD (3500/4999): loss=0.34825798295436844\n",
      "GD (3600/4999): loss=0.3480425149242998\n",
      "GD (3700/4999): loss=0.3478356421183096\n",
      "GD (3800/4999): loss=0.347636735568881\n",
      "GD (3900/4999): loss=0.34744524404225857\n",
      "GD (4000/4999): loss=0.3472606817708162\n",
      "GD (4100/4999): loss=0.3470826183054795\n",
      "GD (4200/4999): loss=0.34691067010631566\n",
      "GD (4300/4999): loss=0.3467444935603218\n",
      "GD (4400/4999): loss=0.34658377917274513\n",
      "GD (4500/4999): loss=0.3464282467246766\n",
      "GD (4600/4999): loss=0.34627764122733745\n",
      "GD (4700/4999): loss=0.34613172953411775\n",
      "GD (4800/4999): loss=0.3459902974963957\n",
      "GD (4900/4999): loss=0.34585314756953695\n",
      "GD (0/4999): loss=0.5\n",
      "GD (100/4999): loss=0.4559350416515404\n",
      "GD (200/4999): loss=0.42909732131012174\n",
      "GD (300/4999): loss=0.41120483998007273\n",
      "GD (400/4999): loss=0.39854322771362594\n",
      "GD (500/4999): loss=0.38921258064427583\n",
      "GD (600/4999): loss=0.38213100631917446\n",
      "GD (700/4999): loss=0.37663217676686556\n",
      "GD (800/4999): loss=0.37228215923241836\n",
      "GD (900/4999): loss=0.36878649745010095\n",
      "GD (1000/4999): loss=0.3659388709627851\n",
      "GD (1100/4999): loss=0.3635908884110689\n",
      "GD (1200/4999): loss=0.361633468145379\n",
      "GD (1300/4999): loss=0.3599849340991142\n",
      "GD (1400/4999): loss=0.3585831684793835\n",
      "GD (1500/4999): loss=0.35738029824249756\n",
      "GD (1600/4999): loss=0.3563390100443958\n",
      "GD (1700/4999): loss=0.35542993911325077\n",
      "GD (1800/4999): loss=0.35462978326607447\n",
      "GD (1900/4999): loss=0.3539199173228186\n",
      "GD (2000/4999): loss=0.3532853597643506\n",
      "GD (2100/4999): loss=0.35271399186349356\n",
      "GD (2200/4999): loss=0.3521959607489775\n",
      "GD (2300/4999): loss=0.35172321844108356\n",
      "GD (2400/4999): loss=0.35128916272506017\n",
      "GD (2500/4999): loss=0.3508883551906935\n",
      "GD (2600/4999): loss=0.35051629835280945\n",
      "GD (2700/4999): loss=0.3501692584248592\n",
      "GD (2800/4999): loss=0.3498441236592996\n",
      "GD (2900/4999): loss=0.3495382905983692\n",
      "GD (3000/4999): loss=0.34924957236779924\n",
      "GD (3100/4999): loss=0.34897612447821025\n",
      "GD (3200/4999): loss=0.3487163846015186\n",
      "GD (3300/4999): loss=0.34846902355154097\n",
      "GD (3400/4999): loss=0.3482329052821054\n",
      "GD (3500/4999): loss=0.3480070541674673\n",
      "GD (3600/4999): loss=0.34779062818140294\n",
      "GD (3700/4999): loss=0.3475828968669523\n",
      "GD (3800/4999): loss=0.3473832232061287\n",
      "GD (3900/4999): loss=0.34719104867123823\n",
      "GD (4000/4999): loss=0.34700588087674694\n",
      "GD (4100/4999): loss=0.3468272833604681\n",
      "GD (4200/4999): loss=0.3466548671110515\n",
      "GD (4300/4999): loss=0.34648828352982436\n",
      "GD (4400/4999): loss=0.3463272185724588\n",
      "GD (4500/4999): loss=0.34617138786245566\n",
      "GD (4600/4999): loss=0.3460205326062118\n",
      "GD (4700/4999): loss=0.3458744161701523\n",
      "GD (4800/4999): loss=0.34573282120545173\n",
      "GD (4900/4999): loss=0.3455955472263056\n",
      "GD (0/4999): loss=0.5\n",
      "GD (100/4999): loss=0.45577798531600894\n",
      "GD (200/4999): loss=0.42894671871318885\n",
      "GD (300/4999): loss=0.4110923202220566\n",
      "GD (400/4999): loss=0.39846847986637957\n",
      "GD (500/4999): loss=0.3891693095264748\n",
      "GD (600/4999): loss=0.38211303850992473\n",
      "GD (700/4999): loss=0.3766345279273278\n",
      "GD (800/4999): loss=0.37230096778370003\n",
      "GD (900/4999): loss=0.3688187842960533\n",
      "GD (1000/4999): loss=0.3659823265441622\n",
      "GD (1100/4999): loss=0.3636437105882666\n",
      "GD (1200/4999): loss=0.3616942412932478\n",
      "GD (1300/4999): loss=0.3600525382389491\n",
      "GD (1400/4999): loss=0.35865671037458924\n",
      "GD (1500/4999): loss=0.3574590589373905\n",
      "GD (1600/4999): loss=0.35642240492936517\n",
      "GD (1700/4999): loss=0.3555174875966893\n",
      "GD (1800/4999): loss=0.35472108580476386\n",
      "GD (1900/4999): loss=0.35401463805746247\n",
      "GD (2000/4999): loss=0.3533832133943253\n",
      "GD (2100/4999): loss=0.3528147337051623\n",
      "GD (2200/4999): loss=0.3522993791704195\n",
      "GD (2300/4999): loss=0.3518291290629251\n",
      "GD (2400/4999): loss=0.3513974039306921\n",
      "GD (2500/4999): loss=0.3509987846072017\n",
      "GD (2600/4999): loss=0.3506287900533723\n",
      "GD (2700/4999): loss=0.350283700670256\n",
      "GD (2800/4999): loss=0.3499604170456602\n",
      "GD (2900/4999): loss=0.34965634651454475\n",
      "GD (3000/4999): loss=0.3493693116920825\n",
      "GD (3100/4999): loss=0.34909747646314954\n",
      "GD (3200/4999): loss=0.3488392859092227\n",
      "GD (3300/4999): loss=0.3485934174116382\n",
      "GD (3400/4999): loss=0.34835874075148987\n",
      "GD (3500/4999): loss=0.3481342854759396\n",
      "GD (3600/4999): loss=0.3479192141508741\n",
      "GD (3700/4999): loss=0.3477128003944387\n",
      "GD (3800/4999): loss=0.3475144108026289\n",
      "GD (3900/4999): loss=0.34732349004995033\n",
      "GD (4000/4999): loss=0.3471395485851092\n",
      "GD (4100/4999): loss=0.34696215245129414\n",
      "GD (4200/4999): loss=0.3467909148486427\n",
      "GD (4300/4999): loss=0.3466254891274382\n",
      "GD (4400/4999): loss=0.3464655629579199\n",
      "GD (4500/4999): loss=0.34631085346904306\n",
      "GD (4600/4999): loss=0.34616110318624815\n",
      "GD (4700/4999): loss=0.3460160766289851\n",
      "GD (4800/4999): loss=0.34587555745374576\n",
      "GD (4900/4999): loss=0.3457393460487673\n",
      "Using method least_squares_GD\n",
      "GD (0/4999): loss=0.5\n",
      "GD (100/4999): loss=0.4419881625016512\n",
      "GD (200/4999): loss=0.411267678685831\n",
      "GD (300/4999): loss=0.3916029185186806\n",
      "GD (400/4999): loss=0.37773968006545156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD (500/4999): loss=0.3674117885519998\n",
      "GD (600/4999): loss=0.35945438110764505\n",
      "GD (700/4999): loss=0.353188749550242\n",
      "GD (800/4999): loss=0.3481799587845124\n",
      "GD (900/4999): loss=0.34412953382542394\n",
      "GD (1000/4999): loss=0.3408229155005411\n",
      "GD (1100/4999): loss=0.33810103976812134\n",
      "GD (1200/4999): loss=0.3358434875930232\n",
      "GD (1300/4999): loss=0.3339577168720511\n",
      "GD (1400/4999): loss=0.3323717883001665\n",
      "GD (1500/4999): loss=0.33102925614988915\n",
      "GD (1600/4999): loss=0.32988548060579187\n",
      "GD (1700/4999): loss=0.3289049134159444\n",
      "GD (1800/4999): loss=0.32805907004522344\n",
      "GD (1900/4999): loss=0.32732499658523184\n",
      "GD (2000/4999): loss=0.32668409911808105\n",
      "GD (2100/4999): loss=0.32612124211049953\n",
      "GD (2200/4999): loss=0.32562404869608286\n",
      "GD (2300/4999): loss=0.32518235390617223\n",
      "GD (2400/4999): loss=0.3247877747534961\n",
      "GD (2500/4999): loss=0.32443337027098035\n",
      "GD (2600/4999): loss=0.32411337127840656\n",
      "GD (2700/4999): loss=0.32382296453950254\n",
      "GD (2800/4999): loss=0.3235581195917519\n",
      "GD (2900/4999): loss=0.3233154492344002\n",
      "GD (3000/4999): loss=0.32309209669530115\n",
      "GD (3100/4999): loss=0.32288564404097697\n",
      "GD (3200/4999): loss=0.3226940375733956\n",
      "GD (3300/4999): loss=0.32251552686341944\n",
      "GD (3400/4999): loss=0.32234861477192017\n",
      "GD (3500/4999): loss=0.32219201635474026\n",
      "GD (3600/4999): loss=0.3220446249738968\n",
      "GD (3700/4999): loss=0.3219054842722221\n",
      "GD (3800/4999): loss=0.32177376493282656\n",
      "GD (3900/4999): loss=0.32164874535411037\n",
      "GD (4000/4999): loss=0.3215297955375866\n",
      "GD (4100/4999): loss=0.3214163636187432\n",
      "GD (4200/4999): loss=0.32130796457769467\n",
      "GD (4300/4999): loss=0.3212041707519859\n",
      "GD (4400/4999): loss=0.32110460384293155\n",
      "GD (4500/4999): loss=0.3210089281626624\n",
      "GD (4600/4999): loss=0.32091684491427136\n",
      "GD (4700/4999): loss=0.3208280873342063\n",
      "GD (4800/4999): loss=0.3207424165559806\n",
      "GD (4900/4999): loss=0.3206596180787156\n",
      "GD (0/4999): loss=0.5\n",
      "GD (100/4999): loss=0.44214779491043943\n",
      "GD (200/4999): loss=0.41143033403149076\n",
      "GD (300/4999): loss=0.3917934630682683\n",
      "GD (400/4999): loss=0.37795731793157505\n",
      "GD (500/4999): loss=0.3676541857506317\n",
      "GD (600/4999): loss=0.3597215302794485\n",
      "GD (700/4999): loss=0.3534816902437128\n",
      "GD (800/4999): loss=0.34849965648413916\n",
      "GD (900/4999): loss=0.3444763616791721\n",
      "GD (1000/4999): loss=0.34119652089850366\n",
      "GD (1100/4999): loss=0.3385004085996792\n",
      "GD (1200/4999): loss=0.3362670913860707\n",
      "GD (1300/4999): loss=0.3344036780661002\n",
      "GD (1400/4999): loss=0.33283803072509544\n",
      "GD (1500/4999): loss=0.3315136271607041\n",
      "GD (1600/4999): loss=0.33038584226360085\n",
      "GD (1700/4999): loss=0.32941920557400023\n",
      "GD (1800/4999): loss=0.32858535036938696\n",
      "GD (1900/4999): loss=0.3278614628940629\n",
      "GD (2000/4999): loss=0.32722909890460355\n",
      "GD (2100/4999): loss=0.3266732732504581\n",
      "GD (2200/4999): loss=0.3261817544465377\n",
      "GD (2300/4999): loss=0.32574451448982883\n",
      "GD (2400/4999): loss=0.3253532971578677\n",
      "GD (2500/4999): loss=0.32500127737197027\n",
      "GD (2600/4999): loss=0.3246827910093325\n",
      "GD (2700/4999): loss=0.3243931195458997\n",
      "GD (2800/4999): loss=0.32412831761607225\n",
      "GD (2900/4999): loss=0.3238850743421745\n",
      "GD (3000/4999): loss=0.32366060136832625\n",
      "GD (3100/4999): loss=0.3234525421101815\n",
      "GD (3200/4999): loss=0.32325889793403184\n",
      "GD (3300/4999): loss=0.323077967900589\n",
      "GD (3400/4999): loss=0.32290829941976157\n",
      "GD (3500/4999): loss=0.32274864771406475\n",
      "GD (3600/4999): loss=0.3225979424180437\n",
      "GD (3700/4999): loss=0.32245525997766067\n",
      "GD (3800/4999): loss=0.32231980077842626\n",
      "GD (3900/4999): loss=0.3221908701403375\n",
      "GD (4000/4999): loss=0.3220678624837561\n",
      "GD (4100/4999): loss=0.32195024810265\n",
      "GD (4200/4999): loss=0.32183756208739284\n",
      "GD (4300/4999): loss=0.3217293950241756\n",
      "GD (4400/4999): loss=0.3216253851663892\n",
      "GD (4500/4999): loss=0.32152521182849253\n",
      "GD (4600/4999): loss=0.32142858979753314\n",
      "GD (4700/4999): loss=0.32133526459376155\n",
      "GD (4800/4999): loss=0.32124500844129766\n",
      "GD (4900/4999): loss=0.3211576168339101\n",
      "GD (0/4999): loss=0.5\n",
      "GD (100/4999): loss=0.44219842195242015\n",
      "GD (200/4999): loss=0.41147170166150465\n",
      "GD (300/4999): loss=0.39181969350801477\n",
      "GD (400/4999): loss=0.37797394559850483\n",
      "GD (500/4999): loss=0.367666810498427\n",
      "GD (600/4999): loss=0.35973363736616015\n",
      "GD (700/4999): loss=0.3534948944999793\n",
      "GD (800/4999): loss=0.348514359827908\n",
      "GD (900/4999): loss=0.34449230178237444\n",
      "GD (1000/4999): loss=0.34121313453152136\n",
      "GD (1100/4999): loss=0.3385170458623473\n",
      "GD (1200/4999): loss=0.33628313046828867\n",
      "GD (1300/4999): loss=0.3344185793743183\n",
      "GD (1400/4999): loss=0.33285135628113166\n",
      "GD (1500/4999): loss=0.3315250413052894\n",
      "GD (1600/4999): loss=0.33039510313609505\n",
      "GD (1700/4999): loss=0.32942615275453285\n",
      "GD (1800/4999): loss=0.3285898916197782\n",
      "GD (1900/4999): loss=0.3278635615035833\n",
      "GD (2000/4999): loss=0.3272287623321197\n",
      "GD (2100/4999): loss=0.3266705433042859\n",
      "GD (2200/4999): loss=0.32617669900507384\n",
      "GD (2300/4999): loss=0.3257372206490531\n",
      "GD (2400/4999): loss=0.3253438656427215\n",
      "GD (2500/4999): loss=0.3249898180363607\n",
      "GD (2600/4999): loss=0.3246694192561876\n",
      "GD (2700/4999): loss=0.32437795351421606\n",
      "GD (2800/4999): loss=0.3241114760006384\n",
      "GD (2900/4999): loss=0.32386667473065855\n",
      "GD (3000/4999): loss=0.3236407589981919\n",
      "GD (3100/4999): loss=0.3234313689638172\n",
      "GD (3200/4999): loss=0.3232365021043361\n",
      "GD (3300/4999): loss=0.3230544531711306\n",
      "GD (3400/4999): loss=0.3228837650136622\n",
      "GD (3500/4999): loss=0.3227231881741758\n",
      "GD (3600/4999): loss=0.3225716475879962\n",
      "GD (3700/4999): loss=0.3224282150591858\n",
      "GD (3800/4999): loss=0.3222920864451444\n",
      "GD (3900/4999): loss=0.3221625626921693\n",
      "GD (4000/4999): loss=0.3220390340293483\n",
      "GD (4100/4999): loss=0.3219209667598666\n",
      "GD (4200/4999): loss=0.3218078921940804\n",
      "GD (4300/4999): loss=0.3216993973531734\n",
      "GD (4400/4999): loss=0.321595117140178\n",
      "GD (4500/4999): loss=0.3214947277300214\n",
      "GD (4600/4999): loss=0.32139794097469043\n",
      "GD (4700/4999): loss=0.3213044996556851\n",
      "GD (4800/4999): loss=0.3212141734453114\n",
      "GD (4900/4999): loss=0.3211267554623276\n",
      "GD (0/4999): loss=0.5\n",
      "GD (100/4999): loss=0.4420395791155073\n",
      "GD (200/4999): loss=0.41132352900371943\n",
      "GD (300/4999): loss=0.39168694923893144\n",
      "GD (400/4999): loss=0.37784146142145625\n",
      "GD (500/4999): loss=0.3675206353049553\n",
      "GD (600/4999): loss=0.3595639818937557\n",
      "GD (700/4999): loss=0.3532957563612478\n",
      "GD (800/4999): loss=0.34828256966020255\n",
      "GD (900/4999): loss=0.3442266397621695\n",
      "GD (1000/4999): loss=0.3409136562860819\n",
      "GD (1100/4999): loss=0.33818460845456577\n",
      "GD (1200/4999): loss=0.3359190727670962\n",
      "GD (1300/4999): loss=0.33402451139396466\n",
      "GD (1400/4999): loss=0.33242902207187175\n",
      "GD (1500/4999): loss=0.33107623038600237\n",
      "GD (1600/4999): loss=0.3299215936741\n",
      "GD (1700/4999): loss=0.3289296755079017\n",
      "GD (1800/4999): loss=0.3280721075427304\n",
      "GD (1900/4999): loss=0.3273260484342185\n",
      "GD (2000/4999): loss=0.3266730077877456\n",
      "GD (2100/4999): loss=0.3260979414272128\n",
      "GD (2200/4999): loss=0.32558855034784767\n",
      "GD (2300/4999): loss=0.32513473390227327\n",
      "GD (2400/4999): loss=0.32472816067850846\n",
      "GD (2500/4999): loss=0.3243619298204184\n",
      "GD (2600/4999): loss=0.3240303023045172\n",
      "GD (2700/4999): loss=0.32372848665719345\n",
      "GD (2800/4999): loss=0.32345246727983756\n",
      "GD (2900/4999): loss=0.32319886630020117\n",
      "GD (3000/4999): loss=0.3229648319373975\n",
      "GD (3100/4999): loss=0.322747947934657\n",
      "GD (3200/4999): loss=0.322546159807756\n",
      "GD (3300/4999): loss=0.3223577145721662\n",
      "GD (3400/4999): loss=0.3221811113174624\n",
      "GD (3500/4999): loss=0.3220150605443686\n",
      "GD (3600/4999): loss=0.3218584506058705\n",
      "GD (3700/4999): loss=0.3217103199273937\n",
      "GD (3800/4999): loss=0.3215698339434335\n",
      "GD (3900/4999): loss=0.32143626589532304\n",
      "GD (4000/4999): loss=0.32130898079930675\n",
      "GD (4100/4999): loss=0.3211874220250896\n",
      "GD (4200/4999): loss=0.3210711000297893\n",
      "GD (4300/4999): loss=0.320959582876267\n",
      "GD (4400/4999): loss=0.3208524882324829\n",
      "GD (4500/4999): loss=0.32074947660317743\n",
      "GD (4600/4999): loss=0.32065024558945976\n",
      "GD (4700/4999): loss=0.3205545250078526\n",
      "GD (4800/4999): loss=0.3204620727296574\n",
      "GD (4900/4999): loss=0.32037267112543355\n",
      "Using method least_squares_GD\n",
      "GD (0/4999): loss=0.5\n",
      "GD (100/4999): loss=0.43763723991425607\n",
      "GD (200/4999): loss=0.4078823443816028\n",
      "GD (300/4999): loss=0.3892759457551101\n",
      "GD (400/4999): loss=0.3761055768083414\n",
      "GD (500/4999): loss=0.3662151552370332\n",
      "GD (600/4999): loss=0.3585471611344103\n",
      "GD (700/4999): loss=0.3524786834370813\n",
      "GD (800/4999): loss=0.34760123629772016\n",
      "GD (900/4999): loss=0.34363062623120993\n",
      "GD (1000/4999): loss=0.340362038765514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD (1100/4999): loss=0.3376442529643002\n",
      "GD (1200/4999): loss=0.3353635158264099\n",
      "GD (1300/4999): loss=0.33343293570367477\n",
      "GD (1400/4999): loss=0.33178527165535554\n",
      "GD (1500/4999): loss=0.33036790787720965\n",
      "GD (1600/4999): loss=0.3291392772618965\n",
      "GD (1700/4999): loss=0.3280662690097418\n",
      "GD (1800/4999): loss=0.3271223183030348\n",
      "GD (1900/4999): loss=0.32628597763897643\n",
      "GD (2000/4999): loss=0.325539834226564\n",
      "GD (2100/4999): loss=0.32486968002565775\n",
      "GD (2200/4999): loss=0.3242638689432229\n",
      "GD (2300/4999): loss=0.3237128145253063\n",
      "GD (2400/4999): loss=0.3232085943778196\n",
      "GD (2500/4999): loss=0.3227446365255419\n",
      "GD (2600/4999): loss=0.3223154692657284\n",
      "GD (2700/4999): loss=0.321916520628051\n",
      "GD (2800/4999): loss=0.32154395686861115\n",
      "GD (2900/4999): loss=0.3211945518716889\n",
      "GD (3000/4999): loss=0.32086558115913166\n",
      "GD (3100/4999): loss=0.32055473558603603\n",
      "GD (3200/4999): loss=0.3202600508527694\n",
      "GD (3300/4999): loss=0.31997984977232535\n",
      "GD (3400/4999): loss=0.3197126948594033\n",
      "GD (3500/4999): loss=0.3194573492976233\n",
      "GD (3600/4999): loss=0.31921274472639904\n",
      "GD (3700/4999): loss=0.31897795459331835\n",
      "GD (3800/4999): loss=0.31875217205954004\n",
      "GD (3900/4999): loss=0.31853469163843284\n",
      "GD (4000/4999): loss=0.3183248939019577\n",
      "GD (4100/4999): loss=0.31812223271322104\n",
      "GD (4200/4999): loss=0.31792622454347463\n",
      "GD (4300/4999): loss=0.3177364395125113\n",
      "GD (4400/4999): loss=0.3175524938567559\n",
      "GD (4500/4999): loss=0.31737404358240234\n",
      "GD (4600/4999): loss=0.31720077910411526\n",
      "GD (4700/4999): loss=0.31703242070501036\n",
      "GD (4800/4999): loss=0.31686871468236916\n",
      "GD (4900/4999): loss=0.3167094300670698\n",
      "GD (0/4999): loss=0.5\n",
      "GD (100/4999): loss=0.4378919575219443\n",
      "GD (200/4999): loss=0.40816061186243374\n",
      "GD (300/4999): loss=0.38959314863290356\n",
      "GD (400/4999): loss=0.37645812227779213\n",
      "GD (500/4999): loss=0.3666003197271212\n",
      "GD (600/4999): loss=0.358963357657065\n",
      "GD (700/4999): loss=0.35292397531872266\n",
      "GD (800/4999): loss=0.3480727511418634\n",
      "GD (900/4999): loss=0.34412461719024\n",
      "GD (1000/4999): loss=0.340874202800182\n",
      "GD (1100/4999): loss=0.33817007611977434\n",
      "GD (1200/4999): loss=0.3358985496898638\n",
      "GD (1300/4999): loss=0.3339729841683084\n",
      "GD (1400/4999): loss=0.332326499285129\n",
      "GD (1500/4999): loss=0.3309068901969584\n",
      "GD (1600/4999): loss=0.32967301224222667\n",
      "GD (1700/4999): loss=0.32859216473917496\n",
      "GD (1800/4999): loss=0.3276381673601847\n",
      "GD (1900/4999): loss=0.32678992503884635\n",
      "GD (2000/4999): loss=0.32603034318674773\n",
      "GD (2100/4999): loss=0.3253454980303341\n",
      "GD (2200/4999): loss=0.32472399545324065\n",
      "GD (2300/4999): loss=0.3241564709911095\n",
      "GD (2400/4999): loss=0.32363519680561414\n",
      "GD (2500/4999): loss=0.3231537706190241\n",
      "GD (2600/4999): loss=0.32270686804514753\n",
      "GD (2700/4999): loss=0.3222900443705053\n",
      "GD (2800/4999): loss=0.3218995751908049\n",
      "GD (2900/4999): loss=0.32153232777259144\n",
      "GD (3000/4999): loss=0.3211856568457314\n",
      "GD (3100/4999): loss=0.3208573199154955\n",
      "GD (3200/4999): loss=0.32054540823600536\n",
      "GD (3300/4999): loss=0.3202482903960036\n",
      "GD (3400/4999): loss=0.31996456609491125\n",
      "GD (3500/4999): loss=0.3196930281765229\n",
      "GD (3600/4999): loss=0.31943263137209965\n",
      "GD (3700/4999): loss=0.3191824665082574\n",
      "GD (3800/4999): loss=0.3189417391760557\n",
      "GD (3900/4999): loss=0.3187097520497951\n",
      "GD (4000/4999): loss=0.31848589019773715\n",
      "GD (4100/4999): loss=0.3182696088503304\n",
      "GD (4200/4999): loss=0.3180604231908594\n",
      "GD (4300/4999): loss=0.31785789981359885\n",
      "GD (4400/4999): loss=0.31766164955942555\n",
      "GD (4500/4999): loss=0.3174713214914304\n",
      "GD (4600/4999): loss=0.31728659781579865\n",
      "GD (4700/4999): loss=0.3171071895880034\n",
      "GD (4800/4999): loss=0.3169328330727063\n",
      "GD (4900/4999): loss=0.31676328664891606\n",
      "GD (0/4999): loss=0.5\n",
      "GD (100/4999): loss=0.4379431097385053\n"
     ]
    }
   ],
   "source": [
    "from implementations import *\n",
    "clean = True\n",
    "dopca = False\n",
    "remove_cols = False\n",
    "stdafter = True\n",
    "cols = (4, 5, 6, 12, 26, 27, 28)\n",
    "max_comp = 30  # For cleaning, and no removing cols\n",
    "args_gd['max_iters']=5000\n",
    "args_gd['pr']=True\n",
    "\n",
    "for deg in range(1, 10):\n",
    "    cross_validation_demo(x,\n",
    "                      y,\n",
    "                      least_squares_GD,\n",
    "                      args_gd,\n",
    "                      k_fold=4,\n",
    "                      degree=deg,\n",
    "                      clean=clean,\n",
    "                      dopca=dopca,\n",
    "                      remove_cols=remove_cols,\n",
    "                      lambda_min = -4,\n",
    "                      lambda_max = 1,\n",
    "                      stdafter=stdafter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-26T15:11:31.355165Z",
     "start_time": "2019-10-26T15:10:33.065349Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(568238, 30) (250000, 30)\n",
      "(250000, 301) (568238, 301)\n",
      "Train loss = 0.276\n"
     ]
    }
   ],
   "source": [
    "subsamp = False\n",
    "y, x, id_ = load_csv_data('../data/train.csv', sub_sample=subsamp)\n",
    "y_out_test, x_out_test, id_out_test = load_csv_data('../data/test.csv', sub_sample=subsamp)\n",
    "clean = True\n",
    "dopca = False\n",
    "remove_cols = False\n",
    "stdafter=False\n",
    "cols = (4, 5, 6, 12, 26, 27, 28)\n",
    "#cols=(0, 4, 5, 6, 12, 23, 24, 25, 26, 27, 28)\n",
    "max_comp = 30  # For cleaning, and no removing cols\n",
    "\n",
    "y_train, x_train, x_train_mean, x_train_var, transform_train, eigenvals_train = preprocess(\n",
    "    x,\n",
    "    y,\n",
    "    clean=clean,\n",
    "    dopca=dopca,\n",
    "    max_comp=max_comp,\n",
    "    remove_cols=remove_cols,\n",
    "    cols=cols)\n",
    "y_test, x_test, x_test_mean, x_test_var, transform_test, eigenvals_test = preprocess(\n",
    "    x_out_test,\n",
    "    y_out_test,\n",
    "    clean=clean,\n",
    "    dopca=dopca,\n",
    "    max_comp=max_comp,\n",
    "    remove_cols=remove_cols,\n",
    "    cols=cols)\n",
    "print(x_test.shape, x_train.shape)\n",
    "degree = 10\n",
    "# Build data matrix with feature expansion\n",
    "tx_train = build_poly(x_train, degree)\n",
    "tx_test = build_poly(x_test, degree)\n",
    "print(tx_train.shape, tx_test.shape)\n",
    "if stdafter:\n",
    "    tx_train[:,1:], _, _ = standardize_features(tx_train[:,1:])\n",
    "    tx_test[:,1:], _, _ = standardize_features(tx_test[:,1:])\n",
    "lambda_rr = 1e-4\n",
    "w_rr, loss_rr = ridge_regression(y_train, tx_train, lambda_rr)\n",
    "rr_prediction = predict_labels(w_rr, tx_test)\n",
    "print('Train loss = %.3f'%loss_rr)\n",
    "create_csv_submission(id_out_test, predict_labels(w_rr, tx_test) , '../results/rr_pred_deg10_cl1_pc0_rmcol0_stdafter0.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
