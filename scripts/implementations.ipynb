{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T20:01:40.422352Z",
     "start_time": "2019-10-15T20:01:40.173636Z"
    }
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "from implementations import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T20:28:17.771388Z",
     "start_time": "2019-10-15T20:27:53.309633Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read train_y=LABELS, train_x=FEATURES and train_id=EVENT_IDS from dataset.\n",
    "subsamp = False\n",
    "y, x, id_ = load_csv_data('../data/train.csv', sub_sample=subsamp)\n",
    "y_out_test, x_out_test, id_out_test = load_csv_data('../data/test.csv', sub_sample=subsamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T20:28:18.426365Z",
     "start_time": "2019-10-15T20:28:17.774048Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.          0.45598693 -0.4847834  ...  0.40851886  0.40851934\n",
      "   0.55606086]\n",
      " [ 1.          0.42345116  0.69984786 ...  0.40851886  0.40851934\n",
      "   0.55606086]\n",
      " [ 1.          0.40143016  0.79374034 ...  0.40851886  0.40851934\n",
      "   0.55606086]\n",
      " ...\n",
      " [ 1.          0.26640857  0.0455462  ...  0.40851886  0.40851934\n",
      "   0.1046841 ]\n",
      " [ 1.          0.39568812  0.44091423 ...  0.40851886  0.40851934\n",
      "   0.02164346]\n",
      " [ 1.          0.62604629  1.17423852 ...  0.40851886  0.40851934\n",
      "   0.55606086]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((225000, 61), (25000, 61))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, y_train, x_test, y_test = split_data(x, y, ratio=0.9, seed = 42)\n",
    "# Clean data\n",
    "#y_train, tx_train = clean_data(y_train, tx_train)\n",
    "#y_test, tx_test = clean_data(y_test, tx_test) #Test data should not be cleaned.\n",
    "# Standardize data\n",
    "#y_train_std = standardize(y_train)\n",
    "x_train_std = standardize_features(x_train)\n",
    "#y_test_std = standardize(y_test)\n",
    "x_test_std = standardize_features(x_test)\n",
    "#y_train = y_train_std[0]\n",
    "x_train = x_train_std[0]\n",
    "#y_test = y_test_std[0]\n",
    "x_test = x_test_std[0]\n",
    "\n",
    "tx_train = build_poly(x_train, 2)\n",
    "tx_test = build_poly(x_test, 2)\n",
    "print(tx_train)\n",
    "tx_train.shape, tx_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T21:18:52.440857Z",
     "start_time": "2019-10-15T20:32:14.804532Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD (0/99999): loss=0.5\n",
      "GD (100/99999): loss=0.3933244847297514\n",
      "GD (200/99999): loss=0.3759187512683832\n",
      "GD (300/99999): loss=0.36616162698599686\n",
      "GD (400/99999): loss=0.3592903953500747\n",
      "GD (500/99999): loss=0.35419231487256114\n",
      "GD (600/99999): loss=0.35030381270122457\n",
      "GD (700/99999): loss=0.3472769715098665\n",
      "GD (800/99999): loss=0.34488191482576264\n",
      "GD (900/99999): loss=0.3429601823185156\n",
      "GD (1000/99999): loss=0.34139906425259764\n",
      "GD (1100/99999): loss=0.3401164525642246\n",
      "GD (1200/99999): loss=0.3390514089135657\n",
      "GD (1300/99999): loss=0.33815802910278536\n",
      "GD (1400/99999): loss=0.33740130805889773\n",
      "GD (1500/99999): loss=0.33675427355084603\n",
      "GD (1600/99999): loss=0.33619595439395655\n",
      "GD (1700/99999): loss=0.33570991396363237\n",
      "GD (1800/99999): loss=0.33528317587461803\n",
      "GD (1900/99999): loss=0.334905427017017\n",
      "GD (2000/99999): loss=0.3345684199165106\n",
      "GD (2100/99999): loss=0.3342655202991315\n",
      "GD (2200/99999): loss=0.3339913616883218\n",
      "GD (2300/99999): loss=0.33374157972047713\n",
      "GD (2400/99999): loss=0.3335126063880692\n",
      "GD (2500/99999): loss=0.33330150970934075\n",
      "GD (2600/99999): loss=0.333105868092152\n",
      "GD (2700/99999): loss=0.33292367137592704\n",
      "GD (2800/99999): loss=0.3327532425146064\n",
      "GD (2900/99999): loss=0.33259317531940735\n",
      "GD (3000/99999): loss=0.3324422847610173\n",
      "GD (3100/99999): loss=0.33229956713990205\n",
      "GD (3200/99999): loss=0.3321641680436869\n",
      "GD (3300/99999): loss=0.33203535647412863\n",
      "GD (3400/99999): loss=0.33191250388056154\n",
      "GD (3500/99999): loss=0.3317950671091757\n",
      "GD (3600/99999): loss=0.33168257448809146\n",
      "GD (3700/99999): loss=0.33157461443177366\n",
      "GD (3800/99999): loss=0.3314708260759332\n",
      "GD (3900/99999): loss=0.3313708915539949\n",
      "GD (4000/99999): loss=0.33127452960477316\n",
      "GD (4100/99999): loss=0.3311814902629637\n",
      "GD (4200/99999): loss=0.33109155043307315\n",
      "GD (4300/99999): loss=0.33100451018633076\n",
      "GD (4400/99999): loss=0.3309201896510758\n",
      "GD (4500/99999): loss=0.3308384263918273\n",
      "GD (4600/99999): loss=0.3307590731920078\n",
      "GD (4700/99999): loss=0.3306819961711399\n",
      "GD (4800/99999): loss=0.3306070731800842\n",
      "GD (4900/99999): loss=0.33053419242815685\n",
      "GD (5000/99999): loss=0.33046325130426196\n",
      "GD (5100/99999): loss=0.33039415536089667\n",
      "GD (5200/99999): loss=0.33032681743533915\n",
      "GD (5300/99999): loss=0.3302611568867706\n",
      "GD (5400/99999): loss=0.3301970989317032\n",
      "GD (5500/99999): loss=0.3301345740630457\n",
      "GD (5600/99999): loss=0.3300735175405686\n",
      "GD (5700/99999): loss=0.33001386894252266\n",
      "GD (5800/99999): loss=0.32995557176981005\n",
      "GD (5900/99999): loss=0.3298985730954667\n",
      "GD (6000/99999): loss=0.3298428232533382\n",
      "GD (6100/99999): loss=0.32978827556076307\n",
      "GD (6200/99999): loss=0.3297348860708632\n",
      "GD (6300/99999): loss=0.32968261335068144\n",
      "GD (6400/99999): loss=0.32963141828195863\n",
      "GD (6500/99999): loss=0.3295812638817944\n",
      "GD (6600/99999): loss=0.3295321151408223\n",
      "GD (6700/99999): loss=0.32948393887685695\n",
      "GD (6800/99999): loss=0.3294367036022407\n",
      "GD (6900/99999): loss=0.3293903794033602\n",
      "GD (7000/99999): loss=0.3293449378309894\n",
      "GD (7100/99999): loss=0.32930035180030093\n",
      "GD (7200/99999): loss=0.32925659549952\n",
      "GD (7300/99999): loss=0.32921364430632644\n",
      "GD (7400/99999): loss=0.32917147471122016\n",
      "GD (7500/99999): loss=0.32913006424715197\n",
      "GD (7600/99999): loss=0.3290893914248097\n",
      "GD (7700/99999): loss=0.3290494356730113\n",
      "GD (7800/99999): loss=0.3290101772837257\n",
      "GD (7900/99999): loss=0.3289715973612894\n",
      "GD (8000/99999): loss=0.32893367777543575\n",
      "GD (8100/99999): loss=0.32889640111779445\n",
      "GD (8200/99999): loss=0.3288597506615535\n",
      "GD (8300/99999): loss=0.3288237103240098\n",
      "GD (8400/99999): loss=0.328788264631758\n",
      "GD (8500/99999): loss=0.32875339868829767\n",
      "GD (8600/99999): loss=0.3287190981438557\n",
      "GD (8700/99999): loss=0.32868534916724307\n",
      "GD (8800/99999): loss=0.3286521384195801\n",
      "GD (8900/99999): loss=0.3286194530297436\n",
      "GD (9000/99999): loss=0.3285872805713977\n",
      "GD (9100/99999): loss=0.32855560904148784\n",
      "GD (9200/99999): loss=0.328524426840084\n",
      "GD (9300/99999): loss=0.328493722751472\n",
      "GD (9400/99999): loss=0.3284634859264001\n",
      "GD (9500/99999): loss=0.3284337058653944\n",
      "GD (9600/99999): loss=0.32840437240306625\n",
      "GD (9700/99999): loss=0.3283754756933405\n",
      "GD (9800/99999): loss=0.3283470061955377\n",
      "GD (9900/99999): loss=0.3283189546612522\n",
      "GD (10000/99999): loss=0.32829131212196894\n",
      "GD (10100/99999): loss=0.3282640698773705\n",
      "GD (10200/99999): loss=0.328237219484284\n",
      "GD (10300/99999): loss=0.3282107527462305\n",
      "GD (10400/99999): loss=0.3281846617035295\n",
      "GD (10500/99999): loss=0.32815893862393053\n",
      "GD (10600/99999): loss=0.32813357599373\n",
      "GD (10700/99999): loss=0.3281085665093468\n",
      "GD (10800/99999): loss=0.32808390306932717\n",
      "GD (10900/99999): loss=0.32805957876674924\n",
      "GD (11000/99999): loss=0.32803558688200507\n",
      "GD (11100/99999): loss=0.3280119208759359\n",
      "GD (11200/99999): loss=0.32798857438330026\n",
      "GD (11300/99999): loss=0.3279655412065522\n",
      "GD (11400/99999): loss=0.327942815309914\n",
      "GD (11500/99999): loss=0.32792039081372565\n",
      "GD (11600/99999): loss=0.3278982619890532\n",
      "GD (11700/99999): loss=0.3278764232525427\n",
      "GD (11800/99999): loss=0.32785486916150525\n",
      "GD (11900/99999): loss=0.3278335944092203\n",
      "GD (12000/99999): loss=0.32781259382044464\n",
      "GD (12100/99999): loss=0.32779186234711594\n",
      "GD (12200/99999): loss=0.32777139506423975\n",
      "GD (12300/99999): loss=0.32775118716595253\n",
      "GD (12400/99999): loss=0.3277312339617459\n",
      "GD (12500/99999): loss=0.3277115308728499\n",
      "GD (12600/99999): loss=0.3276920734287629\n",
      "GD (12700/99999): loss=0.32767285726392165\n",
      "GD (12800/99999): loss=0.3276538781145039\n",
      "GD (12900/99999): loss=0.3276351318153584\n",
      "GD (13000/99999): loss=0.32761661429705397\n",
      "GD (13100/99999): loss=0.3275983215830421\n",
      "GD (13200/99999): loss=0.32758024978693007\n",
      "GD (13300/99999): loss=0.3275623951098539\n",
      "GD (13400/99999): loss=0.3275447538379524\n",
      "GD (13500/99999): loss=0.32752732233993304\n",
      "GD (13600/99999): loss=0.3275100970647278\n",
      "GD (13700/99999): loss=0.32749307453923315\n",
      "GD (13800/99999): loss=0.327476251366132\n",
      "GD (13900/99999): loss=0.32745962422179237\n",
      "GD (14000/99999): loss=0.32744318985424065\n",
      "GD (14100/99999): loss=0.3274269450812037\n",
      "GD (14200/99999): loss=0.32741088678822117\n",
      "GD (14300/99999): loss=0.32739501192681997\n",
      "GD (14400/99999): loss=0.3273793175127515\n",
      "GD (14500/99999): loss=0.3273638006242882\n",
      "GD (14600/99999): loss=0.3273484584005761\n",
      "GD (14700/99999): loss=0.3273332880400423\n",
      "GD (14800/99999): loss=0.3273182867988532\n",
      "GD (14900/99999): loss=0.3273034519894244\n",
      "GD (15000/99999): loss=0.32728878097897735\n",
      "GD (15100/99999): loss=0.32727427118814123\n",
      "GD (15200/99999): loss=0.32725992008960114\n",
      "GD (15300/99999): loss=0.3272457252067864\n",
      "GD (15400/99999): loss=0.32723168411259984\n",
      "GD (15500/99999): loss=0.3272177944281889\n",
      "GD (15600/99999): loss=0.32720405382175\n",
      "GD (15700/99999): loss=0.3271904600073729\n",
      "GD (15800/99999): loss=0.3271770107439181\n",
      "GD (15900/99999): loss=0.32716370383392757\n",
      "GD (16000/99999): loss=0.32715053712256953\n",
      "GD (16100/99999): loss=0.3271375084966125\n",
      "GD (16200/99999): loss=0.32712461588343006\n",
      "GD (16300/99999): loss=0.32711185725003467\n",
      "GD (16400/99999): loss=0.32709923060213975\n",
      "GD (16500/99999): loss=0.3270867339832481\n",
      "GD (16600/99999): loss=0.32707436547376617\n",
      "GD (16700/99999): loss=0.3270621231901444\n",
      "GD (16800/99999): loss=0.32705000528404043\n",
      "GD (16900/99999): loss=0.3270380099415069\n",
      "GD (17000/99999): loss=0.32702613538220177\n",
      "GD (17100/99999): loss=0.3270143798586192\n",
      "GD (17200/99999): loss=0.32700274165534354\n",
      "GD (17300/99999): loss=0.3269912190883216\n",
      "GD (17400/99999): loss=0.326979810504157\n",
      "GD (17500/99999): loss=0.3269685142794217\n",
      "GD (17600/99999): loss=0.32695732881998696\n",
      "GD (17700/99999): loss=0.32694625256037196\n",
      "GD (17800/99999): loss=0.32693528396311033\n",
      "GD (17900/99999): loss=0.32692442151813234\n",
      "GD (18000/99999): loss=0.3269136637421641\n",
      "GD (18100/99999): loss=0.326903009178143\n",
      "GD (18200/99999): loss=0.3268924563946477\n",
      "GD (18300/99999): loss=0.3268820039853423\n",
      "GD (18400/99999): loss=0.32687165056843637\n",
      "GD (18500/99999): loss=0.32686139478615833\n",
      "GD (18600/99999): loss=0.3268512353042412\n",
      "GD (18700/99999): loss=0.3268411708114238\n",
      "GD (18800/99999): loss=0.32683120001896193\n",
      "GD (18900/99999): loss=0.3268213216601539\n",
      "GD (19000/99999): loss=0.32681153448987715\n",
      "GD (19100/99999): loss=0.3268018372841362\n",
      "GD (19200/99999): loss=0.3267922288396234\n",
      "GD (19300/99999): loss=0.3267827079732884\n",
      "GD (19400/99999): loss=0.3267732735219196\n",
      "GD (19500/99999): loss=0.32676392434173623\n",
      "GD (19600/99999): loss=0.3267546593079891\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD (19700/99999): loss=0.3267454773145724\n",
      "GD (19800/99999): loss=0.32673637727364446\n",
      "GD (19900/99999): loss=0.3267273581152574\n",
      "GD (20000/99999): loss=0.3267184187869964\n",
      "GD (20100/99999): loss=0.32670955825362663\n",
      "GD (20200/99999): loss=0.3267007754967499\n",
      "GD (20300/99999): loss=0.32669206951446883\n",
      "GD (20400/99999): loss=0.32668343932105826\n",
      "GD (20500/99999): loss=0.3266748839466463\n",
      "GD (20600/99999): loss=0.32666640243690137\n",
      "GD (20700/99999): loss=0.32665799385272676\n",
      "GD (20800/99999): loss=0.3266496572699633\n",
      "GD (20900/99999): loss=0.3266413917790978\n",
      "GD (21000/99999): loss=0.32663319648497907\n",
      "GD (21100/99999): loss=0.32662507050653977\n",
      "GD (21200/99999): loss=0.32661701297652673\n",
      "GD (21300/99999): loss=0.32660902304123335\n",
      "GD (21400/99999): loss=0.32660109986024294\n",
      "GD (21500/99999): loss=0.32659324260617456\n",
      "GD (21600/99999): loss=0.326585450464436\n",
      "GD (21700/99999): loss=0.3265777226329823\n",
      "GD (21800/99999): loss=0.3265700583220792\n",
      "GD (21900/99999): loss=0.32656245675407275\n",
      "GD (22000/99999): loss=0.326554917163163\n",
      "GD (22100/99999): loss=0.32654743879518366\n",
      "GD (22200/99999): loss=0.3265400209073867\n",
      "GD (22300/99999): loss=0.3265326627682309\n",
      "GD (22400/99999): loss=0.32652536365717577\n",
      "GD (22500/99999): loss=0.3265181228644799\n",
      "GD (22600/99999): loss=0.32651093969100364\n",
      "GD (22700/99999): loss=0.32650381344801566\n",
      "GD (22800/99999): loss=0.32649674345700536\n",
      "GD (22900/99999): loss=0.32648972904949697\n",
      "GD (23000/99999): loss=0.32648276956686945\n",
      "GD (23100/99999): loss=0.32647586436017983\n",
      "GD (23200/99999): loss=0.3264690127899904\n",
      "GD (23300/99999): loss=0.32646221422619887\n",
      "GD (23400/99999): loss=0.3264554680478731\n",
      "GD (23500/99999): loss=0.32644877364308955\n",
      "GD (23600/99999): loss=0.3264421304087735\n",
      "GD (23700/99999): loss=0.3264355377505447\n",
      "GD (23800/99999): loss=0.3264289950825653\n",
      "GD (23900/99999): loss=0.3264225018273904\n",
      "GD (24000/99999): loss=0.3264160574158229\n",
      "GD (24100/99999): loss=0.326409661286771\n",
      "GD (24200/99999): loss=0.3264033128871078\n",
      "GD (24300/99999): loss=0.3263970116715349\n",
      "GD (24400/99999): loss=0.326390757102449\n",
      "GD (24500/99999): loss=0.32638454864980965\n",
      "GD (24600/99999): loss=0.32637838579101114\n",
      "GD (24700/99999): loss=0.32637226801075736\n",
      "GD (24800/99999): loss=0.32636619480093715\n",
      "GD (24900/99999): loss=0.32636016566050446\n",
      "GD (25000/99999): loss=0.32635418009535977\n",
      "GD (25100/99999): loss=0.3263482376182338\n",
      "GD (25200/99999): loss=0.32634233774857413\n",
      "GD (25300/99999): loss=0.3263364800124337\n",
      "GD (25400/99999): loss=0.326330663942362\n",
      "GD (25500/99999): loss=0.3263248890772972\n",
      "GD (25600/99999): loss=0.3263191549624626\n",
      "GD (25700/99999): loss=0.3263134611492628\n",
      "GD (25800/99999): loss=0.326307807195183\n",
      "GD (25900/99999): loss=0.326302192663691\n",
      "GD (26000/99999): loss=0.32629661712413904\n",
      "GD (26100/99999): loss=0.32629108015167047\n",
      "GD (26200/99999): loss=0.3262855813271252\n",
      "GD (26300/99999): loss=0.326280120236949\n",
      "GD (26400/99999): loss=0.32627469647310403\n",
      "GD (26500/99999): loss=0.3262693096329803\n",
      "GD (26600/99999): loss=0.3262639593193106\n",
      "GD (26700/99999): loss=0.3262586451400853\n",
      "GD (26800/99999): loss=0.3262533667084691\n",
      "GD (26900/99999): loss=0.3262481236427209\n",
      "GD (27000/99999): loss=0.32624291556611223\n",
      "GD (27100/99999): loss=0.326237742106851\n",
      "GD (27200/99999): loss=0.3262326028980027\n",
      "GD (27300/99999): loss=0.3262274975774164\n",
      "GD (27400/99999): loss=0.32622242578765015\n",
      "GD (27500/99999): loss=0.32621738717589865\n",
      "GD (27600/99999): loss=0.32621238139392134\n",
      "GD (27700/99999): loss=0.3262074080979733\n",
      "GD (27800/99999): loss=0.32620246694873617\n",
      "GD (27900/99999): loss=0.32619755761125047\n",
      "GD (28000/99999): loss=0.3261926797548498\n",
      "GD (28100/99999): loss=0.3261878330530959\n",
      "GD (28200/99999): loss=0.3261830171837143\n",
      "GD (28300/99999): loss=0.32617823182853223\n",
      "GD (28400/99999): loss=0.32617347667341695\n",
      "GD (28500/99999): loss=0.32616875140821483\n",
      "GD (28600/99999): loss=0.3261640557266932\n",
      "GD (28700/99999): loss=0.3261593893264806\n",
      "GD (28800/99999): loss=0.32615475190901033\n",
      "GD (28900/99999): loss=0.32615014317946456\n",
      "GD (29000/99999): loss=0.32614556284671803\n",
      "GD (29100/99999): loss=0.32614101062328443\n",
      "GD (29200/99999): loss=0.3261364862252635\n",
      "GD (29300/99999): loss=0.3261319893722874\n",
      "GD (29400/99999): loss=0.3261275197874705\n",
      "GD (29500/99999): loss=0.32612307719735795\n",
      "GD (29600/99999): loss=0.3261186613318764\n",
      "GD (29700/99999): loss=0.3261142719242847\n",
      "GD (29800/99999): loss=0.3261099087111263\n",
      "GD (29900/99999): loss=0.32610557143218205\n",
      "GD (30000/99999): loss=0.3261012598304236\n",
      "GD (30100/99999): loss=0.3260969736519674\n",
      "GD (30200/99999): loss=0.3260927126460306\n",
      "GD (30300/99999): loss=0.3260884765648869\n",
      "GD (30400/99999): loss=0.3260842651638226\n",
      "GD (30500/99999): loss=0.32608007820109436\n",
      "GD (30600/99999): loss=0.32607591543788733\n",
      "GD (30700/99999): loss=0.3260717766382741\n",
      "GD (30800/99999): loss=0.3260676615691735\n",
      "GD (30900/99999): loss=0.3260635700003117\n",
      "GD (31000/99999): loss=0.3260595017041823\n",
      "GD (31100/99999): loss=0.32605545645600814\n",
      "GD (31200/99999): loss=0.32605143403370296\n",
      "GD (31300/99999): loss=0.3260474342178349\n",
      "GD (31400/99999): loss=0.32604345679158886\n",
      "GD (31500/99999): loss=0.3260395015407311\n",
      "GD (31600/99999): loss=0.3260355682535734\n",
      "GD (31700/99999): loss=0.3260316567209383\n",
      "GD (31800/99999): loss=0.32602776673612455\n",
      "GD (31900/99999): loss=0.32602389809487314\n",
      "GD (32000/99999): loss=0.3260200505953348\n",
      "GD (32100/99999): loss=0.3260162240380363\n",
      "GD (32200/99999): loss=0.32601241822584864\n",
      "GD (32300/99999): loss=0.3260086329639552\n",
      "GD (32400/99999): loss=0.32600486805982104\n",
      "GD (32500/99999): loss=0.32600112332316117\n",
      "GD (32600/99999): loss=0.32599739856591126\n",
      "GD (32700/99999): loss=0.3259936936021973\n",
      "GD (32800/99999): loss=0.3259900082483064\n",
      "GD (32900/99999): loss=0.3259863423226584\n",
      "GD (33000/99999): loss=0.3259826956457763\n",
      "GD (33100/99999): loss=0.3259790680402599\n",
      "GD (33200/99999): loss=0.3259754593307566\n",
      "GD (33300/99999): loss=0.32597186934393574\n",
      "GD (33400/99999): loss=0.3259682979084608\n",
      "GD (33500/99999): loss=0.3259647448549641\n",
      "GD (33600/99999): loss=0.3259612100160194\n",
      "GD (33700/99999): loss=0.32595769322611856\n",
      "GD (33800/99999): loss=0.32595419432164413\n",
      "GD (33900/99999): loss=0.3259507131408466\n",
      "GD (34000/99999): loss=0.32594724952381854\n",
      "GD (34100/99999): loss=0.3259438033124715\n",
      "GD (34200/99999): loss=0.3259403743505122\n",
      "GD (34300/99999): loss=0.32593696248341897\n",
      "GD (34400/99999): loss=0.3259335675584189\n",
      "GD (34500/99999): loss=0.32593018942446556\n",
      "GD (34600/99999): loss=0.3259268279322164\n",
      "GD (34700/99999): loss=0.32592348293401097\n",
      "GD (34800/99999): loss=0.32592015428384924\n",
      "GD (34900/99999): loss=0.3259168418373703\n",
      "GD (35000/99999): loss=0.3259135454518317\n",
      "GD (35100/99999): loss=0.32591026498608816\n",
      "GD (35200/99999): loss=0.3259070003005716\n",
      "GD (35300/99999): loss=0.3259037512572712\n",
      "GD (35400/99999): loss=0.3259005177197135\n",
      "GD (35500/99999): loss=0.3258972995529426\n",
      "GD (35600/99999): loss=0.325894096623501\n",
      "GD (35700/99999): loss=0.3258909087994115\n",
      "GD (35800/99999): loss=0.32588773595015713\n",
      "GD (35900/99999): loss=0.3258845779466641\n",
      "GD (36000/99999): loss=0.32588143466128267\n",
      "GD (36100/99999): loss=0.3258783059677696\n",
      "GD (36200/99999): loss=0.3258751917412713\n",
      "GD (36300/99999): loss=0.32587209185830474\n",
      "GD (36400/99999): loss=0.3258690061967421\n",
      "GD (36500/99999): loss=0.32586593463579283\n",
      "GD (36600/99999): loss=0.3258628770559871\n",
      "GD (36700/99999): loss=0.3258598333391601\n",
      "GD (36800/99999): loss=0.32585680336843476\n",
      "GD (36900/99999): loss=0.32585378702820655\n",
      "GD (37000/99999): loss=0.3258507842041278\n",
      "GD (37100/99999): loss=0.32584779478309184\n",
      "GD (37200/99999): loss=0.3258448186532175\n",
      "GD (37300/99999): loss=0.3258418557038347\n",
      "GD (37400/99999): loss=0.32583890582546904\n",
      "GD (37500/99999): loss=0.32583596890982747\n",
      "GD (37600/99999): loss=0.32583304484978304\n",
      "GD (37700/99999): loss=0.3258301335393622\n",
      "GD (37800/99999): loss=0.32582723487372894\n",
      "GD (37900/99999): loss=0.325824348749172\n",
      "GD (38000/99999): loss=0.3258214750630909\n",
      "GD (38100/99999): loss=0.3258186137139823\n",
      "GD (38200/99999): loss=0.3258157646014269\n",
      "GD (38300/99999): loss=0.32581292762607644\n",
      "GD (38400/99999): loss=0.32581010268964017\n",
      "GD (38500/99999): loss=0.3258072896948722\n",
      "GD (38600/99999): loss=0.32580448854555943\n",
      "GD (38700/99999): loss=0.32580169914650875\n",
      "GD (38800/99999): loss=0.3257989214035345\n",
      "GD (38900/99999): loss=0.32579615522344685\n",
      "GD (39000/99999): loss=0.32579340051403927\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD (39100/99999): loss=0.3257906571840774\n",
      "GD (39200/99999): loss=0.32578792514328697\n",
      "GD (39300/99999): loss=0.3257852043023422\n",
      "GD (39400/99999): loss=0.3257824945728551\n",
      "GD (39500/99999): loss=0.32577979586736344\n",
      "GD (39600/99999): loss=0.32577710809932076\n",
      "GD (39700/99999): loss=0.32577443118308425\n",
      "GD (39800/99999): loss=0.32577176503390515\n",
      "GD (39900/99999): loss=0.32576910956791727\n",
      "GD (40000/99999): loss=0.325766464702127\n",
      "GD (40100/99999): loss=0.3257638303544033\n",
      "GD (40200/99999): loss=0.3257612064434666\n",
      "GD (40300/99999): loss=0.3257585928888796\n",
      "GD (40400/99999): loss=0.32575598961103663\n",
      "GD (40500/99999): loss=0.32575339653115465\n",
      "GD (40600/99999): loss=0.3257508135712628\n",
      "GD (40700/99999): loss=0.3257482406541934\n",
      "GD (40800/99999): loss=0.32574567770357254\n",
      "GD (40900/99999): loss=0.32574312464381006\n",
      "GD (41000/99999): loss=0.3257405814000914\n",
      "GD (41100/99999): loss=0.3257380478983677\n",
      "GD (41200/99999): loss=0.3257355240653477\n",
      "GD (41300/99999): loss=0.3257330098284879\n",
      "GD (41400/99999): loss=0.32573050511598445\n",
      "GD (41500/99999): loss=0.32572800985676487\n",
      "GD (41600/99999): loss=0.32572552398047855\n",
      "GD (41700/99999): loss=0.32572304741748964\n",
      "GD (41800/99999): loss=0.32572058009886756\n",
      "GD (41900/99999): loss=0.32571812195637967\n",
      "GD (42000/99999): loss=0.3257156729224828\n",
      "GD (42100/99999): loss=0.3257132329303154\n",
      "GD (42200/99999): loss=0.3257108019136899\n",
      "GD (42300/99999): loss=0.3257083798070844\n",
      "GD (42400/99999): loss=0.3257059665456353\n",
      "GD (42500/99999): loss=0.3257035620651302\n",
      "GD (42600/99999): loss=0.32570116630199936\n",
      "GD (42700/99999): loss=0.3256987791933098\n",
      "GD (42800/99999): loss=0.32569640067675637\n",
      "GD (42900/99999): loss=0.32569403069065567\n",
      "GD (43000/99999): loss=0.32569166917393855\n",
      "GD (43100/99999): loss=0.32568931606614343\n",
      "GD (43200/99999): loss=0.32568697130740865\n",
      "GD (43300/99999): loss=0.32568463483846655\n",
      "GD (43400/99999): loss=0.32568230660063613\n",
      "GD (43500/99999): loss=0.3256799865358161\n",
      "GD (43600/99999): loss=0.32567767458647945\n",
      "GD (43700/99999): loss=0.3256753706956653\n",
      "GD (43800/99999): loss=0.3256730748069744\n",
      "GD (43900/99999): loss=0.3256707868645605\n",
      "GD (44000/99999): loss=0.3256685068131268\n",
      "GD (44100/99999): loss=0.32566623459791705\n",
      "GD (44200/99999): loss=0.32566397016471127\n",
      "GD (44300/99999): loss=0.32566171345981887\n",
      "GD (44400/99999): loss=0.32565946443007326\n",
      "GD (44500/99999): loss=0.3256572230228252\n",
      "GD (44600/99999): loss=0.3256549891859379\n",
      "GD (44700/99999): loss=0.32565276286778055\n",
      "GD (44800/99999): loss=0.3256505440172228\n",
      "GD (44900/99999): loss=0.32564833258362935\n",
      "GD (45000/99999): loss=0.3256461285168543\n",
      "GD (45100/99999): loss=0.3256439317672357\n",
      "GD (45200/99999): loss=0.32564174228559006\n",
      "GD (45300/99999): loss=0.32563956002320704\n",
      "GD (45400/99999): loss=0.3256373849318444\n",
      "GD (45500/99999): loss=0.3256352169637226\n",
      "GD (45600/99999): loss=0.3256330560715194\n",
      "GD (45700/99999): loss=0.3256309022083653\n",
      "GD (45800/99999): loss=0.32562875532783814\n",
      "GD (45900/99999): loss=0.3256266153839583\n",
      "GD (46000/99999): loss=0.32562448233118374\n",
      "GD (46100/99999): loss=0.32562235612440515\n",
      "GD (46200/99999): loss=0.32562023671894125\n",
      "GD (46300/99999): loss=0.32561812407053414\n",
      "GD (46400/99999): loss=0.3256160181353443\n",
      "GD (46500/99999): loss=0.325613918869946\n",
      "GD (46600/99999): loss=0.3256118262313234\n",
      "GD (46700/99999): loss=0.32560974017686517\n",
      "GD (46800/99999): loss=0.32560766066436075\n",
      "GD (46900/99999): loss=0.3256055876519954\n",
      "GD (47000/99999): loss=0.3256035210983463\n",
      "GD (47100/99999): loss=0.32560146096237785\n",
      "GD (47200/99999): loss=0.32559940720343794\n",
      "GD (47300/99999): loss=0.325597359781253\n",
      "GD (47400/99999): loss=0.3255953186559249\n",
      "GD (47500/99999): loss=0.32559328378792596\n",
      "GD (47600/99999): loss=0.32559125513809534\n",
      "GD (47700/99999): loss=0.3255892326676347\n",
      "GD (47800/99999): loss=0.325587216338105\n",
      "GD (47900/99999): loss=0.32558520611142155\n",
      "GD (48000/99999): loss=0.32558320194985124\n",
      "GD (48100/99999): loss=0.3255812038160079\n",
      "GD (48200/99999): loss=0.3255792116728487\n",
      "GD (48300/99999): loss=0.3255772254836709\n",
      "GD (48400/99999): loss=0.32557524521210796\n",
      "GD (48500/99999): loss=0.3255732708221251\n",
      "GD (48600/99999): loss=0.3255713022780174\n",
      "GD (48700/99999): loss=0.32556933954440453\n",
      "GD (48800/99999): loss=0.32556738258622825\n",
      "GD (48900/99999): loss=0.3255654313687488\n",
      "GD (49000/99999): loss=0.32556348585754114\n",
      "GD (49100/99999): loss=0.325561546018492\n",
      "GD (49200/99999): loss=0.32555961181779614\n",
      "GD (49300/99999): loss=0.32555768322195366\n",
      "GD (49400/99999): loss=0.32555576019776566\n",
      "GD (49500/99999): loss=0.3255538427123323\n",
      "GD (49600/99999): loss=0.3255519307330486\n",
      "GD (49700/99999): loss=0.3255500242276019\n",
      "GD (49800/99999): loss=0.3255481231639685\n",
      "GD (49900/99999): loss=0.3255462275104103\n",
      "GD (50000/99999): loss=0.32554433723547277\n",
      "GD (50100/99999): loss=0.3255424523079805\n",
      "GD (50200/99999): loss=0.32554057269703535\n",
      "GD (50300/99999): loss=0.32553869837201305\n",
      "GD (50400/99999): loss=0.3255368293025603\n",
      "GD (50500/99999): loss=0.3255349654585923\n",
      "GD (50600/99999): loss=0.32553310681028924\n",
      "GD (50700/99999): loss=0.3255312533280939\n",
      "GD (50800/99999): loss=0.32552940498270905\n",
      "GD (50900/99999): loss=0.3255275617450944\n",
      "GD (51000/99999): loss=0.32552572358646387\n",
      "GD (51100/99999): loss=0.3255238904782833\n",
      "GD (51200/99999): loss=0.32552206239226716\n",
      "GD (51300/99999): loss=0.32552023930037666\n",
      "GD (51400/99999): loss=0.3255184211748166\n",
      "GD (51500/99999): loss=0.32551660798803306\n",
      "GD (51600/99999): loss=0.32551479971271124\n",
      "GD (51700/99999): loss=0.32551299632177205\n",
      "GD (51800/99999): loss=0.32551119778837045\n",
      "GD (51900/99999): loss=0.3255094040858924\n",
      "GD (52000/99999): loss=0.32550761518795335\n",
      "GD (52100/99999): loss=0.3255058310683948\n",
      "GD (52200/99999): loss=0.32550405170128294\n",
      "GD (52300/99999): loss=0.32550227706090484\n",
      "GD (52400/99999): loss=0.3255005071217683\n",
      "GD (52500/99999): loss=0.3254987418585976\n",
      "GD (52600/99999): loss=0.3254969812463325\n",
      "GD (52700/99999): loss=0.3254952252601252\n",
      "GD (52800/99999): loss=0.32549347387533856\n",
      "GD (52900/99999): loss=0.325491727067544\n",
      "GD (53000/99999): loss=0.32548998481251934\n",
      "GD (53100/99999): loss=0.3254882470862461\n",
      "GD (53200/99999): loss=0.3254865138649082\n",
      "GD (53300/99999): loss=0.32548478512488954\n",
      "GD (53400/99999): loss=0.3254830608427717\n",
      "GD (53500/99999): loss=0.32548134099533227\n",
      "GD (53600/99999): loss=0.32547962555954285\n",
      "GD (53700/99999): loss=0.32547791451256675\n",
      "GD (53800/99999): loss=0.3254762078317573\n",
      "GD (53900/99999): loss=0.3254745054946557\n",
      "GD (54000/99999): loss=0.3254728074789892\n",
      "GD (54100/99999): loss=0.32547111376266974\n",
      "GD (54200/99999): loss=0.3254694243237907\n",
      "GD (54300/99999): loss=0.3254677391406265\n",
      "GD (54400/99999): loss=0.3254660581916301\n",
      "GD (54500/99999): loss=0.3254643814554313\n",
      "GD (54600/99999): loss=0.32546270891083445\n",
      "GD (54700/99999): loss=0.3254610405368176\n",
      "GD (54800/99999): loss=0.32545937631253025\n",
      "GD (54900/99999): loss=0.3254577162172914\n",
      "GD (55000/99999): loss=0.32545606023058843\n",
      "GD (55100/99999): loss=0.32545440833207445\n",
      "GD (55200/99999): loss=0.32545276050156824\n",
      "GD (55300/99999): loss=0.32545111671905047\n",
      "GD (55400/99999): loss=0.32544947696466386\n",
      "GD (55500/99999): loss=0.3254478412187108\n",
      "GD (55600/99999): loss=0.3254462094616516\n",
      "GD (55700/99999): loss=0.3254445816741034\n",
      "GD (55800/99999): loss=0.3254429578368382\n",
      "GD (55900/99999): loss=0.3254413379307813\n",
      "GD (56000/99999): loss=0.32543972193701026\n",
      "GD (56100/99999): loss=0.32543810983675264\n",
      "GD (56200/99999): loss=0.32543650161138543\n",
      "GD (56300/99999): loss=0.32543489724243246\n",
      "GD (56400/99999): loss=0.3254332967115639\n",
      "GD (56500/99999): loss=0.3254317000005943\n",
      "GD (56600/99999): loss=0.3254301070914813\n",
      "GD (56700/99999): loss=0.32542851796632394\n",
      "GD (56800/99999): loss=0.32542693260736216\n",
      "GD (56900/99999): loss=0.32542535099697434\n",
      "GD (57000/99999): loss=0.3254237731176761\n",
      "GD (57100/99999): loss=0.32542219895211993\n",
      "GD (57200/99999): loss=0.3254206284830922\n",
      "GD (57300/99999): loss=0.32541906169351364\n",
      "GD (57400/99999): loss=0.3254174985664367\n",
      "GD (57500/99999): loss=0.3254159390850448\n",
      "GD (57600/99999): loss=0.32541438323265065\n",
      "GD (57700/99999): loss=0.32541283099269597\n",
      "GD (57800/99999): loss=0.32541128234874883\n",
      "GD (57900/99999): loss=0.3254097372845035\n",
      "GD (58000/99999): loss=0.3254081957837787\n",
      "GD (58100/99999): loss=0.32540665783051664\n",
      "GD (58200/99999): loss=0.32540512340878186\n",
      "GD (58300/99999): loss=0.32540359250275946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD (58400/99999): loss=0.32540206509675473\n",
      "GD (58500/99999): loss=0.32540054117519146\n",
      "GD (58600/99999): loss=0.325399020722611\n",
      "GD (58700/99999): loss=0.32539750372367104\n",
      "GD (58800/99999): loss=0.32539599016314463\n",
      "GD (58900/99999): loss=0.32539448002591886\n",
      "GD (59000/99999): loss=0.32539297329699374\n",
      "GD (59100/99999): loss=0.3253914699614815\n",
      "GD (59200/99999): loss=0.32538997000460507\n",
      "GD (59300/99999): loss=0.32538847341169713\n",
      "GD (59400/99999): loss=0.3253869801681995\n",
      "GD (59500/99999): loss=0.3253854902596611\n",
      "GD (59600/99999): loss=0.3253840036717381\n",
      "GD (59700/99999): loss=0.3253825203901919\n",
      "GD (59800/99999): loss=0.32538104040088883\n",
      "GD (59900/99999): loss=0.3253795636897988\n",
      "GD (60000/99999): loss=0.3253780902429944\n",
      "GD (60100/99999): loss=0.3253766200466502\n",
      "GD (60200/99999): loss=0.32537515308704074\n",
      "GD (60300/99999): loss=0.3253736893505412\n",
      "GD (60400/99999): loss=0.3253722288236248\n",
      "GD (60500/99999): loss=0.3253707714928635\n",
      "GD (60600/99999): loss=0.3253693173449257\n",
      "GD (60700/99999): loss=0.3253678663665757\n",
      "GD (60800/99999): loss=0.32536641854467346\n",
      "GD (60900/99999): loss=0.32536497386617297\n",
      "GD (61000/99999): loss=0.32536353231812143\n",
      "GD (61100/99999): loss=0.3253620938876588\n",
      "GD (61200/99999): loss=0.3253606585620164\n",
      "GD (61300/99999): loss=0.32535922632851666\n",
      "GD (61400/99999): loss=0.3253577971745717\n",
      "GD (61500/99999): loss=0.3253563710876827\n",
      "GD (61600/99999): loss=0.3253549480554392\n",
      "GD (61700/99999): loss=0.32535352806551804\n",
      "GD (61800/99999): loss=0.3253521111056831\n",
      "GD (61900/99999): loss=0.32535069716378334\n",
      "GD (62000/99999): loss=0.3253492862277535\n",
      "GD (62100/99999): loss=0.325347878285612\n",
      "GD (62200/99999): loss=0.32534647332546096\n",
      "GD (62300/99999): loss=0.32534507133548496\n",
      "GD (62400/99999): loss=0.32534367230395106\n",
      "GD (62500/99999): loss=0.3253422762192069\n",
      "GD (62600/99999): loss=0.32534088306968073\n",
      "GD (62700/99999): loss=0.3253394928438806\n",
      "GD (62800/99999): loss=0.32533810553039316\n",
      "GD (62900/99999): loss=0.32533672111788375\n",
      "GD (63000/99999): loss=0.3253353395950948\n",
      "GD (63100/99999): loss=0.32533396095084594\n",
      "GD (63200/99999): loss=0.32533258517403274\n",
      "GD (63300/99999): loss=0.32533121225362577\n",
      "GD (63400/99999): loss=0.32532984217867095\n",
      "GD (63500/99999): loss=0.32532847493828804\n",
      "GD (63600/99999): loss=0.32532711052166996\n",
      "GD (63700/99999): loss=0.3253257489180824\n",
      "GD (63800/99999): loss=0.32532439011686354\n",
      "GD (63900/99999): loss=0.3253230341074223\n",
      "GD (64000/99999): loss=0.32532168087923863\n",
      "GD (64100/99999): loss=0.3253203304218628\n",
      "GD (64200/99999): loss=0.32531898272491416\n",
      "GD (64300/99999): loss=0.3253176377780815\n",
      "GD (64400/99999): loss=0.32531629557112113\n",
      "GD (64500/99999): loss=0.3253149560938575\n",
      "GD (64600/99999): loss=0.325313619336182\n",
      "GD (64700/99999): loss=0.32531228528805217\n",
      "GD (64800/99999): loss=0.3253109539394919\n",
      "GD (64900/99999): loss=0.3253096252805898\n",
      "GD (65000/99999): loss=0.3253082993014995\n",
      "GD (65100/99999): loss=0.32530697599243863\n",
      "GD (65200/99999): loss=0.32530565534368816\n",
      "GD (65300/99999): loss=0.3253043373455923\n",
      "GD (65400/99999): loss=0.32530302198855765\n",
      "GD (65500/99999): loss=0.3253017092630522\n",
      "GD (65600/99999): loss=0.3253003991596059\n",
      "GD (65700/99999): loss=0.3252990916688093\n",
      "GD (65800/99999): loss=0.32529778678131277\n",
      "GD (65900/99999): loss=0.3252964844878269\n",
      "GD (66000/99999): loss=0.3252951847791211\n",
      "GD (66100/99999): loss=0.3252938876460238\n",
      "GD (66200/99999): loss=0.3252925930794211\n",
      "GD (66300/99999): loss=0.32529130107025717\n",
      "GD (66400/99999): loss=0.3252900116095331\n",
      "GD (66500/99999): loss=0.32528872468830683\n",
      "GD (66600/99999): loss=0.32528744029769213\n",
      "GD (66700/99999): loss=0.32528615842885894\n",
      "GD (66800/99999): loss=0.32528487907303166\n",
      "GD (66900/99999): loss=0.3252836022214902\n",
      "GD (67000/99999): loss=0.32528232786556804\n",
      "GD (67100/99999): loss=0.32528105599665263\n",
      "GD (67200/99999): loss=0.32527978660618523\n",
      "GD (67300/99999): loss=0.325278519685659\n",
      "GD (67400/99999): loss=0.32527725522662043\n",
      "GD (67500/99999): loss=0.32527599322066725\n",
      "GD (67600/99999): loss=0.3252747336594492\n",
      "GD (67700/99999): loss=0.32527347653466654\n",
      "GD (67800/99999): loss=0.3252722218380707\n",
      "GD (67900/99999): loss=0.3252709695614629\n",
      "GD (68000/99999): loss=0.3252697196966944\n",
      "GD (68100/99999): loss=0.32526847223566574\n",
      "GD (68200/99999): loss=0.3252672271703262\n",
      "GD (68300/99999): loss=0.3252659844926737\n",
      "GD (68400/99999): loss=0.32526474419475443\n",
      "GD (68500/99999): loss=0.325263506268662\n",
      "GD (68600/99999): loss=0.3252622707065375\n",
      "GD (68700/99999): loss=0.32526103750056856\n",
      "GD (68800/99999): loss=0.32525980664299\n",
      "GD (68900/99999): loss=0.32525857812608205\n",
      "GD (69000/99999): loss=0.32525735194217104\n",
      "GD (69100/99999): loss=0.32525612808362864\n",
      "GD (69200/99999): loss=0.3252549065428712\n",
      "GD (69300/99999): loss=0.3252536873123602\n",
      "GD (69400/99999): loss=0.32525247038460064\n",
      "GD (69500/99999): loss=0.32525125575214225\n",
      "GD (69600/99999): loss=0.325250043407577\n",
      "GD (69700/99999): loss=0.3252488333435414\n",
      "GD (69800/99999): loss=0.32524762555271364\n",
      "GD (69900/99999): loss=0.32524642002781495\n",
      "GD (70000/99999): loss=0.3252452167616085\n",
      "GD (70100/99999): loss=0.3252440157468989\n",
      "GD (70200/99999): loss=0.3252428169765324\n",
      "GD (70300/99999): loss=0.3252416204433963\n",
      "GD (70400/99999): loss=0.3252404261404186\n",
      "GD (70500/99999): loss=0.3252392340605675\n",
      "GD (70600/99999): loss=0.32523804419685154\n",
      "GD (70700/99999): loss=0.32523685654231865\n",
      "GD (70800/99999): loss=0.32523567109005647\n",
      "GD (70900/99999): loss=0.32523448783319125\n",
      "GD (71000/99999): loss=0.3252333067648887\n",
      "GD (71100/99999): loss=0.32523212787835226\n",
      "GD (71200/99999): loss=0.32523095116682416\n",
      "GD (71300/99999): loss=0.3252297766235839\n",
      "GD (71400/99999): loss=0.3252286042419486\n",
      "GD (71500/99999): loss=0.3252274340152731\n",
      "GD (71600/99999): loss=0.32522626593694826\n",
      "GD (71700/99999): loss=0.3252251000004027\n",
      "GD (71800/99999): loss=0.3252239361991004\n",
      "GD (71900/99999): loss=0.3252227745265418\n",
      "GD (72000/99999): loss=0.3252216149762632\n",
      "GD (72100/99999): loss=0.32522045754183593\n",
      "GD (72200/99999): loss=0.32521930221686707\n",
      "GD (72300/99999): loss=0.32521814899499824\n",
      "GD (72400/99999): loss=0.32521699786990577\n",
      "GD (72500/99999): loss=0.3252158488353004\n",
      "GD (72600/99999): loss=0.3252147018849269\n",
      "GD (72700/99999): loss=0.325213557012564\n",
      "GD (72800/99999): loss=0.3252124142120238\n",
      "GD (72900/99999): loss=0.32521127347715184\n",
      "GD (73000/99999): loss=0.32521013480182653\n",
      "GD (73100/99999): loss=0.3252089981799592\n",
      "GD (73200/99999): loss=0.32520786360549353\n",
      "GD (73300/99999): loss=0.32520673107240583\n",
      "GD (73400/99999): loss=0.3252056005747039\n",
      "GD (73500/99999): loss=0.32520447210642783\n",
      "GD (73600/99999): loss=0.32520334566164877\n",
      "GD (73700/99999): loss=0.32520222123446924\n",
      "GD (73800/99999): loss=0.3252010988190232\n",
      "GD (73900/99999): loss=0.3251999784094749\n",
      "GD (74000/99999): loss=0.3251988600000193\n",
      "GD (74100/99999): loss=0.3251977435848819\n",
      "GD (74200/99999): loss=0.32519662915831793\n",
      "GD (74300/99999): loss=0.3251955167146127\n",
      "GD (74400/99999): loss=0.32519440624808116\n",
      "GD (74500/99999): loss=0.3251932977530676\n",
      "GD (74600/99999): loss=0.32519219122394527\n",
      "GD (74700/99999): loss=0.32519108665511687\n",
      "GD (74800/99999): loss=0.3251899840410134\n",
      "GD (74900/99999): loss=0.3251888833760945\n",
      "GD (75000/99999): loss=0.32518778465484827\n",
      "GD (75100/99999): loss=0.3251866878717907\n",
      "GD (75200/99999): loss=0.3251855930214657\n",
      "GD (75300/99999): loss=0.3251845000984451\n",
      "GD (75400/99999): loss=0.3251834090973276\n",
      "GD (75500/99999): loss=0.32518232001273983\n",
      "GD (75600/99999): loss=0.3251812328393348\n",
      "GD (75700/99999): loss=0.3251801475717932\n",
      "GD (75800/99999): loss=0.32517906420482146\n",
      "GD (75900/99999): loss=0.325177982733153\n",
      "GD (76000/99999): loss=0.32517690315154757\n",
      "GD (76100/99999): loss=0.3251758254547907\n",
      "GD (76200/99999): loss=0.3251747496376936\n",
      "GD (76300/99999): loss=0.32517367569509364\n",
      "GD (76400/99999): loss=0.3251726036218533\n",
      "GD (76500/99999): loss=0.32517153341286065\n",
      "GD (76600/99999): loss=0.3251704650630287\n",
      "GD (76700/99999): loss=0.3251693985672951\n",
      "GD (76800/99999): loss=0.325168333920623\n",
      "GD (76900/99999): loss=0.32516727111799953\n",
      "GD (77000/99999): loss=0.32516621015443603\n",
      "GD (77100/99999): loss=0.32516515102496857\n",
      "GD (77200/99999): loss=0.3251640937246567\n",
      "GD (77300/99999): loss=0.32516303824858467\n",
      "GD (77400/99999): loss=0.32516198459185947\n",
      "GD (77500/99999): loss=0.325160932749612\n",
      "GD (77600/99999): loss=0.32515988271699664\n",
      "GD (77700/99999): loss=0.3251588344891905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD (77800/99999): loss=0.32515778806139445\n",
      "GD (77900/99999): loss=0.3251567434288312\n",
      "GD (78000/99999): loss=0.32515570058674714\n",
      "GD (78100/99999): loss=0.32515465953041045\n",
      "GD (78200/99999): loss=0.32515362025511174\n",
      "GD (78300/99999): loss=0.32515258275616427\n",
      "GD (78400/99999): loss=0.32515154702890287\n",
      "GD (78500/99999): loss=0.32515051306868437\n",
      "GD (78600/99999): loss=0.3251494808708875\n",
      "GD (78700/99999): loss=0.3251484504309123\n",
      "GD (78800/99999): loss=0.32514742174418026\n",
      "GD (78900/99999): loss=0.32514639480613455\n",
      "GD (79000/99999): loss=0.32514536961223883\n",
      "GD (79100/99999): loss=0.32514434615797805\n",
      "GD (79200/99999): loss=0.3251433244388579\n",
      "GD (79300/99999): loss=0.32514230445040476\n",
      "GD (79400/99999): loss=0.32514128618816573\n",
      "GD (79500/99999): loss=0.32514026964770804\n",
      "GD (79600/99999): loss=0.32513925482461903\n",
      "GD (79700/99999): loss=0.3251382417145066\n",
      "GD (79800/99999): loss=0.32513723031299846\n",
      "GD (79900/99999): loss=0.3251362206157418\n",
      "GD (80000/99999): loss=0.3251352126184038\n",
      "GD (80100/99999): loss=0.32513420631667106\n",
      "GD (80200/99999): loss=0.3251332017062501\n",
      "GD (80300/99999): loss=0.3251321987828655\n",
      "GD (80400/99999): loss=0.32513119754226255\n",
      "GD (80500/99999): loss=0.3251301979802044\n",
      "GD (80600/99999): loss=0.3251292000924735\n",
      "GD (80700/99999): loss=0.32512820387487124\n",
      "GD (80800/99999): loss=0.32512720932321726\n",
      "GD (80900/99999): loss=0.3251262164333499\n",
      "GD (81000/99999): loss=0.32512522520112586\n",
      "GD (81100/99999): loss=0.32512423562242015\n",
      "GD (81200/99999): loss=0.3251232476931258\n",
      "GD (81300/99999): loss=0.3251222614091542\n",
      "GD (81400/99999): loss=0.3251212767664338\n",
      "GD (81500/99999): loss=0.3251202937609118\n",
      "GD (81600/99999): loss=0.32511931238855224\n",
      "GD (81700/99999): loss=0.3251183326453374\n",
      "GD (81800/99999): loss=0.3251173545272666\n",
      "GD (81900/99999): loss=0.3251163780303563\n",
      "GD (82000/99999): loss=0.3251154031506403\n",
      "GD (82100/99999): loss=0.32511442988416983\n",
      "GD (82200/99999): loss=0.32511345822701226\n",
      "GD (82300/99999): loss=0.3251124881752526\n",
      "GD (82400/99999): loss=0.3251115197249921\n",
      "GD (82500/99999): loss=0.32511055287234913\n",
      "GD (82600/99999): loss=0.32510958761345776\n",
      "GD (82700/99999): loss=0.3251086239444691\n",
      "GD (82800/99999): loss=0.32510766186155055\n",
      "GD (82900/99999): loss=0.32510670136088543\n",
      "GD (83000/99999): loss=0.32510574243867324\n",
      "GD (83100/99999): loss=0.3251047850911296\n",
      "GD (83200/99999): loss=0.3251038293144857\n",
      "GD (83300/99999): loss=0.32510287510498853\n",
      "GD (83400/99999): loss=0.32510192245890124\n",
      "GD (83500/99999): loss=0.325100971372502\n",
      "GD (83600/99999): loss=0.32510002184208464\n",
      "GD (83700/99999): loss=0.32509907386395825\n",
      "GD (83800/99999): loss=0.3250981274344476\n",
      "GD (83900/99999): loss=0.32509718254989195\n",
      "GD (84000/99999): loss=0.32509623920664615\n",
      "GD (84100/99999): loss=0.32509529740107984\n",
      "GD (84200/99999): loss=0.32509435712957774\n",
      "GD (84300/99999): loss=0.3250934183885387\n",
      "GD (84400/99999): loss=0.32509248117437733\n",
      "GD (84500/99999): loss=0.325091545483522\n",
      "GD (84600/99999): loss=0.3250906113124157\n",
      "GD (84700/99999): loss=0.325089678657516\n",
      "GD (84800/99999): loss=0.32508874751529476\n",
      "GD (84900/99999): loss=0.3250878178822379\n",
      "GD (85000/99999): loss=0.32508688975484606\n",
      "GD (85100/99999): loss=0.3250859631296331\n",
      "GD (85200/99999): loss=0.3250850380031275\n",
      "GD (85300/99999): loss=0.32508411437187135\n",
      "GD (85400/99999): loss=0.3250831922324201\n",
      "GD (85500/99999): loss=0.325082271581344\n",
      "GD (85600/99999): loss=0.32508135241522595\n",
      "GD (85700/99999): loss=0.3250804347306625\n",
      "GD (85800/99999): loss=0.3250795185242641\n",
      "GD (85900/99999): loss=0.3250786037926543\n",
      "GD (86000/99999): loss=0.3250776905324698\n",
      "GD (86100/99999): loss=0.32507677874036073\n",
      "GD (86200/99999): loss=0.3250758684129903\n",
      "GD (86300/99999): loss=0.32507495954703464\n",
      "GD (86400/99999): loss=0.3250740521391829\n",
      "GD (86500/99999): loss=0.32507314618613714\n",
      "GD (86600/99999): loss=0.32507224168461213\n",
      "GD (86700/99999): loss=0.32507133863133547\n",
      "GD (86800/99999): loss=0.3250704370230475\n",
      "GD (86900/99999): loss=0.32506953685650086\n",
      "GD (87000/99999): loss=0.3250686381284608\n",
      "GD (87100/99999): loss=0.3250677408357051\n",
      "GD (87200/99999): loss=0.32506684497502375\n",
      "GD (87300/99999): loss=0.32506595054321885\n",
      "GD (87400/99999): loss=0.32506505753710524\n",
      "GD (87500/99999): loss=0.3250641659535094\n",
      "GD (87600/99999): loss=0.32506327578926963\n",
      "GD (87700/99999): loss=0.3250623870412371\n",
      "GD (87800/99999): loss=0.325061499706274\n",
      "GD (87900/99999): loss=0.3250606137812549\n",
      "GD (88000/99999): loss=0.32505972926306576\n",
      "GD (88100/99999): loss=0.32505884614860453\n",
      "GD (88200/99999): loss=0.32505796443478063\n",
      "GD (88300/99999): loss=0.32505708411851486\n",
      "GD (88400/99999): loss=0.32505620519674006\n",
      "GD (88500/99999): loss=0.3250553276663997\n",
      "GD (88600/99999): loss=0.32505445152444945\n",
      "GD (88700/99999): loss=0.3250535767678557\n",
      "GD (88800/99999): loss=0.32505270339359604\n",
      "GD (88900/99999): loss=0.3250518313986596\n",
      "GD (89000/99999): loss=0.32505096078004625\n",
      "GD (89100/99999): loss=0.3250500915347671\n",
      "GD (89200/99999): loss=0.3250492236598439\n",
      "GD (89300/99999): loss=0.3250483571523099\n",
      "GD (89400/99999): loss=0.32504749200920874\n",
      "GD (89500/99999): loss=0.3250466282275944\n",
      "GD (89600/99999): loss=0.3250457658045326\n",
      "GD (89700/99999): loss=0.32504490473709896\n",
      "GD (89800/99999): loss=0.32504404502237993\n",
      "GD (89900/99999): loss=0.32504318665747234\n",
      "GD (90000/99999): loss=0.3250423296394838\n",
      "GD (90100/99999): loss=0.3250414739655319\n",
      "GD (90200/99999): loss=0.32504061963274505\n",
      "GD (90300/99999): loss=0.32503976663826123\n",
      "GD (90400/99999): loss=0.3250389149792296\n",
      "GD (90500/99999): loss=0.32503806465280866\n",
      "GD (90600/99999): loss=0.3250372156561674\n",
      "GD (90700/99999): loss=0.3250363679864849\n",
      "GD (90800/99999): loss=0.32503552164094995\n",
      "GD (90900/99999): loss=0.3250346766167618\n",
      "GD (91000/99999): loss=0.3250338329111288\n",
      "GD (91100/99999): loss=0.3250329905212699\n",
      "GD (91200/99999): loss=0.3250321494444136\n",
      "GD (91300/99999): loss=0.3250313096777979\n",
      "GD (91400/99999): loss=0.32503047121867035\n",
      "GD (91500/99999): loss=0.32502963406428875\n",
      "GD (91600/99999): loss=0.3250287982119197\n",
      "GD (91700/99999): loss=0.32502796365884\n",
      "GD (91800/99999): loss=0.32502713040233544\n",
      "GD (91900/99999): loss=0.3250262984397012\n",
      "GD (92000/99999): loss=0.325025467768242\n",
      "GD (92100/99999): loss=0.325024638385272\n",
      "GD (92200/99999): loss=0.32502381028811395\n",
      "GD (92300/99999): loss=0.32502298347410086\n",
      "GD (92400/99999): loss=0.32502215794057393\n",
      "GD (92500/99999): loss=0.3250213336848835\n",
      "GD (92600/99999): loss=0.3250205107043898\n",
      "GD (92700/99999): loss=0.3250196889964613\n",
      "GD (92800/99999): loss=0.32501886855847545\n",
      "GD (92900/99999): loss=0.32501804938781886\n",
      "GD (93000/99999): loss=0.32501723148188705\n",
      "GD (93100/99999): loss=0.3250164148380839\n",
      "GD (93200/99999): loss=0.3250155994538227\n",
      "GD (93300/99999): loss=0.3250147853265246\n",
      "GD (93400/99999): loss=0.32501397245362024\n",
      "GD (93500/99999): loss=0.32501316083254833\n",
      "GD (93600/99999): loss=0.3250123504607565\n",
      "GD (93700/99999): loss=0.3250115413357005\n",
      "GD (93800/99999): loss=0.3250107334548451\n",
      "GD (93900/99999): loss=0.3250099268156627\n",
      "GD (94000/99999): loss=0.32500912141563515\n",
      "GD (94100/99999): loss=0.32500831725225166\n",
      "GD (94200/99999): loss=0.3250075143230103\n",
      "GD (94300/99999): loss=0.32500671262541714\n",
      "GD (94400/99999): loss=0.3250059121569867\n",
      "GD (94500/99999): loss=0.32500511291524126\n",
      "GD (94600/99999): loss=0.3250043148977117\n",
      "GD (94700/99999): loss=0.3250035181019366\n",
      "GD (94800/99999): loss=0.3250027225254629\n",
      "GD (94900/99999): loss=0.3250019281658452\n",
      "GD (95000/99999): loss=0.32500113502064637\n",
      "GD (95100/99999): loss=0.3250003430874369\n",
      "GD (95200/99999): loss=0.3249995523637954\n",
      "GD (95300/99999): loss=0.3249987628473082\n",
      "GD (95400/99999): loss=0.3249979745355694\n",
      "GD (95500/99999): loss=0.32499718742618083\n",
      "GD (95600/99999): loss=0.3249964015167521\n",
      "GD (95700/99999): loss=0.32499561680490047\n",
      "GD (95800/99999): loss=0.3249948332882509\n",
      "GD (95900/99999): loss=0.32499405096443573\n",
      "GD (96000/99999): loss=0.32499326983109506\n",
      "GD (96100/99999): loss=0.32499248988587637\n",
      "GD (96200/99999): loss=0.32499171112643466\n",
      "GD (96300/99999): loss=0.32499093355043246\n",
      "GD (96400/99999): loss=0.32499015715553947\n",
      "GD (96500/99999): loss=0.3249893819394331\n",
      "GD (96600/99999): loss=0.32498860789979783\n",
      "GD (96700/99999): loss=0.3249878350343254\n",
      "GD (96800/99999): loss=0.3249870633407148\n",
      "GD (96900/99999): loss=0.32498629281667235\n",
      "GD (97000/99999): loss=0.3249855234599116\n",
      "GD (97100/99999): loss=0.3249847552681532\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD (97200/99999): loss=0.32498398823912467\n",
      "GD (97300/99999): loss=0.3249832223705609\n",
      "GD (97400/99999): loss=0.32498245766020356\n",
      "GD (97500/99999): loss=0.3249816941058015\n",
      "GD (97600/99999): loss=0.32498093170511066\n",
      "GD (97700/99999): loss=0.32498017045589345\n",
      "GD (97800/99999): loss=0.32497941035591965\n",
      "GD (97900/99999): loss=0.32497865140296567\n",
      "GD (98000/99999): loss=0.3249778935948146\n",
      "GD (98100/99999): loss=0.324977136929257\n",
      "GD (98200/99999): loss=0.32497638140408897\n",
      "GD (98300/99999): loss=0.32497562701711474\n",
      "GD (98400/99999): loss=0.3249748737661442\n",
      "GD (98500/99999): loss=0.3249741216489942\n",
      "GD (98600/99999): loss=0.3249733706634886\n",
      "GD (98700/99999): loss=0.32497262080745715\n",
      "GD (98800/99999): loss=0.3249718720787368\n",
      "GD (98900/99999): loss=0.32497112447517046\n",
      "GD (99000/99999): loss=0.32497037799460826\n",
      "GD (99100/99999): loss=0.32496963263490614\n",
      "GD (99200/99999): loss=0.3249688883939266\n",
      "GD (99300/99999): loss=0.32496814526953915\n",
      "GD (99400/99999): loss=0.3249674032596187\n",
      "GD (99500/99999): loss=0.32496666236204724\n",
      "GD (99600/99999): loss=0.32496592257471274\n",
      "GD (99700/99999): loss=0.3249651838955095\n",
      "GD (99800/99999): loss=0.3249644463223381\n",
      "GD (99900/99999): loss=0.3249637098531055\n",
      "Accuracy ratio = 0.761\n",
      "Test loss = 0.324\n",
      "Train loss = 0.325\n"
     ]
    }
   ],
   "source": [
    "w_init = np.array([0]*tx_train.shape[1])\n",
    "max_iter = 100000\n",
    "gamma = 0.001\n",
    "w_gd, loss_gd = least_squares_GD(y_train, tx_train, w_init, max_iter, gamma, pr=True, adapt_gamma = False, kind = 'mse')\n",
    "gd_prediction = predict_labels(w_gd, tx_test)\n",
    "acc_gd = accuracy_ratio(gd_prediction, y_test)\n",
    "print('Accuracy ratio = %.3f'%acc_gd)\n",
    "print('Test loss = %.3f'%compute_loss(y_test, tx_test, w_gd))\n",
    "print('Train loss = %.3f'%loss_gd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T20:07:29.778162Z",
     "start_time": "2019-10-15T20:07:05.727604Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD (0/9999): loss=0.5\n",
      "SGD (100/9999): loss=0.49632190110009655\n",
      "SGD (200/9999): loss=0.5007559172544005\n",
      "SGD (300/9999): loss=0.49452567187431024\n",
      "SGD (400/9999): loss=0.5010617689449421\n",
      "SGD (500/9999): loss=0.48218886339651934\n",
      "SGD (600/9999): loss=0.4903904835994901\n",
      "SGD (700/9999): loss=0.4781623675685008\n",
      "SGD (800/9999): loss=0.4973767049384825\n",
      "SGD (900/9999): loss=0.33693654336154566\n",
      "SGD (1000/9999): loss=0.5086762191190576\n",
      "SGD (1100/9999): loss=0.5435669333755644\n",
      "SGD (1200/9999): loss=0.48507904695984394\n",
      "SGD (1300/9999): loss=0.5182184600315236\n",
      "SGD (1400/9999): loss=0.47802151893619993\n",
      "SGD (1500/9999): loss=0.3744420053299664\n",
      "SGD (1600/9999): loss=0.4475136359907532\n",
      "SGD (1700/9999): loss=0.4789743036525788\n",
      "SGD (1800/9999): loss=0.5635950412197825\n",
      "SGD (1900/9999): loss=0.4812303523674276\n",
      "SGD (2000/9999): loss=0.4647460214384664\n",
      "SGD (2100/9999): loss=0.4731453047915628\n",
      "SGD (2200/9999): loss=0.22866954826462624\n",
      "SGD (2300/9999): loss=0.5188842895311421\n",
      "SGD (2400/9999): loss=0.4716170906890032\n",
      "SGD (2500/9999): loss=0.4291500596114757\n",
      "SGD (2600/9999): loss=0.46540805828563026\n",
      "SGD (2700/9999): loss=0.46444229542341847\n",
      "SGD (2800/9999): loss=0.535979285982925\n",
      "SGD (2900/9999): loss=0.47454862222597716\n",
      "SGD (3000/9999): loss=0.4924209598230709\n",
      "SGD (3100/9999): loss=0.5146626248237774\n",
      "SGD (3200/9999): loss=0.537351673343092\n",
      "SGD (3300/9999): loss=0.527126982026969\n",
      "SGD (3400/9999): loss=0.34290946370509745\n",
      "SGD (3500/9999): loss=0.4323780331448788\n",
      "SGD (3600/9999): loss=0.4605958176488349\n",
      "SGD (3700/9999): loss=0.4574427435743503\n",
      "SGD (3800/9999): loss=0.40189383490272246\n",
      "SGD (3900/9999): loss=0.504029300424912\n",
      "SGD (4000/9999): loss=0.4816284711333132\n",
      "SGD (4100/9999): loss=0.35239709030094285\n",
      "SGD (4200/9999): loss=0.5295169740637853\n",
      "SGD (4300/9999): loss=0.48349047298663034\n",
      "SGD (4400/9999): loss=0.3786937729038355\n",
      "SGD (4500/9999): loss=0.07359332967753893\n",
      "SGD (4600/9999): loss=0.3674284923107672\n",
      "SGD (4700/9999): loss=0.46066808987977476\n",
      "SGD (4800/9999): loss=0.4560427567198096\n",
      "SGD (4900/9999): loss=0.45022301006769705\n",
      "SGD (5000/9999): loss=0.28218924480450636\n",
      "SGD (5100/9999): loss=0.4485735196180494\n",
      "SGD (5200/9999): loss=0.37216678515392637\n",
      "SGD (5300/9999): loss=0.39044231748300456\n",
      "SGD (5400/9999): loss=0.43830896773051226\n",
      "SGD (5500/9999): loss=0.4775850295098342\n",
      "SGD (5600/9999): loss=0.480922329451404\n",
      "SGD (5700/9999): loss=0.3549358405608035\n",
      "SGD (5800/9999): loss=0.5030127839749412\n",
      "SGD (5900/9999): loss=0.3673710183316838\n",
      "SGD (6000/9999): loss=0.17519476248612306\n",
      "SGD (6100/9999): loss=0.3799003780910754\n",
      "SGD (6200/9999): loss=0.48111963410219927\n",
      "SGD (6300/9999): loss=0.47660180456361745\n",
      "SGD (6400/9999): loss=0.5512076729777723\n",
      "SGD (6500/9999): loss=0.49202710420422024\n",
      "SGD (6600/9999): loss=0.48471092701036345\n",
      "SGD (6700/9999): loss=0.5639305473683521\n",
      "SGD (6800/9999): loss=0.5721079006226466\n",
      "SGD (6900/9999): loss=0.5198337211961505\n",
      "SGD (7000/9999): loss=0.32731650507252175\n",
      "SGD (7100/9999): loss=0.5503461527626771\n",
      "SGD (7200/9999): loss=0.4299454109782626\n",
      "SGD (7300/9999): loss=0.5083876383595669\n",
      "SGD (7400/9999): loss=0.5185234793018012\n",
      "SGD (7500/9999): loss=0.4574590117864766\n",
      "SGD (7600/9999): loss=0.30900002062122295\n",
      "SGD (7700/9999): loss=0.3070087956827479\n",
      "SGD (7800/9999): loss=0.5084702406773333\n",
      "SGD (7900/9999): loss=0.46931022963417723\n",
      "SGD (8000/9999): loss=0.5163748790557942\n",
      "SGD (8100/9999): loss=0.5502177546473902\n",
      "SGD (8200/9999): loss=0.4312159547199419\n",
      "SGD (8300/9999): loss=0.4237510912232462\n",
      "SGD (8400/9999): loss=0.46989299317686106\n",
      "SGD (8500/9999): loss=0.49471867109846956\n",
      "SGD (8600/9999): loss=0.42335620166477844\n",
      "SGD (8700/9999): loss=0.44767403562241115\n",
      "SGD (8800/9999): loss=0.5056071009867036\n",
      "SGD (8900/9999): loss=0.4641885880263905\n",
      "SGD (9000/9999): loss=0.4792641490635973\n",
      "SGD (9100/9999): loss=0.3112446767012028\n",
      "SGD (9200/9999): loss=0.12494236234384865\n",
      "SGD (9300/9999): loss=0.5011959445802678\n",
      "SGD (9400/9999): loss=0.5697458667747819\n",
      "SGD (9500/9999): loss=0.8879952800412414\n",
      "SGD (9600/9999): loss=0.5094213746263305\n",
      "SGD (9700/9999): loss=0.2534554945413114\n",
      "SGD (9800/9999): loss=0.5514380865962173\n",
      "SGD (9900/9999): loss=0.518769244427716\n",
      "Accuracy ratio = 0.73\n",
      "Test loss = 4.65e-01\n",
      "Train loss = 4.75e-01\n"
     ]
    }
   ],
   "source": [
    "w_init = np.array([0]*tx_train.shape[1])\n",
    "max_iter = 10000\n",
    "gamma = 1e-6\n",
    "batch_size = 1\n",
    "\n",
    "w_sgd, loss_sgd = least_squares_SGD(y_train, tx_train, w_init, batch_size, max_iter, gamma, pr = True, adapt_gamma=False, choose_best=True)\n",
    "sgd_prediction = predict_labels(w_sgd, tx_test)\n",
    "acc_sgd = accuracy_ratio(sgd_prediction, y_test)\n",
    "print('Accuracy ratio = %.2f'%acc_sgd)\n",
    "print('Test loss = %.2e'%compute_loss(y_test, tx_test, w_sgd))\n",
    "print('Train loss = %.2e'%loss_sgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T20:07:35.743947Z",
     "start_time": "2019-10-15T20:07:35.721625Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy ratio = 0.61\n",
      "Train loss = 0.29\n",
      "Test loss = 5.63e+07\n"
     ]
    }
   ],
   "source": [
    "w_lsq, loss_lsq = least_squares(y_train, tx_train)\n",
    "lsq_prediction = predict_labels(w_lsq, tx_test)\n",
    "acc_lsq = accuracy_ratio(lsq_prediction, y_test)\n",
    "print('Accuracy ratio = %.2f'%acc_lsq)\n",
    "print('Train loss = %.2f'%loss_lsq)\n",
    "print('Test loss = %.2e'%compute_loss(y_test, tx_test, w_lsq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T20:07:37.276191Z",
     "start_time": "2019-10-15T20:07:37.256730Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy ratio = 0.752\n",
      "Test loss = 0.367\n",
      "Train loss = 0.296\n"
     ]
    }
   ],
   "source": [
    "lambda_ = 2.6e-4\n",
    "w_rr, loss_rr = ridge_regression(y_train, tx_train, lambda_)\n",
    "rr_prediction = predict_labels(w_rr, tx_test)\n",
    "acc_rr = accuracy_ratio(rr_prediction, y_test)\n",
    "print('Accuracy ratio = %.3f'%acc_rr)\n",
    "print('Test loss = %.3f'%compute_loss(y_test, tx_test, w_rr))\n",
    "print('Train loss = %.3f'%loss_rr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T20:07:38.742279Z",
     "start_time": "2019-10-15T20:07:38.717473Z"
    }
   },
   "outputs": [],
   "source": [
    "def ridge_regression_demo(x, y, degree, ratio, seed):\n",
    "    \"\"\"ridge regression demo.\"\"\"\n",
    "    # define parameter\n",
    "    lambdas = np.logspace(-5, 3, 50)\n",
    "    # split the data, and return train and test data\n",
    "    x_train, y_train, x_test, y_test = split_data(x, y, ratio, seed)\n",
    "    # form train and test data with offset column\n",
    "    x_train_std = standardize_features(x_train)[0]\n",
    "    x_test_std = standardize_features(x_test)[0]\n",
    "    tx_train=build_poly(x_train_std, degree)\n",
    "    tx_test=build_poly(x_test_std, degree)\n",
    "    rmse_tr = []\n",
    "    rmse_te = []\n",
    "    accuracies = []\n",
    "    for ind, lambda_ in enumerate(lambdas):\n",
    "        # calcualte weight through least square.\n",
    "        w_train, loss_train = ridge_regression(y_train, tx_train, lambda_)\n",
    "        rmse_tr.append(np.sqrt(2*loss_train))\n",
    "        rmse_te.append(np.sqrt(2*compute_loss(y_test, tx_test, w_train, kind = 'mse')))\n",
    "        accuracies.append(accuracy_ratio(predict_labels(w_train, tx_test), y_test))\n",
    "        print(\"proportion={p}, degree={d}, lambda={l:.3e}, Training RMSE={tr:.3f}, Testing RMSE={te:.3f}, Accuracy={ac:.3f}\".format(\n",
    "               p=ratio, d=degree, l=lambda_, tr=rmse_tr[ind], te=rmse_te[ind], ac=accuracies[ind]))\n",
    "        \n",
    "    # Plot the obtained results\n",
    "    plot_train_test(rmse_tr, rmse_te, lambdas, degree)\n",
    "    plt.figure()\n",
    "    plt.semilogx(lambdas,accuracies, marker='o')\n",
    "def plot_train_test(train_errors, test_errors, lambdas, degree):\n",
    "    \"\"\"\n",
    "    train_errors, test_errors and lambas should be list (of the same size) the respective train error and test error for a given lambda,\n",
    "    * lambda[0] = 1\n",
    "    * train_errors[0] = RMSE of a ridge regression on the train set\n",
    "    * test_errors[0] = RMSE of the parameter found by ridge regression applied on the test set\n",
    "    \n",
    "    degree is just used for the title of the plot.\n",
    "    \"\"\"\n",
    "    plt.semilogx(lambdas, train_errors, color='b', marker='*', label=\"Train error\")\n",
    "    plt.semilogx(lambdas, test_errors, color='r', marker='*', label=\"Test error\")\n",
    "    plt.xlabel(\"lambda\")\n",
    "    plt.ylabel(\"RMSE\")\n",
    "    plt.title(\"Ridge regression for polynomial degree \" + str(degree))\n",
    "    leg = plt.legend(loc=1, shadow=True)\n",
    "    leg.draw_frame(False)\n",
    "    plt.savefig(\"../results/ridge_regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T20:10:14.401426Z",
     "start_time": "2019-10-15T20:10:12.397115Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proportion=0.9, degree=5, lambda=1.000e-05, Training RMSE=0.749, Testing RMSE=3.918, Accuracy=0.612\n",
      "proportion=0.9, degree=5, lambda=1.456e-05, Training RMSE=0.749, Testing RMSE=2.743, Accuracy=0.624\n",
      "proportion=0.9, degree=5, lambda=2.121e-05, Training RMSE=0.750, Testing RMSE=1.817, Accuracy=0.630\n",
      "proportion=0.9, degree=5, lambda=3.089e-05, Training RMSE=0.751, Testing RMSE=1.183, Accuracy=0.632\n",
      "proportion=0.9, degree=5, lambda=4.498e-05, Training RMSE=0.752, Testing RMSE=0.874, Accuracy=0.758\n",
      "proportion=0.9, degree=5, lambda=6.551e-05, Training RMSE=0.753, Testing RMSE=0.827, Accuracy=0.800\n",
      "proportion=0.9, degree=5, lambda=9.541e-05, Training RMSE=0.753, Testing RMSE=0.874, Accuracy=0.808\n",
      "proportion=0.9, degree=5, lambda=1.389e-04, Training RMSE=0.754, Testing RMSE=0.910, Accuracy=0.802\n",
      "proportion=0.9, degree=5, lambda=2.024e-04, Training RMSE=0.755, Testing RMSE=0.911, Accuracy=0.796\n",
      "proportion=0.9, degree=5, lambda=2.947e-04, Training RMSE=0.756, Testing RMSE=0.885, Accuracy=0.790\n",
      "proportion=0.9, degree=5, lambda=4.292e-04, Training RMSE=0.757, Testing RMSE=0.849, Accuracy=0.794\n",
      "proportion=0.9, degree=5, lambda=6.251e-04, Training RMSE=0.758, Testing RMSE=0.815, Accuracy=0.794\n",
      "proportion=0.9, degree=5, lambda=9.103e-04, Training RMSE=0.759, Testing RMSE=0.792, Accuracy=0.790\n",
      "proportion=0.9, degree=5, lambda=1.326e-03, Training RMSE=0.760, Testing RMSE=0.780, Accuracy=0.794\n",
      "proportion=0.9, degree=5, lambda=1.931e-03, Training RMSE=0.761, Testing RMSE=0.774, Accuracy=0.788\n",
      "proportion=0.9, degree=5, lambda=2.812e-03, Training RMSE=0.761, Testing RMSE=0.773, Accuracy=0.786\n",
      "proportion=0.9, degree=5, lambda=4.095e-03, Training RMSE=0.762, Testing RMSE=0.774, Accuracy=0.788\n",
      "proportion=0.9, degree=5, lambda=5.964e-03, Training RMSE=0.763, Testing RMSE=0.774, Accuracy=0.788\n",
      "proportion=0.9, degree=5, lambda=8.685e-03, Training RMSE=0.764, Testing RMSE=0.775, Accuracy=0.790\n",
      "proportion=0.9, degree=5, lambda=1.265e-02, Training RMSE=0.764, Testing RMSE=0.775, Accuracy=0.788\n",
      "proportion=0.9, degree=5, lambda=1.842e-02, Training RMSE=0.765, Testing RMSE=0.776, Accuracy=0.788\n",
      "proportion=0.9, degree=5, lambda=2.683e-02, Training RMSE=0.767, Testing RMSE=0.776, Accuracy=0.788\n",
      "proportion=0.9, degree=5, lambda=3.907e-02, Training RMSE=0.768, Testing RMSE=0.776, Accuracy=0.788\n",
      "proportion=0.9, degree=5, lambda=5.690e-02, Training RMSE=0.771, Testing RMSE=0.777, Accuracy=0.782\n",
      "proportion=0.9, degree=5, lambda=8.286e-02, Training RMSE=0.774, Testing RMSE=0.778, Accuracy=0.784\n",
      "proportion=0.9, degree=5, lambda=1.207e-01, Training RMSE=0.778, Testing RMSE=0.781, Accuracy=0.786\n",
      "proportion=0.9, degree=5, lambda=1.758e-01, Training RMSE=0.782, Testing RMSE=0.785, Accuracy=0.782\n",
      "proportion=0.9, degree=5, lambda=2.560e-01, Training RMSE=0.788, Testing RMSE=0.790, Accuracy=0.782\n",
      "proportion=0.9, degree=5, lambda=3.728e-01, Training RMSE=0.794, Testing RMSE=0.796, Accuracy=0.790\n",
      "proportion=0.9, degree=5, lambda=5.429e-01, Training RMSE=0.801, Testing RMSE=0.802, Accuracy=0.782\n",
      "proportion=0.9, degree=5, lambda=7.906e-01, Training RMSE=0.808, Testing RMSE=0.808, Accuracy=0.782\n",
      "proportion=0.9, degree=5, lambda=1.151e+00, Training RMSE=0.815, Testing RMSE=0.813, Accuracy=0.774\n",
      "proportion=0.9, degree=5, lambda=1.677e+00, Training RMSE=0.822, Testing RMSE=0.819, Accuracy=0.766\n",
      "proportion=0.9, degree=5, lambda=2.442e+00, Training RMSE=0.829, Testing RMSE=0.824, Accuracy=0.760\n",
      "proportion=0.9, degree=5, lambda=3.556e+00, Training RMSE=0.836, Testing RMSE=0.828, Accuracy=0.754\n",
      "proportion=0.9, degree=5, lambda=5.179e+00, Training RMSE=0.843, Testing RMSE=0.833, Accuracy=0.756\n",
      "proportion=0.9, degree=5, lambda=7.543e+00, Training RMSE=0.850, Testing RMSE=0.839, Accuracy=0.746\n",
      "proportion=0.9, degree=5, lambda=1.099e+01, Training RMSE=0.857, Testing RMSE=0.845, Accuracy=0.742\n",
      "proportion=0.9, degree=5, lambda=1.600e+01, Training RMSE=0.865, Testing RMSE=0.852, Accuracy=0.728\n",
      "proportion=0.9, degree=5, lambda=2.330e+01, Training RMSE=0.874, Testing RMSE=0.860, Accuracy=0.724\n",
      "proportion=0.9, degree=5, lambda=3.393e+01, Training RMSE=0.883, Testing RMSE=0.869, Accuracy=0.718\n",
      "proportion=0.9, degree=5, lambda=4.942e+01, Training RMSE=0.891, Testing RMSE=0.878, Accuracy=0.718\n",
      "proportion=0.9, degree=5, lambda=7.197e+01, Training RMSE=0.900, Testing RMSE=0.886, Accuracy=0.718\n",
      "proportion=0.9, degree=5, lambda=1.048e+02, Training RMSE=0.909, Testing RMSE=0.895, Accuracy=0.702\n",
      "proportion=0.9, degree=5, lambda=1.526e+02, Training RMSE=0.918, Testing RMSE=0.903, Accuracy=0.696\n",
      "proportion=0.9, degree=5, lambda=2.223e+02, Training RMSE=0.926, Testing RMSE=0.913, Accuracy=0.694\n",
      "proportion=0.9, degree=5, lambda=3.237e+02, Training RMSE=0.936, Testing RMSE=0.923, Accuracy=0.696\n",
      "proportion=0.9, degree=5, lambda=4.715e+02, Training RMSE=0.945, Testing RMSE=0.934, Accuracy=0.696\n",
      "proportion=0.9, degree=5, lambda=6.866e+02, Training RMSE=0.954, Testing RMSE=0.945, Accuracy=0.696\n",
      "proportion=0.9, degree=5, lambda=1.000e+03, Training RMSE=0.962, Testing RMSE=0.955, Accuracy=0.696\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "seed = 42\n",
    "degree = 5\n",
    "split_ratio = 0.9\n",
    "ridge_regression_demo(x, y, degree, split_ratio, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T20:11:28.134416Z",
     "start_time": "2019-10-15T20:11:28.119402Z"
    }
   },
   "outputs": [],
   "source": [
    "def cross_validation(y, x, k_indices, k, lambda_, degree):\n",
    "    \"\"\"return the loss of ridge regression.\"\"\"\n",
    "    # get k'th subgroup in test, others in train\n",
    "    id_test = k_indices[k]\n",
    "    id_train = k_indices[~(np.arange(k_indices.shape[0]) == k)].reshape(-1)\n",
    "    x_test = x[id_test]\n",
    "    x_train = x[id_train]\n",
    "    y_test = y[id_test]\n",
    "    y_train = y[id_train]\n",
    "    x_train_std = standardize(x_train)[0]\n",
    "    x_test_std = standardize(x_test)[0]\n",
    "    tx_train = build_poly(x_train, degree)\n",
    "    tx_test = build_poly(x_test, degree)\n",
    "    # ridge regression\n",
    "    weight, loss_tr = ridge_regression(y_train, tx_train, lambda_)\n",
    "    # calculate the loss for train and test data\n",
    "    loss_te = compute_loss(y_test, tx_test, weight)\n",
    "    accuracy = accuracy_ratio(predict_labels(weight, tx_test), y_test)\n",
    "\n",
    "    return loss_tr, loss_te, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T20:11:56.440032Z",
     "start_time": "2019-10-15T20:11:31.046930Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best lambda from accuracy: 6.21e-03\n",
      "Best lambda from error: 1.08e-01\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def cross_validation_visualization(lambds, mse_tr, mse_te):\n",
    "    \"\"\"visualization the curves of mse_tr and mse_te.\"\"\"\n",
    "    best_l_err = lambds[np.argmin(mse_te)]\n",
    "    print('Best lambda from error: %.2e'%best_l_err)\n",
    "    plt.semilogx(lambds, mse_tr, marker=\".\", color='b', label='train error')\n",
    "    plt.semilogx(lambds, mse_te, marker=\".\", color='r', label='test error')\n",
    "    plt.axvline(best_l_err, c = 'g', label = '$\\lambda^*_{rmse}=%.1e$'%best_l_err, ls = ':')\n",
    "    plt.xlabel(\"lambda\")\n",
    "    plt.ylabel(\"rmse\")\n",
    "    plt.title(\"cross validation\")\n",
    "    plt.legend(loc=0)\n",
    "    plt.grid(True)\n",
    "    plt.savefig(\"../results/cross_validation\")\n",
    "def cross_validation_visualization_accuracy(lambdas, accuracies):\n",
    "    \"\"\"visualization the curves of mse_tr and mse_te.\"\"\"\n",
    "    plt.semilogx(lambdas, accuracies, lw =2, marker = '*', label = 'Accuracy ratio')\n",
    "    best_l_acc = lambdas[np.argmax(accuracies)]\n",
    "    plt.axvline(best_l_acc, c= 'k', label = '$\\lambda^*_{acc}=%.1e$'%best_l_acc, ls = ':')\n",
    "    print('Best lambda from accuracy: %.2e'%best_l_acc)\n",
    "    plt.xlabel(\"lambda\")\n",
    "    plt.ylabel(\"rmse\")\n",
    "    plt.title(\"cross validation\")\n",
    "    plt.legend(loc=0)\n",
    "    plt.grid(True)\n",
    "    plt.savefig(\"../results/cross_validation_accuracies\")\n",
    "def cross_validation_demo():\n",
    "    seed = 42\n",
    "    degree = 7\n",
    "    k_fold = 10\n",
    "    lambdas = np.logspace(-4, 0, 30)\n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    # define lists to store the loss of training data and test data\n",
    "    rmse_tr = []\n",
    "    rmse_te = []\n",
    "    std_tr = []\n",
    "    std_te = []\n",
    "    accuracies = []\n",
    "    # cross validation\n",
    "    for ind, lambda_ in enumerate(lambdas):\n",
    "        x_validation = np.array([cross_validation(y, x, k_indices, k, lambda_, degree) for k in range(k_fold)])\n",
    "        rmse_tr.append(np.mean(np.sqrt(2 * x_validation[:, 0])))\n",
    "        rmse_te.append(np.mean(np.sqrt(2 * x_validation[:, 1])))\n",
    "        std_tr.append(np.std(np.sqrt(2 * x_validation[:, 0])))\n",
    "        std_te.append(np.std(np.sqrt(2 * x_validation[:, 1])))\n",
    "        accuracies.append(np.mean(x_validation[:,2]))\n",
    "    cross_validation_visualization_accuracy(lambdas, accuracies)\n",
    "    plt.figure()\n",
    "    cross_validation_visualization(lambdas, rmse_tr, rmse_te)\n",
    "\n",
    "cross_validation_demo()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T14:39:07.559498Z",
     "start_time": "2019-10-09T14:38:59.016864Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(568238,)\n"
     ]
    }
   ],
   "source": [
    "print(id_out_test.shape)\n",
    "x_out_test_std = standardize_features(x_out_test)\n",
    "x_out = x_out_test_std[0]\n",
    "tx_out = add_offset_column(x_out)\n",
    "\n",
    "create_csv_submission(id_out_test, predict_labels(w_rr, tx_out) , '../results/rr_pred.csv')\n",
    "create_csv_submission(id_out_test, predict_labels(w_gd, tx_out) , '../results/gd_pred_noadapt.csv')\n",
    "create_csv_submission(id_out_test, predict_labels(w_lsq, tx_out) , '../results/lsq_pred.csv')\n",
    "create_csv_submission(id_out_test, predict_labels(w_sgd, tx_out) , '../results/sgd_pred_noadapt.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T19:56:40.502785Z",
     "start_time": "2019-10-15T19:56:32.944405Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression GD (0/3999): loss=3119.1623125199\n",
      "Logistic Regression GD (100/3999): loss=-5052.666973716754\n",
      "Logistic Regression GD (200/3999): loss=-11433.464618887743\n",
      "Logistic Regression GD (300/3999): loss=-17413.60415105426\n",
      "Logistic Regression GD (400/3999): loss=-23223.98806948015\n",
      "Logistic Regression GD (500/3999): loss=-28945.610910390023\n",
      "Logistic Regression GD (600/3999): loss=-34614.76643112936\n",
      "Logistic Regression GD (700/3999): loss=-40250.402858839385\n",
      "Logistic Regression GD (800/3999): loss=-45863.44758469955\n",
      "Logistic Regression GD (900/3999): loss=-51460.63998304881\n",
      "Logistic Regression GD (1000/3999): loss=-57046.342020135344\n",
      "Logistic Regression GD (1100/3999): loss=-62623.481785886215\n",
      "Logistic Regression GD (1200/3999): loss=-68194.07948376483\n",
      "Logistic Regression GD (1300/3999): loss=-73759.56306488346\n",
      "Logistic Regression GD (1400/3999): loss=-79320.97905633446\n",
      "Logistic Regression GD (1500/3999): loss=-84879.11791756989\n",
      "Logistic Regression GD (1600/3999): loss=-90434.58244345462\n",
      "Logistic Regression GD (1700/3999): loss=-95987.83777402012\n",
      "Logistic Regression GD (1800/3999): loss=-101539.24770951094\n",
      "Logistic Regression GD (1900/3999): loss=-107089.10041091037\n",
      "Logistic Regression GD (2000/3999): loss=-112637.62686403707\n",
      "Logistic Regression GD (2100/3999): loss=-118185.01430831033\n",
      "Logistic Regression GD (2200/3999): loss=-123731.416111573\n",
      "Logistic Regression GD (2300/3999): loss=-129276.9591130256\n",
      "Logistic Regression GD (2400/3999): loss=-134821.74914539861\n",
      "Logistic Regression GD (2500/3999): loss=-140365.87523463654\n",
      "Logistic Regression GD (2600/3999): loss=-145909.4128289802\n",
      "Logistic Regression GD (2700/3999): loss=-151452.42630812994\n",
      "Logistic Regression GD (2800/3999): loss=-156994.9709527565\n",
      "Logistic Regression GD (2900/3999): loss=-162537.09450526276\n",
      "Logistic Regression GD (3000/3999): loss=-168078.83841780457\n",
      "Logistic Regression GD (3100/3999): loss=-173620.238858693\n",
      "Logistic Regression GD (3200/3999): loss=-179161.327530384\n",
      "Logistic Regression GD (3300/3999): loss=-184702.1323392457\n",
      "Logistic Regression GD (3400/3999): loss=-190242.6779477429\n",
      "Logistic Regression GD (3500/3999): loss=-195782.98623260937\n",
      "Logistic Regression GD (3600/3999): loss=-201323.0766672925\n",
      "Logistic Regression GD (3700/3999): loss=-206862.96664297345\n",
      "Logistic Regression GD (3800/3999): loss=-212402.6717394368\n",
      "Logistic Regression GD (3900/3999): loss=-217942.20595474137\n",
      "Accuracy ratio = 0.650\n",
      "Test loss = -25823.906\n",
      "Train loss = -223426.189\n"
     ]
    }
   ],
   "source": [
    "w_init = np.array([0]*tx_train.shape[1])\n",
    "max_iter = 4000\n",
    "gamma = 1e-7\n",
    "w_lrgd, loss_lrgd = logistic_regression(y_train, tx_train, w_init, max_iter, gamma, pr=True, adapt_gamma = False)\n",
    "lrgd_prediction = predict_labels(w_lrgd, tx_test)\n",
    "acc_lrgd = accuracy_ratio(lrgd_prediction, y_test)\n",
    "print('Accuracy ratio = %.3f'%acc_lrgd)\n",
    "print('Test loss = %.3f'%compute_loss_logistic(y_test, tx_test, w_lrgd))\n",
    "print('Train loss = %.3f'%loss_lrgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T20:26:25.163485Z",
     "start_time": "2019-10-15T20:24:37.811107Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Regularized Logistic Regression GD (0/99999): loss=3119.1623125199\n",
      " Regularized Logistic Regression GD (100/99999): loss=2688.691845895465\n",
      " Regularized Logistic Regression GD (200/99999): loss=2304.1297938103958\n",
      " Regularized Logistic Regression GD (300/99999): loss=1956.6286948875052\n",
      " Regularized Logistic Regression GD (400/99999): loss=1639.7748428355728\n",
      " Regularized Logistic Regression GD (500/99999): loss=1348.914140418746\n",
      " Regularized Logistic Regression GD (600/99999): loss=1080.5033867836107\n",
      " Regularized Logistic Regression GD (700/99999): loss=831.7359821070947\n",
      " Regularized Logistic Regression GD (800/99999): loss=600.3250439258538\n",
      " Regularized Logistic Regression GD (900/99999): loss=384.36877008910847\n",
      " Regularized Logistic Regression GD (1000/99999): loss=182.26125630421242\n",
      " Regularized Logistic Regression GD (1100/99999): loss=-7.369722867539792\n",
      " Regularized Logistic Regression GD (1200/99999): loss=-185.70792245373315\n",
      " Regularized Logistic Regression GD (1300/99999): loss=-353.7825494377411\n",
      " Regularized Logistic Regression GD (1400/99999): loss=-512.4943819615405\n",
      " Regularized Logistic Regression GD (1500/99999): loss=-662.636306060723\n",
      " Regularized Logistic Regression GD (1600/99999): loss=-804.9097428973747\n",
      " Regularized Logistic Regression GD (1700/99999): loss=-939.9379771369386\n",
      " Regularized Logistic Regression GD (1800/99999): loss=-1068.2771005418208\n",
      " Regularized Logistic Regression GD (1900/99999): loss=-1190.4250883643144\n",
      " Regularized Logistic Regression GD (2000/99999): loss=-1306.8293921529373\n",
      " Regularized Logistic Regression GD (2100/99999): loss=-1417.8933388738578\n",
      " Regularized Logistic Regression GD (2200/99999): loss=-1523.9815591733793\n",
      " Regularized Logistic Regression GD (2300/99999): loss=-1625.4246185997067\n",
      " Regularized Logistic Regression GD (2400/99999): loss=-1722.5229891316862\n",
      " Regularized Logistic Regression GD (2500/99999): loss=-1815.550470773639\n",
      " Regularized Logistic Regression GD (2600/99999): loss=-1904.7571517994015\n",
      " Regularized Logistic Regression GD (2700/99999): loss=-1990.371979762943\n",
      " Regularized Logistic Regression GD (2800/99999): loss=-2072.6050024421265\n",
      " Regularized Logistic Regression GD (2900/99999): loss=-2151.6493275907023\n",
      " Regularized Logistic Regression GD (3000/99999): loss=-2227.6828421213418\n",
      " Regularized Logistic Regression GD (3100/99999): loss=-2300.869724670907\n",
      " Regularized Logistic Regression GD (3200/99999): loss=-2371.361780066146\n",
      " Regularized Logistic Regression GD (3300/99999): loss=-2439.2996197547063\n",
      " Regularized Logistic Regression GD (3400/99999): loss=-2504.813708594098\n",
      " Regularized Logistic Regression GD (3500/99999): loss=-2568.02529534695\n",
      " Regularized Logistic Regression GD (3600/99999): loss=-2629.047241694522\n",
      " Regularized Logistic Regression GD (3700/99999): loss=-2687.9847624578933\n",
      " Regularized Logistic Regression GD (3800/99999): loss=-2744.936087932221\n",
      " Regularized Logistic Regression GD (3900/99999): loss=-2799.9930577345895\n",
      " Regularized Logistic Regression GD (4000/99999): loss=-2853.241654291567\n",
      " Regularized Logistic Regression GD (4100/99999): loss=-2904.762483009991\n",
      " Regularized Logistic Regression GD (4200/99999): loss=-2954.6312052517815\n",
      " Regularized Logistic Regression GD (4300/99999): loss=-3002.9189294446824\n",
      " Regularized Logistic Regression GD (4400/99999): loss=-3049.6925649847335\n",
      " Regularized Logistic Regression GD (4500/99999): loss=-3095.015143004818\n",
      " Regularized Logistic Regression GD (4600/99999): loss=-3138.9461075826684\n",
      " Regularized Logistic Regression GD (4700/99999): loss=-3181.541580528955\n",
      " Regularized Logistic Regression GD (4800/99999): loss=-3222.854602521056\n",
      " Regularized Logistic Regression GD (4900/99999): loss=-3262.9353530228673\n",
      " Regularized Logistic Regression GD (5000/99999): loss=-3301.831351147932\n",
      " Regularized Logistic Regression GD (5100/99999): loss=-3339.587639376388\n",
      " Regularized Logistic Regression GD (5200/99999): loss=-3376.2469518208154\n",
      " Regularized Logistic Regression GD (5300/99999): loss=-3411.8498685473833\n",
      " Regularized Logistic Regression GD (5400/99999): loss=-3446.4349572932547\n",
      " Regularized Logistic Regression GD (5500/99999): loss=-3480.0389037760388\n",
      " Regularized Logistic Regression GD (5600/99999): loss=-3512.6966316630246\n",
      " Regularized Logistic Regression GD (5700/99999): loss=-3544.441413155365\n",
      " Regularized Logistic Regression GD (5800/99999): loss=-3575.304971042715\n",
      " Regularized Logistic Regression GD (5900/99999): loss=-3605.31757299578\n",
      " Regularized Logistic Regression GD (6000/99999): loss=-3634.508118786248\n",
      " Regularized Logistic Regression GD (6100/99999): loss=-3662.904221054364\n",
      " Regularized Logistic Regression GD (6200/99999): loss=-3690.5322801827506\n",
      " Regularized Logistic Regression GD (6300/99999): loss=-3717.4175537805377\n",
      " Regularized Logistic Regression GD (6400/99999): loss=-3743.584221232859\n",
      " Regularized Logistic Regression GD (6500/99999): loss=-3769.0554437273345\n",
      " Regularized Logistic Regression GD (6600/99999): loss=-3793.853420130212\n",
      " Regularized Logistic Regression GD (6700/99999): loss=-3817.999439049916\n",
      " Regularized Logistic Regression GD (6800/99999): loss=-3841.5139273946843\n",
      " Regularized Logistic Regression GD (6900/99999): loss=-3864.416495702779\n",
      " Regularized Logistic Regression GD (7000/99999): loss=-3886.725980498738\n",
      " Regularized Logistic Regression GD (7100/99999): loss=-3908.4604839063973\n",
      " Regularized Logistic Regression GD (7200/99999): loss=-3929.6374107290385\n",
      " Regularized Logistic Regression GD (7300/99999): loss=-3950.273503188717\n",
      " Regularized Logistic Regression GD (7400/99999): loss=-3970.384873500065\n",
      " Regularized Logistic Regression GD (7500/99999): loss=-3989.987034439031\n",
      " Regularized Logistic Regression GD (7600/99999): loss=-4009.0949280533596\n",
      " Regularized Logistic Regression GD (7700/99999): loss=-4027.7229526493184\n",
      " Regularized Logistic Regression GD (7800/99999): loss=-4045.884988178075\n",
      " Regularized Logistic Regression GD (7900/99999): loss=-4063.5944201350453\n",
      " Regularized Logistic Regression GD (8000/99999): loss=-4080.864162076202\n",
      " Regularized Logistic Regression GD (8100/99999): loss=-4097.706676847185\n",
      " Regularized Logistic Regression GD (8200/99999): loss=-4114.13399661326\n",
      " Regularized Logistic Regression GD (8300/99999): loss=-4130.157741771377\n",
      " Regularized Logistic Regression GD (8400/99999): loss=-4145.789138819149\n",
      " Regularized Logistic Regression GD (8500/99999): loss=-4161.039037249926\n",
      " Regularized Logistic Regression GD (8600/99999): loss=-4175.917925537717\n",
      " Regularized Logistic Regression GD (8700/99999): loss=-4190.435946271042\n",
      " Regularized Logistic Regression GD (8800/99999): loss=-4204.602910490243\n",
      " Regularized Logistic Regression GD (8900/99999): loss=-4218.428311278881\n",
      " Regularized Logistic Regression GD (9000/99999): loss=-4231.921336655989\n",
      " Regularized Logistic Regression GD (9100/99999): loss=-4245.090881812658\n",
      " Regularized Logistic Regression GD (9200/99999): loss=-4257.945560733281\n",
      " Regularized Logistic Regression GD (9300/99999): loss=-4270.493717238891\n",
      " Regularized Logistic Regression GD (9400/99999): loss=-4282.743435487339\n",
      " Regularized Logistic Regression GD (9500/99999): loss=-4294.702549962816\n",
      " Regularized Logistic Regression GD (9600/99999): loss=-4306.378654984674\n",
      " Regularized Logistic Regression GD (9700/99999): loss=-4317.779113763732\n",
      " Regularized Logistic Regression GD (9800/99999): loss=-4328.911067032197\n",
      " Regularized Logistic Regression GD (9900/99999): loss=-4339.781441271521\n",
      " Regularized Logistic Regression GD (10000/99999): loss=-4350.396956561051\n",
      " Regularized Logistic Regression GD (10100/99999): loss=-4360.764134068704\n",
      " Regularized Logistic Regression GD (10200/99999): loss=-4370.88930320349\n",
      " Regularized Logistic Regression GD (10300/99999): loss=-4380.778608448516\n",
      " Regularized Logistic Regression GD (10400/99999): loss=-4390.4380158918475\n",
      " Regularized Logistic Regression GD (10500/99999): loss=-4399.873319471475\n",
      " Regularized Logistic Regression GD (10600/99999): loss=-4409.090146949671\n",
      " Regularized Logistic Regression GD (10700/99999): loss=-4418.093965631004\n",
      " Regularized Logistic Regression GD (10800/99999): loss=-4426.890087837432\n",
      " Regularized Logistic Regression GD (10900/99999): loss=-4435.483676153063\n",
      " Regularized Logistic Regression GD (11000/99999): loss=-4443.879748450388\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Regularized Logistic Regression GD (11100/99999): loss=-4452.0831827090915\n",
      " Regularized Logistic Regression GD (11200/99999): loss=-4460.098721637905\n",
      " Regularized Logistic Regression GD (11300/99999): loss=-4467.930977109281\n",
      " Regularized Logistic Regression GD (11400/99999): loss=-4475.584434416149\n",
      " Regularized Logistic Regression GD (11500/99999): loss=-4483.06345635949\n",
      " Regularized Logistic Regression GD (11600/99999): loss=-4490.372287174869\n",
      " Regularized Logistic Regression GD (11700/99999): loss=-4497.515056305698\n",
      " Regularized Logistic Regression GD (11800/99999): loss=-4504.495782030524\n",
      " Regularized Logistic Regression GD (11900/99999): loss=-4511.318374951188\n",
      " Regularized Logistic Regression GD (12000/99999): loss=-4517.986641348391\n",
      " Regularized Logistic Regression GD (12100/99999): loss=-4524.504286410783\n",
      " Regularized Logistic Regression GD (12200/99999): loss=-4530.87491734338\n",
      " Regularized Logistic Regression GD (12300/99999): loss=-4537.1020463608065\n",
      " Regularized Logistic Regression GD (12400/99999): loss=-4543.189093570509\n",
      " Regularized Logistic Regression GD (12500/99999): loss=-4549.139389750928\n",
      " Regularized Logistic Regression GD (12600/99999): loss=-4554.956179029197\n",
      " Regularized Logistic Regression GD (12700/99999): loss=-4560.642621462836\n",
      " Regularized Logistic Regression GD (12800/99999): loss=-4566.201795529589\n",
      " Regularized Logistic Regression GD (12900/99999): loss=-4571.63670052935\n",
      " Regularized Logistic Regression GD (13000/99999): loss=-4576.95025890199\n",
      " Regularized Logistic Regression GD (13100/99999): loss=-4582.145318464587\n",
      " Regularized Logistic Regression GD (13200/99999): loss=-4587.224654571468\n",
      " Regularized Logistic Regression GD (13300/99999): loss=-4592.190972200321\n",
      " Regularized Logistic Regression GD (13400/99999): loss=-4597.046907967346\n",
      " Regularized Logistic Regression GD (13500/99999): loss=-4601.795032074449\n",
      " Regularized Logistic Regression GD (13600/99999): loss=-4606.4378501911615\n",
      " Regularized Logistic Regression GD (13700/99999): loss=-4610.977805273974\n",
      " Regularized Logistic Regression GD (13800/99999): loss=-4615.417279325538\n",
      " Regularized Logistic Regression GD (13900/99999): loss=-4619.7585950961775\n",
      " Regularized Logistic Regression GD (14000/99999): loss=-4624.004017729924\n",
      " Regularized Logistic Regression GD (14100/99999): loss=-4628.155756357282\n",
      " Regularized Logistic Regression GD (14200/99999): loss=-4632.215965636772\n",
      " Regularized Logistic Regression GD (14300/99999): loss=-4636.186747247205\n",
      " Regularized Logistic Regression GD (14400/99999): loss=-4640.07015133262\n",
      " Regularized Logistic Regression GD (14500/99999): loss=-4643.868177901599\n",
      " Regularized Logistic Regression GD (14600/99999): loss=-4647.582778182741\n",
      " Regularized Logistic Regression GD (14700/99999): loss=-4651.215855937906\n",
      " Regularized Logistic Regression GD (14800/99999): loss=-4654.769268734768\n",
      " Regularized Logistic Regression GD (14900/99999): loss=-4658.24482918023\n",
      " Regularized Logistic Regression GD (15000/99999): loss=-4661.644306116044\n",
      " Regularized Logistic Regression GD (15100/99999): loss=-4664.969425778105\n",
      " Regularized Logistic Regression GD (15200/99999): loss=-4668.221872920611\n",
      " Regularized Logistic Regression GD (15300/99999): loss=-4671.403291906491\n",
      " Regularized Logistic Regression GD (15400/99999): loss=-4674.515287765117\n",
      " Regularized Logistic Regression GD (15500/99999): loss=-4677.559427218612\n",
      " Regularized Logistic Regression GD (15600/99999): loss=-4680.537239677757\n",
      " Regularized Logistic Regression GD (15700/99999): loss=-4683.450218208578\n",
      " Regularized Logistic Regression GD (15800/99999): loss=-4686.299820470649\n",
      " Regularized Logistic Regression GD (15900/99999): loss=-4689.087469628037\n",
      " Regularized Logistic Regression GD (16000/99999): loss=-4691.814555233839\n",
      " Regularized Logistic Regression GD (16100/99999): loss=-4694.482434089236\n",
      " Regularized Logistic Regression GD (16200/99999): loss=-4697.092431077852\n",
      " Regularized Logistic Regression GD (16300/99999): loss=-4699.645839976309\n",
      " Regularized Logistic Regression GD (16400/99999): loss=-4702.143924241716\n",
      " Regularized Logistic Regression GD (16500/99999): loss=-4704.587917776911\n",
      " Regularized Logistic Regression GD (16600/99999): loss=-4706.979025674107\n",
      " Regularized Logistic Regression GD (16700/99999): loss=-4709.318424937706\n",
      " Regularized Logistic Regression GD (16800/99999): loss=-4711.607265186969\n",
      " Regularized Logistic Regression GD (16900/99999): loss=-4713.846669339093\n",
      " Regularized Logistic Regression GD (17000/99999): loss=-4716.037734273445\n",
      " Regularized Logistic Regression GD (17100/99999): loss=-4718.181531477466\n",
      " Regularized Logistic Regression GD (17200/99999): loss=-4720.279107674868\n",
      " Regularized Logistic Regression GD (17300/99999): loss=-4722.331485436638\n",
      " Regularized Logistic Regression GD (17400/99999): loss=-4724.33966377546\n",
      " Regularized Logistic Regression GD (17500/99999): loss=-4726.3046187239715\n",
      " Regularized Logistic Regression GD (17600/99999): loss=-4728.227303897464\n",
      " Regularized Logistic Regression GD (17700/99999): loss=-4730.108651041397\n",
      " Regularized Logistic Regression GD (17800/99999): loss=-4731.94957056428\n",
      " Regularized Logistic Regression GD (17900/99999): loss=-4733.750952056313\n",
      " Regularized Logistic Regression GD (18000/99999): loss=-4735.513664794244\n",
      " Regularized Logistic Regression GD (18100/99999): loss=-4737.238558232833\n",
      " Regularized Logistic Regression GD (18200/99999): loss=-4738.926462483366\n",
      " Regularized Logistic Regression GD (18300/99999): loss=-4740.578188779547\n",
      " Regularized Logistic Regression GD (18400/99999): loss=-4742.194529931207\n",
      " Regularized Logistic Regression GD (18500/99999): loss=-4743.776260766131\n",
      " Regularized Logistic Regression GD (18600/99999): loss=-4745.324138560395\n",
      " Regularized Logistic Regression GD (18700/99999): loss=-4746.83890345751\n",
      " Regularized Logistic Regression GD (18800/99999): loss=-4748.321278876766\n",
      " Regularized Logistic Regression GD (18900/99999): loss=-4749.771971910988\n",
      " Regularized Logistic Regression GD (19000/99999): loss=-4751.191673714112\n",
      " Regularized Logistic Regression GD (19100/99999): loss=-4752.581059878789\n",
      " Regularized Logistic Regression GD (19200/99999): loss=-4753.940790804381\n",
      " Regularized Logistic Regression GD (19300/99999): loss=-4755.271512055535\n",
      " Regularized Logistic Regression GD (19400/99999): loss=-4756.573854711691\n",
      " Regularized Logistic Regression GD (19500/99999): loss=-4757.848435707705\n",
      " Regularized Logistic Regression GD (19600/99999): loss=-4759.095858165895\n",
      " Regularized Logistic Regression GD (19700/99999): loss=-4760.3167117197\n",
      " Regularized Logistic Regression GD (19800/99999): loss=-4761.511572829215\n",
      " Regularized Logistic Regression GD (19900/99999): loss=-4762.68100508885\n",
      " Regularized Logistic Regression GD (20000/99999): loss=-4763.825559527264\n",
      " Regularized Logistic Regression GD (20100/99999): loss=-4764.945774899833\n",
      " Regularized Logistic Regression GD (20200/99999): loss=-4766.042177973893\n",
      " Regularized Logistic Regression GD (20300/99999): loss=-4767.115283806822\n",
      " Regularized Logistic Regression GD (20400/99999): loss=-4768.16559601737\n",
      " Regularized Logistic Regression GD (20500/99999): loss=-4769.193607050164\n",
      " Regularized Logistic Regression GD (20600/99999): loss=-4770.199798433802\n",
      " Regularized Logistic Regression GD (20700/99999): loss=-4771.184641032576\n",
      " Regularized Logistic Regression GD (20800/99999): loss=-4772.148595292019\n",
      " Regularized Logistic Regression GD (20900/99999): loss=-4773.092111478464\n",
      " Regularized Logistic Regression GD (21000/99999): loss=-4774.015629912775\n",
      " Regularized Logistic Regression GD (21100/99999): loss=-4774.919581198374\n",
      " Regularized Logistic Regression GD (21200/99999): loss=-4775.804386443737\n",
      " Regularized Logistic Regression GD (21300/99999): loss=-4776.67045747951\n",
      " Regularized Logistic Regression GD (21400/99999): loss=-4777.518197070371\n",
      " Regularized Logistic Regression GD (21500/99999): loss=-4778.347999121776\n",
      " Regularized Logistic Regression GD (21600/99999): loss=-4779.160248881766\n",
      " Regularized Logistic Regression GD (21700/99999): loss=-4779.955323137854\n",
      " Regularized Logistic Regression GD (21800/99999): loss=-4780.733590409287\n",
      " Regularized Logistic Regression GD (21900/99999): loss=-4781.495411134625\n",
      " Regularized Logistic Regression GD (22000/99999): loss=-4782.2411378548895\n",
      " Regularized Logistic Regression GD (22100/99999): loss=-4782.971115392335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Regularized Logistic Regression GD (22200/99999): loss=-4783.685681024971\n",
      " Regularized Logistic Regression GD (22300/99999): loss=-4784.385164656912\n",
      " Regularized Logistic Regression GD (22400/99999): loss=-4785.069888984767\n",
      " Regularized Logistic Regression GD (22500/99999): loss=-4785.7401696599845\n",
      " Regularized Logistic Regression GD (22600/99999): loss=-4786.396315447468\n",
      " Regularized Logistic Regression GD (22700/99999): loss=-4787.038628380373\n",
      " Regularized Logistic Regression GD (22800/99999): loss=-4787.667403911321\n",
      " Regularized Logistic Regression GD (22900/99999): loss=-4788.282931060018\n",
      " Regularized Logistic Regression GD (23000/99999): loss=-4788.885492557426\n",
      " Regularized Logistic Regression GD (23100/99999): loss=-4789.475364986581\n",
      " Regularized Logistic Regression GD (23200/99999): loss=-4790.05281892007\n",
      " Regularized Logistic Regression GD (23300/99999): loss=-4790.61811905435\n",
      " Regularized Logistic Regression GD (23400/99999): loss=-4791.171524340907\n",
      " Regularized Logistic Regression GD (23500/99999): loss=-4791.713288114365\n",
      " Regularized Logistic Regression GD (23600/99999): loss=-4792.243658217639\n",
      " Regularized Logistic Regression GD (23700/99999): loss=-4792.7628771241525\n",
      " Regularized Logistic Regression GD (23800/99999): loss=-4793.271182057261\n",
      " Regularized Logistic Regression GD (23900/99999): loss=-4793.768805106878\n",
      " Regularized Logistic Regression GD (24000/99999): loss=-4794.255973343439\n",
      " Regularized Logistic Regression GD (24100/99999): loss=-4794.732908929204\n",
      " Regularized Logistic Regression GD (24200/99999): loss=-4795.199829227028\n",
      " Regularized Logistic Regression GD (24300/99999): loss=-4795.656946906611\n",
      " Regularized Logistic Regression GD (24400/99999): loss=-4796.104470048313\n",
      " Regularized Logistic Regression GD (24500/99999): loss=-4796.542602244578\n",
      " Regularized Logistic Regression GD (24600/99999): loss=-4796.971542699067\n",
      " Regularized Logistic Regression GD (24700/99999): loss=-4797.391486323472\n",
      " Regularized Logistic Regression GD (24800/99999): loss=-4797.802623832157\n",
      " Regularized Logistic Regression GD (24900/99999): loss=-4798.205141834634\n",
      " Regularized Logistic Regression GD (25000/99999): loss=-4798.599222925896\n",
      " Regularized Logistic Regression GD (25100/99999): loss=-4798.985045774768\n",
      " Regularized Logistic Regression GD (25200/99999): loss=-4799.36278521016\n",
      " Regularized Logistic Regression GD (25300/99999): loss=-4799.732612305421\n",
      " Regularized Logistic Regression GD (25400/99999): loss=-4800.094694460758\n",
      " Regularized Logistic Regression GD (25500/99999): loss=-4800.449195483795\n",
      " Regularized Logistic Regression GD (25600/99999): loss=-4800.79627566831\n",
      " Regularized Logistic Regression GD (25700/99999): loss=-4801.136091871164\n",
      " Regularized Logistic Regression GD (25800/99999): loss=-4801.468797587535\n",
      " Regularized Logistic Regression GD (25900/99999): loss=-4801.794543024437\n",
      " Regularized Logistic Regression GD (26000/99999): loss=-4802.113475172566\n",
      " Regularized Logistic Regression GD (26100/99999): loss=-4802.425737876553\n",
      " Regularized Logistic Regression GD (26200/99999): loss=-4802.7314719036185\n",
      " Regularized Logistic Regression GD (26300/99999): loss=-4803.030815010702\n",
      " Regularized Logistic Regression GD (26400/99999): loss=-4803.323902010063\n",
      " Regularized Logistic Regression GD (26500/99999): loss=-4803.610864833443\n",
      " Regularized Logistic Regression GD (26600/99999): loss=-4803.8918325947525\n",
      " Regularized Logistic Regression GD (26700/99999): loss=-4804.1669316513935\n",
      " Regularized Logistic Regression GD (26800/99999): loss=-4804.436285664161\n",
      " Regularized Logistic Regression GD (26900/99999): loss=-4804.700015655873\n",
      " Regularized Logistic Regression GD (27000/99999): loss=-4804.958240068629\n",
      " Regularized Logistic Regression GD (27100/99999): loss=-4805.211074819837\n",
      " Regularized Logistic Regression GD (27200/99999): loss=-4805.458633356973\n",
      " Regularized Logistic Regression GD (27300/99999): loss=-4805.7010267111045\n",
      " Regularized Logistic Regression GD (27400/99999): loss=-4805.938363549275\n",
      " Regularized Logistic Regression GD (27500/99999): loss=-4806.170750225663\n",
      " Regularized Logistic Regression GD (27600/99999): loss=-4806.398290831665\n",
      " Regularized Logistic Regression GD (27700/99999): loss=-4806.621087244809\n",
      " Regularized Logistic Regression GD (27800/99999): loss=-4806.839239176634\n",
      " Regularized Logistic Regression GD (27900/99999): loss=-4807.052844219491\n",
      " Regularized Logistic Regression GD (28000/99999): loss=-4807.2619978922885\n",
      " Regularized Logistic Regression GD (28100/99999): loss=-4807.466793685259\n",
      " Regularized Logistic Regression GD (28200/99999): loss=-4807.667323103729\n",
      " Regularized Logistic Regression GD (28300/99999): loss=-4807.863675710896\n",
      " Regularized Logistic Regression GD (28400/99999): loss=-4808.055939169707\n",
      " Regularized Logistic Regression GD (28500/99999): loss=-4808.244199283774\n",
      " Regularized Logistic Regression GD (28600/99999): loss=-4808.4285400374\n",
      " Regularized Logistic Regression GD (28700/99999): loss=-4808.6090436347595\n",
      " Regularized Logistic Regression GD (28800/99999): loss=-4808.785790538155\n",
      " Regularized Logistic Regression GD (28900/99999): loss=-4808.95885950548\n",
      " Regularized Logistic Regression GD (29000/99999): loss=-4809.128327626833\n",
      " Regularized Logistic Regression GD (29100/99999): loss=-4809.294270360363\n",
      " Regularized Logistic Regression GD (29200/99999): loss=-4809.456761567288\n",
      " Regularized Logistic Regression GD (29300/99999): loss=-4809.615873546163\n",
      " Regularized Logistic Regression GD (29400/99999): loss=-4809.771677066411\n",
      " Regularized Logistic Regression GD (29500/99999): loss=-4809.924241401094\n",
      " Regularized Logistic Regression GD (29600/99999): loss=-4810.073634358995\n",
      " Regularized Logistic Regression GD (29700/99999): loss=-4810.219922315977\n",
      " Regularized Logistic Regression GD (29800/99999): loss=-4810.3631702456705\n",
      " Regularized Logistic Regression GD (29900/99999): loss=-4810.503441749487\n",
      " Regularized Logistic Regression GD (30000/99999): loss=-4810.640799085971\n",
      " Regularized Logistic Regression GD (30100/99999): loss=-4810.775303199531\n",
      " Regularized Logistic Regression GD (30200/99999): loss=-4810.90701374851\n",
      " Regularized Logistic Regression GD (30300/99999): loss=-4811.035989132698\n",
      " Regularized Logistic Regression GD (30400/99999): loss=-4811.162286520192\n",
      " Regularized Logistic Regression GD (30500/99999): loss=-4811.285961873701\n",
      " Regularized Logistic Regression GD (30600/99999): loss=-4811.407069976289\n",
      " Regularized Logistic Regression GD (30700/99999): loss=-4811.525664456507\n",
      " Regularized Logistic Regression GD (30800/99999): loss=-4811.641797813054\n",
      " Regularized Logistic Regression GD (30900/99999): loss=-4811.755521438851\n",
      " Regularized Logistic Regression GD (31000/99999): loss=-4811.8668856446\n",
      " Regularized Logistic Regression GD (31100/99999): loss=-4811.975939681838\n",
      " Regularized Logistic Regression GD (31200/99999): loss=-4812.082731765517\n",
      " Regularized Logistic Regression GD (31300/99999): loss=-4812.187309096041\n",
      " Regularized Logistic Regression GD (31400/99999): loss=-4812.289717880879\n",
      " Regularized Logistic Regression GD (31500/99999): loss=-4812.390003355672\n",
      " Regularized Logistic Regression GD (31600/99999): loss=-4812.488209804905\n",
      " Regularized Logistic Regression GD (31700/99999): loss=-4812.584380582143\n",
      " Regularized Logistic Regression GD (31800/99999): loss=-4812.678558129781\n",
      " Regularized Logistic Regression GD (31900/99999): loss=-4812.770783998441\n",
      " Regularized Logistic Regression GD (32000/99999): loss=-4812.861098865875\n",
      " Regularized Logistic Regression GD (32100/99999): loss=-4812.949542555525\n",
      " Regularized Logistic Regression GD (32200/99999): loss=-4813.036154054626\n",
      " Regularized Logistic Regression GD (32300/99999): loss=-4813.120971531985\n",
      " Regularized Logistic Regression GD (32400/99999): loss=-4813.204032355293\n",
      " Regularized Logistic Regression GD (32500/99999): loss=-4813.28537310816\n",
      " Regularized Logistic Regression GD (32600/99999): loss=-4813.365029606693\n",
      " Regularized Logistic Regression GD (32700/99999): loss=-4813.443036915785\n",
      " Regularized Logistic Regression GD (32800/99999): loss=-4813.519429365011\n",
      " Regularized Logistic Regression GD (32900/99999): loss=-4813.594240564233\n",
      " Regularized Logistic Regression GD (33000/99999): loss=-4813.667503418784\n",
      " Regularized Logistic Regression GD (33100/99999): loss=-4813.739250144426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Regularized Logistic Regression GD (33200/99999): loss=-4813.809512281921\n",
      " Regularized Logistic Regression GD (33300/99999): loss=-4813.878320711301\n",
      " Regularized Logistic Regression GD (33400/99999): loss=-4813.94570566586\n",
      " Regularized Logistic Regression GD (33500/99999): loss=-4814.011696745802\n",
      " Regularized Logistic Regression GD (33600/99999): loss=-4814.076322931651\n",
      " Regularized Logistic Regression GD (33700/99999): loss=-4814.139612597307\n",
      " Regularized Logistic Regression GD (33800/99999): loss=-4814.201593522888\n",
      " Regularized Logistic Regression GD (33900/99999): loss=-4814.262292907268\n",
      " Regularized Logistic Regression GD (34000/99999): loss=-4814.321737380318\n",
      " Regularized Logistic Regression GD (34100/99999): loss=-4814.379953014953\n",
      " Regularized Logistic Regression GD (34200/99999): loss=-4814.436965338865\n",
      " Regularized Logistic Regression GD (34300/99999): loss=-4814.492799346009\n",
      " Regularized Logistic Regression GD (34400/99999): loss=-4814.547479507894\n",
      " Regularized Logistic Regression GD (34500/99999): loss=-4814.6010297845705\n",
      " Regularized Logistic Regression GD (34600/99999): loss=-4814.65347363542\n",
      " Regularized Logistic Regression GD (34700/99999): loss=-4814.704834029695\n",
      " Regularized Logistic Regression GD (34800/99999): loss=-4814.755133456874\n",
      " Regularized Logistic Regression GD (34900/99999): loss=-4814.8043939367235\n",
      " Regularized Logistic Regression GD (35000/99999): loss=-4814.852637029233\n",
      " Regularized Logistic Regression GD (35100/99999): loss=-4814.899883844251\n",
      " Regularized Logistic Regression GD (35200/99999): loss=-4814.946155050998\n",
      " Regularized Logistic Regression GD (35300/99999): loss=-4814.991470887313\n",
      " Regularized Logistic Regression GD (35400/99999): loss=-4815.0358511687355\n",
      " Regularized Logistic Regression GD (35500/99999): loss=-4815.0793152973865\n",
      " Regularized Logistic Regression GD (35600/99999): loss=-4815.121882270664\n",
      " Regularized Logistic Regression GD (35700/99999): loss=-4815.163570689749\n",
      " Regularized Logistic Regression GD (35800/99999): loss=-4815.204398767928\n",
      " Regularized Logistic Regression GD (35900/99999): loss=-4815.244384338754\n",
      " Regularized Logistic Regression GD (36000/99999): loss=-4815.28354486402\n",
      " Regularized Logistic Regression GD (36100/99999): loss=-4815.321897441559\n",
      " Regularized Logistic Regression GD (36200/99999): loss=-4815.359458812897\n",
      " Regularized Logistic Regression GD (36300/99999): loss=-4815.396245370722\n",
      " Regularized Logistic Regression GD (36400/99999): loss=-4815.432273166229\n",
      " Regularized Logistic Regression GD (36500/99999): loss=-4815.467557916252\n",
      " Regularized Logistic Regression GD (36600/99999): loss=-4815.502115010303\n",
      " Regularized Logistic Regression GD (36700/99999): loss=-4815.535959517442\n",
      " Regularized Logistic Regression GD (36800/99999): loss=-4815.569106192974\n",
      " Regularized Logistic Regression GD (36900/99999): loss=-4815.601569485047\n",
      " Regularized Logistic Regression GD (37000/99999): loss=-4815.633363541084\n",
      " Regularized Logistic Regression GD (37100/99999): loss=-4815.6645022140865\n",
      " Regularized Logistic Regression GD (37200/99999): loss=-4815.6949990688045\n",
      " Regularized Logistic Regression GD (37300/99999): loss=-4815.7248673877775\n",
      " Regularized Logistic Regression GD (37400/99999): loss=-4815.754120177236\n",
      " Regularized Logistic Regression GD (37500/99999): loss=-4815.782770172904\n",
      " Regularized Logistic Regression GD (37600/99999): loss=-4815.810829845649\n",
      " Regularized Logistic Regression GD (37700/99999): loss=-4815.838311407031\n",
      " Regularized Logistic Regression GD (37800/99999): loss=-4815.865226814737\n",
      " Regularized Logistic Regression GD (37900/99999): loss=-4815.8915877778845\n",
      " Regularized Logistic Regression GD (38000/99999): loss=-4815.917405762219\n",
      " Regularized Logistic Regression GD (38100/99999): loss=-4815.94269199521\n",
      " Regularized Logistic Regression GD (38200/99999): loss=-4815.967457471047\n",
      " Regularized Logistic Regression GD (38300/99999): loss=-4815.991712955508\n",
      " Regularized Logistic Regression GD (38400/99999): loss=-4816.015468990703\n",
      " Regularized Logistic Regression GD (38500/99999): loss=-4816.0387358998105\n",
      " Regularized Logistic Regression GD (38600/99999): loss=-4816.061523791611\n",
      " Regularized Logistic Regression GD (38700/99999): loss=-4816.08384256497\n",
      " Regularized Logistic Regression GD (38800/99999): loss=-4816.105701913243\n",
      " Regularized Logistic Regression GD (38900/99999): loss=-4816.1271113285475\n",
      " Regularized Logistic Regression GD (39000/99999): loss=-4816.1480801059815\n",
      " Regularized Logistic Regression GD (39100/99999): loss=-4816.168617347721\n",
      " Regularized Logistic Regression GD (39200/99999): loss=-4816.188731967061\n",
      " Regularized Logistic Regression GD (39300/99999): loss=-4816.208432692364\n",
      " Regularized Logistic Regression GD (39400/99999): loss=-4816.227728070893\n",
      " Regularized Logistic Regression GD (39500/99999): loss=-4816.246626472625\n",
      " Regularized Logistic Regression GD (39600/99999): loss=-4816.2651360939235\n",
      " Regularized Logistic Regression GD (39700/99999): loss=-4816.283264961167\n",
      " Regularized Logistic Regression GD (39800/99999): loss=-4816.301020934309\n",
      " Regularized Logistic Regression GD (39900/99999): loss=-4816.318411710316\n",
      " Regularized Logistic Regression GD (40000/99999): loss=-4816.335444826608\n",
      " Regularized Logistic Regression GD (40100/99999): loss=-4816.352127664343\n",
      " Regularized Logistic Regression GD (40200/99999): loss=-4816.368467451706\n",
      " Regularized Logistic Regression GD (40300/99999): loss=-4816.384471267088\n",
      " Regularized Logistic Regression GD (40400/99999): loss=-4816.400146042193\n",
      " Regularized Logistic Regression GD (40500/99999): loss=-4816.415498565109\n",
      " Regularized Logistic Regression GD (40600/99999): loss=-4816.4305354832995\n",
      " Regularized Logistic Regression GD (40700/99999): loss=-4816.445263306529\n",
      " Regularized Logistic Regression GD (40800/99999): loss=-4816.459688409724\n",
      " Regularized Logistic Regression GD (40900/99999): loss=-4816.473817035792\n",
      " Regularized Logistic Regression GD (41000/99999): loss=-4816.4876552983715\n",
      " Regularized Logistic Regression GD (41100/99999): loss=-4816.501209184501\n",
      " Regularized Logistic Regression GD (41200/99999): loss=-4816.514484557279\n",
      " Regularized Logistic Regression GD (41300/99999): loss=-4816.527487158433\n",
      " Regularized Logistic Regression GD (41400/99999): loss=-4816.540222610846\n",
      " Regularized Logistic Regression GD (41500/99999): loss=-4816.552696421026\n",
      " Regularized Logistic Regression GD (41600/99999): loss=-4816.564913981525\n",
      " Regularized Logistic Regression GD (41700/99999): loss=-4816.576880573317\n",
      " Regularized Logistic Regression GD (41800/99999): loss=-4816.588601368118\n",
      " Regularized Logistic Regression GD (41900/99999): loss=-4816.600081430648\n",
      " Regularized Logistic Regression GD (42000/99999): loss=-4816.611325720865\n",
      " Regularized Logistic Regression GD (42100/99999): loss=-4816.622339096134\n",
      " Regularized Logistic Regression GD (42200/99999): loss=-4816.63312631337\n",
      " Regularized Logistic Regression GD (42300/99999): loss=-4816.6436920311135\n",
      " Regularized Logistic Regression GD (42400/99999): loss=-4816.654040811587\n",
      " Regularized Logistic Regression GD (42500/99999): loss=-4816.664177122686\n",
      " Regularized Logistic Regression GD (42600/99999): loss=-4816.674105339944\n",
      " Regularized Logistic Regression GD (42700/99999): loss=-4816.683829748448\n",
      " Regularized Logistic Regression GD (42800/99999): loss=-4816.693354544718\n",
      " Regularized Logistic Regression GD (42900/99999): loss=-4816.702683838545\n",
      " Regularized Logistic Regression GD (43000/99999): loss=-4816.711821654793\n",
      " Regularized Logistic Regression GD (43100/99999): loss=-4816.72077193517\n",
      " Regularized Logistic Regression GD (43200/99999): loss=-4816.7295385399275\n",
      " Regularized Logistic Regression GD (43300/99999): loss=-4816.738125249585\n",
      " Regularized Logistic Regression GD (43400/99999): loss=-4816.746535766572\n",
      " Regularized Logistic Regression GD (43500/99999): loss=-4816.754773716825\n",
      " Regularized Logistic Regression GD (43600/99999): loss=-4816.762842651412\n",
      " Regularized Logistic Regression GD (43700/99999): loss=-4816.770746048069\n",
      " Regularized Logistic Regression GD (43800/99999): loss=-4816.778487312709\n",
      " Regularized Logistic Regression GD (43900/99999): loss=-4816.786069780929\n",
      " Regularized Logistic Regression GD (44000/99999): loss=-4816.793496719469\n",
      " Regularized Logistic Regression GD (44100/99999): loss=-4816.800771327619\n",
      " Regularized Logistic Regression GD (44200/99999): loss=-4816.807896738644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Regularized Logistic Regression GD (44300/99999): loss=-4816.814876021127\n",
      " Regularized Logistic Regression GD (44400/99999): loss=-4816.821712180339\n",
      " Regularized Logistic Regression GD (44500/99999): loss=-4816.828408159519\n",
      " Regularized Logistic Regression GD (44600/99999): loss=-4816.834966841185\n",
      " Regularized Logistic Regression GD (44700/99999): loss=-4816.841391048381\n",
      " Regularized Logistic Regression GD (44800/99999): loss=-4816.847683545903\n",
      " Regularized Logistic Regression GD (44900/99999): loss=-4816.853847041524\n",
      " Regularized Logistic Regression GD (45000/99999): loss=-4816.85988418716\n",
      " Regularized Logistic Regression GD (45100/99999): loss=-4816.8657975800215\n",
      " Regularized Logistic Regression GD (45200/99999): loss=-4816.871589763774\n",
      " Regularized Logistic Regression GD (45300/99999): loss=-4816.877263229618\n",
      " Regularized Logistic Regression GD (45400/99999): loss=-4816.882820417382\n",
      " Regularized Logistic Regression GD (45500/99999): loss=-4816.888263716602\n",
      " Regularized Logistic Regression GD (45600/99999): loss=-4816.893595467542\n",
      " Regularized Logistic Regression GD (45700/99999): loss=-4816.898817962223\n",
      " Regularized Logistic Regression GD (45800/99999): loss=-4816.9039334454255\n",
      " Regularized Logistic Regression GD (45900/99999): loss=-4816.90894411566\n",
      " Regularized Logistic Regression GD (46000/99999): loss=-4816.913852126122\n",
      " Regularized Logistic Regression GD (46100/99999): loss=-4816.918659585643\n",
      " Regularized Logistic Regression GD (46200/99999): loss=-4816.923368559593\n",
      " Regularized Logistic Regression GD (46300/99999): loss=-4816.927981070786\n",
      " Regularized Logistic Regression GD (46400/99999): loss=-4816.932499100364\n",
      " Regularized Logistic Regression GD (46500/99999): loss=-4816.936924588654\n",
      " Regularized Logistic Regression GD (46600/99999): loss=-4816.941259436013\n",
      " Regularized Logistic Regression GD (46700/99999): loss=-4816.94550550365\n",
      " Regularized Logistic Regression GD (46800/99999): loss=-4816.949664614445\n",
      " Regularized Logistic Regression GD (46900/99999): loss=-4816.953738553738\n",
      " Regularized Logistic Regression GD (47000/99999): loss=-4816.9577290700945\n",
      " Regularized Logistic Regression GD (47100/99999): loss=-4816.96163787609\n",
      " Regularized Logistic Regression GD (47200/99999): loss=-4816.965466649016\n",
      " Regularized Logistic Regression GD (47300/99999): loss=-4816.969217031652\n",
      " Regularized Logistic Regression GD (47400/99999): loss=-4816.972890632942\n",
      " Regularized Logistic Regression GD (47500/99999): loss=-4816.976489028718\n",
      " Regularized Logistic Regression GD (47600/99999): loss=-4816.980013762378\n",
      " Regularized Logistic Regression GD (47700/99999): loss=-4816.983466345544\n",
      " Regularized Logistic Regression GD (47800/99999): loss=-4816.986848258733\n",
      " Regularized Logistic Regression GD (47900/99999): loss=-4816.990160951996\n",
      " Regularized Logistic Regression GD (48000/99999): loss=-4816.993405845542\n",
      " Regularized Logistic Regression GD (48100/99999): loss=-4816.996584330361\n",
      " Regularized Logistic Regression GD (48200/99999): loss=-4816.999697768816\n",
      " Regularized Logistic Regression GD (48300/99999): loss=-4817.002747495252\n",
      " Regularized Logistic Regression GD (48400/99999): loss=-4817.0057348165565\n",
      " Regularized Logistic Regression GD (48500/99999): loss=-4817.008661012737\n",
      " Regularized Logistic Regression GD (48600/99999): loss=-4817.0115273374795\n",
      " Regularized Logistic Regression GD (48700/99999): loss=-4817.014335018666\n",
      " Regularized Logistic Regression GD (48800/99999): loss=-4817.017085258949\n",
      " Regularized Logistic Regression GD (48900/99999): loss=-4817.019779236229\n",
      " Regularized Logistic Regression GD (49000/99999): loss=-4817.022418104206\n",
      " Regularized Logistic Regression GD (49100/99999): loss=-4817.025002992836\n",
      " Regularized Logistic Regression GD (49200/99999): loss=-4817.027535008859\n",
      " Regularized Logistic Regression GD (49300/99999): loss=-4817.030015236258\n",
      " Regularized Logistic Regression GD (49400/99999): loss=-4817.0324447367275\n",
      " Regularized Logistic Regression GD (49500/99999): loss=-4817.03482455014\n",
      " Regularized Logistic Regression GD (49600/99999): loss=-4817.0371556949995\n",
      " Regularized Logistic Regression GD (49700/99999): loss=-4817.039439168866\n",
      " Regularized Logistic Regression GD (49800/99999): loss=-4817.041675948808\n",
      " Regularized Logistic Regression GD (49900/99999): loss=-4817.043866991809\n",
      " Regularized Logistic Regression GD (50000/99999): loss=-4817.046013235195\n",
      " Regularized Logistic Regression GD (50100/99999): loss=-4817.048115597013\n",
      " Regularized Logistic Regression GD (50200/99999): loss=-4817.050174976473\n",
      " Regularized Logistic Regression GD (50300/99999): loss=-4817.052192254295\n",
      " Regularized Logistic Regression GD (50400/99999): loss=-4817.054168293107\n",
      " Regularized Logistic Regression GD (50500/99999): loss=-4817.056103937818\n",
      " Regularized Logistic Regression GD (50600/99999): loss=-4817.05800001599\n",
      " Regularized Logistic Regression GD (50700/99999): loss=-4817.059857338169\n",
      " Regularized Logistic Regression GD (50800/99999): loss=-4817.061676698272\n",
      " Regularized Logistic Regression GD (50900/99999): loss=-4817.063458873902\n",
      " Regularized Logistic Regression GD (51000/99999): loss=-4817.065204626692\n",
      " Regularized Logistic Regression GD (51100/99999): loss=-4817.0669147026465\n",
      " Regularized Logistic Regression GD (51200/99999): loss=-4817.068589832439\n",
      " Regularized Logistic Regression GD (51300/99999): loss=-4817.07023073174\n",
      " Regularized Logistic Regression GD (51400/99999): loss=-4817.071838101538\n",
      " Regularized Logistic Regression GD (51500/99999): loss=-4817.0734126284315\n",
      " Regularized Logistic Regression GD (51600/99999): loss=-4817.074954984909\n",
      " Regularized Logistic Regression GD (51700/99999): loss=-4817.076465829669\n",
      " Regularized Logistic Regression GD (51800/99999): loss=-4817.077945807889\n",
      " Regularized Logistic Regression GD (51900/99999): loss=-4817.079395551493\n",
      " Regularized Logistic Regression GD (52000/99999): loss=-4817.080815679451\n",
      " Regularized Logistic Regression GD (52100/99999): loss=-4817.082206798014\n",
      " Regularized Logistic Regression GD (52200/99999): loss=-4817.083569501015\n",
      " Regularized Logistic Regression GD (52300/99999): loss=-4817.084904370084\n",
      " Regularized Logistic Regression GD (52400/99999): loss=-4817.086211974915\n",
      " Regularized Logistic Regression GD (52500/99999): loss=-4817.087492873529\n",
      " Regularized Logistic Regression GD (52600/99999): loss=-4817.0887476124935\n",
      " Regularized Logistic Regression GD (52700/99999): loss=-4817.089976727143\n",
      " Regularized Logistic Regression GD (52800/99999): loss=-4817.09118074187\n",
      " Regularized Logistic Regression GD (52900/99999): loss=-4817.0923601702725\n",
      " Regularized Logistic Regression GD (53000/99999): loss=-4817.0935155154375\n",
      " Regularized Logistic Regression GD (53100/99999): loss=-4817.094647270124\n",
      " Regularized Logistic Regression GD (53200/99999): loss=-4817.095755916991\n",
      " Regularized Logistic Regression GD (53300/99999): loss=-4817.096841928795\n",
      " Regularized Logistic Regression GD (53400/99999): loss=-4817.097905768592\n",
      " Regularized Logistic Regression GD (53500/99999): loss=-4817.098947889956\n",
      " Regularized Logistic Regression GD (53600/99999): loss=-4817.099968737161\n",
      " Regularized Logistic Regression GD (53700/99999): loss=-4817.100968745363\n",
      " Regularized Logistic Regression GD (53800/99999): loss=-4817.101948340798\n",
      " Regularized Logistic Regression GD (53900/99999): loss=-4817.102907940959\n",
      " Regularized Logistic Regression GD (54000/99999): loss=-4817.1038479548015\n",
      " Regularized Logistic Regression GD (54100/99999): loss=-4817.1047687828805\n",
      " Regularized Logistic Regression GD (54200/99999): loss=-4817.105670817537\n",
      " Regularized Logistic Regression GD (54300/99999): loss=-4817.106554443088\n",
      " Regularized Logistic Regression GD (54400/99999): loss=-4817.107420035959\n",
      " Regularized Logistic Regression GD (54500/99999): loss=-4817.108267964868\n",
      " Regularized Logistic Regression GD (54600/99999): loss=-4817.109098590973\n",
      " Regularized Logistic Regression GD (54700/99999): loss=-4817.109912268038\n",
      " Regularized Logistic Regression GD (54800/99999): loss=-4817.110709342575\n",
      " Regularized Logistic Regression GD (54900/99999): loss=-4817.111490153994\n",
      " Regularized Logistic Regression GD (55000/99999): loss=-4817.112255034745\n",
      " Regularized Logistic Regression GD (55100/99999): loss=-4817.113004310486\n",
      " Regularized Logistic Regression GD (55200/99999): loss=-4817.113738300184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Regularized Logistic Regression GD (55300/99999): loss=-4817.114457316294\n",
      " Regularized Logistic Regression GD (55400/99999): loss=-4817.115161664837\n",
      " Regularized Logistic Regression GD (55500/99999): loss=-4817.115851645603\n",
      " Regularized Logistic Regression GD (55600/99999): loss=-4817.116527552217\n",
      " Regularized Logistic Regression GD (55700/99999): loss=-4817.117189672299\n",
      " Regularized Logistic Regression GD (55800/99999): loss=-4817.117838287572\n",
      " Regularized Logistic Regression GD (55900/99999): loss=-4817.118473674004\n",
      " Regularized Logistic Regression GD (56000/99999): loss=-4817.119096101899\n",
      " Regularized Logistic Regression GD (56100/99999): loss=-4817.1197058360385\n",
      " Regularized Logistic Regression GD (56200/99999): loss=-4817.120303135764\n",
      " Regularized Logistic Regression GD (56300/99999): loss=-4817.120888255119\n",
      " Regularized Logistic Regression GD (56400/99999): loss=-4817.121461442951\n",
      " Regularized Logistic Regression GD (56500/99999): loss=-4817.1220229430055\n",
      " Regularized Logistic Regression GD (56600/99999): loss=-4817.122572994033\n",
      " Regularized Logistic Regression GD (56700/99999): loss=-4817.123111829909\n",
      " Regularized Logistic Regression GD (56800/99999): loss=-4817.123639679715\n",
      " Regularized Logistic Regression GD (56900/99999): loss=-4817.124156767844\n",
      " Regularized Logistic Regression GD (57000/99999): loss=-4817.124663314096\n",
      " Regularized Logistic Regression GD (57100/99999): loss=-4817.1251595337735\n",
      " Regularized Logistic Regression GD (57200/99999): loss=-4817.125645637774\n",
      " Regularized Logistic Regression GD (57300/99999): loss=-4817.126121832681\n",
      " Regularized Logistic Regression GD (57400/99999): loss=-4817.12658832084\n",
      " Regularized Logistic Regression GD (57500/99999): loss=-4817.127045300467\n",
      " Regularized Logistic Regression GD (57600/99999): loss=-4817.127492965718\n",
      " Regularized Logistic Regression GD (57700/99999): loss=-4817.127931506786\n",
      " Regularized Logistic Regression GD (57800/99999): loss=-4817.128361109953\n",
      " Regularized Logistic Regression GD (57900/99999): loss=-4817.128781957706\n",
      " Regularized Logistic Regression GD (58000/99999): loss=-4817.129194228798\n",
      " Regularized Logistic Regression GD (58100/99999): loss=-4817.129598098314\n",
      " Regularized Logistic Regression GD (58200/99999): loss=-4817.12999373777\n",
      " Regularized Logistic Regression GD (58300/99999): loss=-4817.130381315164\n",
      " Regularized Logistic Regression GD (58400/99999): loss=-4817.130760995072\n",
      " Regularized Logistic Regression GD (58500/99999): loss=-4817.131132938682\n",
      " Regularized Logistic Regression GD (58600/99999): loss=-4817.1314973039125\n",
      " Regularized Logistic Regression GD (58700/99999): loss=-4817.131854245427\n",
      " Regularized Logistic Regression GD (58800/99999): loss=-4817.132203914744\n",
      " Regularized Logistic Regression GD (58900/99999): loss=-4817.132546460278\n",
      " Regularized Logistic Regression GD (59000/99999): loss=-4817.132882027397\n",
      " Regularized Logistic Regression GD (59100/99999): loss=-4817.133210758521\n",
      " Regularized Logistic Regression GD (59200/99999): loss=-4817.1335327931365\n",
      " Regularized Logistic Regression GD (59300/99999): loss=-4817.133848267891\n",
      " Regularized Logistic Regression GD (59400/99999): loss=-4817.134157316632\n",
      " Regularized Logistic Regression GD (59500/99999): loss=-4817.134460070477\n",
      " Regularized Logistic Regression GD (59600/99999): loss=-4817.13475665785\n",
      " Regularized Logistic Regression GD (59700/99999): loss=-4817.135047204566\n",
      " Regularized Logistic Regression GD (59800/99999): loss=-4817.135331833856\n",
      " Regularized Logistic Regression GD (59900/99999): loss=-4817.135610666438\n",
      " Regularized Logistic Regression GD (60000/99999): loss=-4817.135883820565\n",
      " Regularized Logistic Regression GD (60100/99999): loss=-4817.136151412074\n",
      " Regularized Logistic Regression GD (60200/99999): loss=-4817.136413554418\n",
      " Regularized Logistic Regression GD (60300/99999): loss=-4817.13667035876\n",
      " Regularized Logistic Regression GD (60400/99999): loss=-4817.136921933963\n",
      " Regularized Logistic Regression GD (60500/99999): loss=-4817.137168386684\n",
      " Regularized Logistic Regression GD (60600/99999): loss=-4817.137409821397\n",
      " Regularized Logistic Regression GD (60700/99999): loss=-4817.137646340431\n",
      " Regularized Logistic Regression GD (60800/99999): loss=-4817.137878044042\n",
      " Regularized Logistic Regression GD (60900/99999): loss=-4817.1381050304235\n",
      " Regularized Logistic Regression GD (61000/99999): loss=-4817.13832739577\n",
      " Regularized Logistic Regression GD (61100/99999): loss=-4817.138545234308\n",
      " Regularized Logistic Regression GD (61200/99999): loss=-4817.138758638338\n",
      " Regularized Logistic Regression GD (61300/99999): loss=-4817.138967698277\n",
      " Regularized Logistic Regression GD (61400/99999): loss=-4817.139172502698\n",
      " Regularized Logistic Regression GD (61500/99999): loss=-4817.139373138364\n",
      " Regularized Logistic Regression GD (61600/99999): loss=-4817.139569690261\n",
      " Regularized Logistic Regression GD (61700/99999): loss=-4817.139762241643\n",
      " Regularized Logistic Regression GD (61800/99999): loss=-4817.13995087406\n",
      " Regularized Logistic Regression GD (61900/99999): loss=-4817.140135667398\n",
      " Regularized Logistic Regression GD (62000/99999): loss=-4817.140316699921\n",
      " Regularized Logistic Regression GD (62100/99999): loss=-4817.14049404828\n",
      " Regularized Logistic Regression GD (62200/99999): loss=-4817.140667787575\n",
      " Regularized Logistic Regression GD (62300/99999): loss=-4817.140837991362\n",
      " Regularized Logistic Regression GD (62400/99999): loss=-4817.141004731695\n",
      " Regularized Logistic Regression GD (62500/99999): loss=-4817.141168079166\n",
      " Regularized Logistic Regression GD (62600/99999): loss=-4817.141328102913\n",
      " Regularized Logistic Regression GD (62700/99999): loss=-4817.141484870685\n",
      " Regularized Logistic Regression GD (62800/99999): loss=-4817.141638448819\n",
      " Regularized Logistic Regression GD (62900/99999): loss=-4817.141788902316\n",
      " Regularized Logistic Regression GD (63000/99999): loss=-4817.141936294846\n",
      " Regularized Logistic Regression GD (63100/99999): loss=-4817.14208068878\n",
      " Regularized Logistic Regression GD (63200/99999): loss=-4817.142222145215\n",
      " Regularized Logistic Regression GD (63300/99999): loss=-4817.142360724004\n",
      " Regularized Logistic Regression GD (63400/99999): loss=-4817.1424964837715\n",
      " Regularized Logistic Regression GD (63500/99999): loss=-4817.142629481946\n",
      " Regularized Logistic Regression GD (63600/99999): loss=-4817.142759774805\n",
      " Regularized Logistic Regression GD (63700/99999): loss=-4817.142887417442\n",
      " Regularized Logistic Regression GD (63800/99999): loss=-4817.143012463853\n",
      " Regularized Logistic Regression GD (63900/99999): loss=-4817.143134966927\n",
      " Regularized Logistic Regression GD (64000/99999): loss=-4817.143254978456\n",
      " Regularized Logistic Regression GD (64100/99999): loss=-4817.143372549209\n",
      " Regularized Logistic Regression GD (64200/99999): loss=-4817.1434877288875\n",
      " Regularized Logistic Regression GD (64300/99999): loss=-4817.143600566199\n",
      " Regularized Logistic Regression GD (64400/99999): loss=-4817.143711108845\n",
      " Regularized Logistic Regression GD (64500/99999): loss=-4817.14381940356\n",
      " Regularized Logistic Regression GD (64600/99999): loss=-4817.143925496129\n",
      " Regularized Logistic Regression GD (64700/99999): loss=-4817.14402943139\n",
      " Regularized Logistic Regression GD (64800/99999): loss=-4817.144131253282\n",
      " Regularized Logistic Regression GD (64900/99999): loss=-4817.144231004832\n",
      " Regularized Logistic Regression GD (65000/99999): loss=-4817.144328728203\n",
      " Regularized Logistic Regression GD (65100/99999): loss=-4817.144424464693\n",
      " Regularized Logistic Regression GD (65200/99999): loss=-4817.144518254759\n",
      " Regularized Logistic Regression GD (65300/99999): loss=-4817.14461013802\n",
      " Regularized Logistic Regression GD (65400/99999): loss=-4817.144700153316\n",
      " Regularized Logistic Regression GD (65500/99999): loss=-4817.144788338664\n",
      " Regularized Logistic Regression GD (65600/99999): loss=-4817.144874731325\n",
      " Regularized Logistic Regression GD (65700/99999): loss=-4817.144959367789\n",
      " Regularized Logistic Regression GD (65800/99999): loss=-4817.145042283817\n",
      " Regularized Logistic Regression GD (65900/99999): loss=-4817.145123514422\n",
      " Regularized Logistic Regression GD (66000/99999): loss=-4817.145203093911\n",
      " Regularized Logistic Regression GD (66100/99999): loss=-4817.145281055902\n",
      " Regularized Logistic Regression GD (66200/99999): loss=-4817.145357433308\n",
      " Regularized Logistic Regression GD (66300/99999): loss=-4817.14543225839\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Regularized Logistic Regression GD (66400/99999): loss=-4817.145505562739\n",
      " Regularized Logistic Regression GD (66500/99999): loss=-4817.145577377308\n",
      " Regularized Logistic Regression GD (66600/99999): loss=-4817.145647732413\n",
      " Regularized Logistic Regression GD (66700/99999): loss=-4817.1457166577575\n",
      " Regularized Logistic Regression GD (66800/99999): loss=-4817.145784182436\n",
      " Regularized Logistic Regression GD (66900/99999): loss=-4817.145850334957\n",
      " Regularized Logistic Regression GD (67000/99999): loss=-4817.145915143238\n",
      " Regularized Logistic Regression GD (67100/99999): loss=-4817.145978634638\n",
      " Regularized Logistic Regression GD (67200/99999): loss=-4817.146040835946\n",
      " Regularized Logistic Regression GD (67300/99999): loss=-4817.146101773415\n",
      " Regularized Logistic Regression GD (67400/99999): loss=-4817.146161472758\n",
      " Regularized Logistic Regression GD (67500/99999): loss=-4817.14621995917\n",
      " Regularized Logistic Regression GD (67600/99999): loss=-4817.14627725732\n",
      " Regularized Logistic Regression GD (67700/99999): loss=-4817.146333391398\n",
      " Regularized Logistic Regression GD (67800/99999): loss=-4817.146388385072\n",
      " Regularized Logistic Regression GD (67900/99999): loss=-4817.146442261549\n",
      " Regularized Logistic Regression GD (68000/99999): loss=-4817.146495043548\n",
      " Regularized Logistic Regression GD (68100/99999): loss=-4817.146546753346\n",
      " Regularized Logistic Regression GD (68200/99999): loss=-4817.14659741274\n",
      " Regularized Logistic Regression GD (68300/99999): loss=-4817.146647043106\n",
      " Regularized Logistic Regression GD (68400/99999): loss=-4817.146695665366\n",
      " Regularized Logistic Regression GD (68500/99999): loss=-4817.146743300031\n",
      " Regularized Logistic Regression GD (68600/99999): loss=-4817.146789967187\n",
      " Regularized Logistic Regression GD (68700/99999): loss=-4817.146835686508\n",
      " Regularized Logistic Regression GD (68800/99999): loss=-4817.146880477272\n",
      " Regularized Logistic Regression GD (68900/99999): loss=-4817.146924358367\n",
      " Regularized Logistic Regression GD (69000/99999): loss=-4817.14696734829\n",
      " Regularized Logistic Regression GD (69100/99999): loss=-4817.1470094651595\n",
      " Regularized Logistic Regression GD (69200/99999): loss=-4817.147050726737\n",
      " Regularized Logistic Regression GD (69300/99999): loss=-4817.147091150407\n",
      " Regularized Logistic Regression GD (69400/99999): loss=-4817.147130753211\n",
      " Regularized Logistic Regression GD (69500/99999): loss=-4817.147169551841\n",
      " Regularized Logistic Regression GD (69600/99999): loss=-4817.147207562649\n",
      " Regularized Logistic Regression GD (69700/99999): loss=-4817.1472448016475\n",
      " Regularized Logistic Regression GD (69800/99999): loss=-4817.147281284538\n",
      " Regularized Logistic Regression GD (69900/99999): loss=-4817.147317026684\n",
      " Regularized Logistic Regression GD (70000/99999): loss=-4817.147352043147\n",
      " Regularized Logistic Regression GD (70100/99999): loss=-4817.147386348684\n",
      " Regularized Logistic Regression GD (70200/99999): loss=-4817.147419957743\n",
      " Regularized Logistic Regression GD (70300/99999): loss=-4817.147452884485\n",
      " Regularized Logistic Regression GD (70400/99999): loss=-4817.147485142776\n",
      " Regularized Logistic Regression GD (70500/99999): loss=-4817.147516746213\n",
      " Regularized Logistic Regression GD (70600/99999): loss=-4817.147547708097\n",
      " Regularized Logistic Regression GD (70700/99999): loss=-4817.147578041473\n",
      " Regularized Logistic Regression GD (70800/99999): loss=-4817.147607759114\n",
      " Regularized Logistic Regression GD (70900/99999): loss=-4817.147636873536\n",
      " Regularized Logistic Regression GD (71000/99999): loss=-4817.147665397002\n",
      " Regularized Logistic Regression GD (71100/99999): loss=-4817.147693341513\n",
      " Regularized Logistic Regression GD (71200/99999): loss=-4817.14772071885\n",
      " Regularized Logistic Regression GD (71300/99999): loss=-4817.147747540525\n",
      " Regularized Logistic Regression GD (71400/99999): loss=-4817.147773817837\n",
      " Regularized Logistic Regression GD (71500/99999): loss=-4817.147799561852\n",
      " Regularized Logistic Regression GD (71600/99999): loss=-4817.147824783394\n",
      " Regularized Logistic Regression GD (71700/99999): loss=-4817.14784949309\n",
      " Regularized Logistic Regression GD (71800/99999): loss=-4817.147873701339\n",
      " Regularized Logistic Regression GD (71900/99999): loss=-4817.147897418332\n",
      " Regularized Logistic Regression GD (72000/99999): loss=-4817.147920654049\n",
      " Regularized Logistic Regression GD (72100/99999): loss=-4817.147943418263\n",
      " Regularized Logistic Regression GD (72200/99999): loss=-4817.147965720557\n",
      " Regularized Logistic Regression GD (72300/99999): loss=-4817.147987570323\n",
      " Regularized Logistic Regression GD (72400/99999): loss=-4817.148008976748\n",
      " Regularized Logistic Regression GD (72500/99999): loss=-4817.148029948836\n",
      " Regularized Logistic Regression GD (72600/99999): loss=-4817.148050495412\n",
      " Regularized Logistic Regression GD (72700/99999): loss=-4817.148070625124\n",
      " Regularized Logistic Regression GD (72800/99999): loss=-4817.148090346441\n",
      " Regularized Logistic Regression GD (72900/99999): loss=-4817.148109667652\n",
      " Regularized Logistic Regression GD (73000/99999): loss=-4817.148128596889\n",
      " Regularized Logistic Regression GD (73100/99999): loss=-4817.148147142111\n",
      " Regularized Logistic Regression GD (73200/99999): loss=-4817.148165311125\n",
      " Regularized Logistic Regression GD (73300/99999): loss=-4817.148183111561\n",
      " Regularized Logistic Regression GD (73400/99999): loss=-4817.1482005509115\n",
      " Regularized Logistic Regression GD (73500/99999): loss=-4817.148217636512\n",
      " Regularized Logistic Regression GD (73600/99999): loss=-4817.148234375548\n",
      " Regularized Logistic Regression GD (73700/99999): loss=-4817.148250775048\n",
      " Regularized Logistic Regression GD (73800/99999): loss=-4817.148266841914\n",
      " Regularized Logistic Regression GD (73900/99999): loss=-4817.148282582902\n",
      " Regularized Logistic Regression GD (74000/99999): loss=-4817.148298004625\n",
      " Regularized Logistic Regression GD (74100/99999): loss=-4817.148313113576\n",
      " Regularized Logistic Regression GD (74200/99999): loss=-4817.1483279160975\n",
      " Regularized Logistic Regression GD (74300/99999): loss=-4817.1483424184125\n",
      " Regularized Logistic Regression GD (74400/99999): loss=-4817.148356626616\n",
      " Regularized Logistic Regression GD (74500/99999): loss=-4817.148370546684\n",
      " Regularized Logistic Regression GD (74600/99999): loss=-4817.148384184465\n",
      " Regularized Logistic Regression GD (74700/99999): loss=-4817.1483975456895\n",
      " Regularized Logistic Regression GD (74800/99999): loss=-4817.148410635974\n",
      " Regularized Logistic Regression GD (74900/99999): loss=-4817.148423460813\n",
      " Regularized Logistic Regression GD (75000/99999): loss=-4817.148436025604\n",
      " Regularized Logistic Regression GD (75100/99999): loss=-4817.148448335621\n",
      " Regularized Logistic Regression GD (75200/99999): loss=-4817.148460396042\n",
      " Regularized Logistic Regression GD (75300/99999): loss=-4817.148472211924\n",
      " Regularized Logistic Regression GD (75400/99999): loss=-4817.14848378824\n",
      " Regularized Logistic Regression GD (75500/99999): loss=-4817.148495129846\n",
      " Regularized Logistic Regression GD (75600/99999): loss=-4817.1485062415095\n",
      " Regularized Logistic Regression GD (75700/99999): loss=-4817.1485171279\n",
      " Regularized Logistic Regression GD (75800/99999): loss=-4817.148527793589\n",
      " Regularized Logistic Regression GD (75900/99999): loss=-4817.148538243052\n",
      " Regularized Logistic Regression GD (76000/99999): loss=-4817.148548480681\n",
      " Regularized Logistic Regression GD (76100/99999): loss=-4817.148558510774\n",
      " Regularized Logistic Regression GD (76200/99999): loss=-4817.148568337549\n",
      " Regularized Logistic Regression GD (76300/99999): loss=-4817.148577965122\n",
      " Regularized Logistic Regression GD (76400/99999): loss=-4817.148587397539\n",
      " Regularized Logistic Regression GD (76500/99999): loss=-4817.148596638765\n",
      " Regularized Logistic Regression GD (76600/99999): loss=-4817.148605692677\n",
      " Regularized Logistic Regression GD (76700/99999): loss=-4817.148614563073\n",
      " Regularized Logistic Regression GD (76800/99999): loss=-4817.148623253684\n",
      " Regularized Logistic Regression GD (76900/99999): loss=-4817.148631768152\n",
      " Regularized Logistic Regression GD (77000/99999): loss=-4817.148640110052\n",
      " Regularized Logistic Regression GD (77100/99999): loss=-4817.148648282888\n",
      " Regularized Logistic Regression GD (77200/99999): loss=-4817.148656290089\n",
      " Regularized Logistic Regression GD (77300/99999): loss=-4817.148664135016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Regularized Logistic Regression GD (77400/99999): loss=-4817.148671820955\n",
      " Regularized Logistic Regression GD (77500/99999): loss=-4817.148679351146\n",
      " Regularized Logistic Regression GD (77600/99999): loss=-4817.148686728736\n",
      " Regularized Logistic Regression GD (77700/99999): loss=-4817.148693956828\n",
      " Regularized Logistic Regression GD (77800/99999): loss=-4817.148701038446\n",
      " Regularized Logistic Regression GD (77900/99999): loss=-4817.148707976573\n",
      " Regularized Logistic Regression GD (78000/99999): loss=-4817.148714774111\n",
      " Regularized Logistic Regression GD (78100/99999): loss=-4817.148721433914\n",
      " Regularized Logistic Regression GD (78200/99999): loss=-4817.148727958781\n",
      " Regularized Logistic Regression GD (78300/99999): loss=-4817.148734351444\n",
      " Regularized Logistic Regression GD (78400/99999): loss=-4817.148740614584\n",
      " Regularized Logistic Regression GD (78500/99999): loss=-4817.148746750831\n",
      " Regularized Logistic Regression GD (78600/99999): loss=-4817.148752762758\n",
      " Regularized Logistic Regression GD (78700/99999): loss=-4817.1487586528865\n",
      " Regularized Logistic Regression GD (78800/99999): loss=-4817.148764423681\n",
      " Regularized Logistic Regression GD (78900/99999): loss=-4817.148770077573\n",
      " Regularized Logistic Regression GD (79000/99999): loss=-4817.14877561693\n",
      " Regularized Logistic Regression GD (79100/99999): loss=-4817.148781044068\n",
      " Regularized Logistic Regression GD (79200/99999): loss=-4817.148786361265\n",
      " Regularized Logistic Regression GD (79300/99999): loss=-4817.148791570755\n",
      " Regularized Logistic Regression GD (79400/99999): loss=-4817.14879667472\n",
      " Regularized Logistic Regression GD (79500/99999): loss=-4817.148801675302\n",
      " Regularized Logistic Regression GD (79600/99999): loss=-4817.148806574589\n",
      " Regularized Logistic Regression GD (79700/99999): loss=-4817.1488113746445\n",
      " Regularized Logistic Regression GD (79800/99999): loss=-4817.148816077477\n",
      " Regularized Logistic Regression GD (79900/99999): loss=-4817.148820685062\n",
      " Regularized Logistic Regression GD (80000/99999): loss=-4817.14882519932\n",
      " Regularized Logistic Regression GD (80100/99999): loss=-4817.148829622154\n",
      " Regularized Logistic Regression GD (80200/99999): loss=-4817.148833955412\n",
      " Regularized Logistic Regression GD (80300/99999): loss=-4817.148838200911\n",
      " Regularized Logistic Regression GD (80400/99999): loss=-4817.14884236043\n",
      " Regularized Logistic Regression GD (80500/99999): loss=-4817.148846435714\n",
      " Regularized Logistic Regression GD (80600/99999): loss=-4817.148850428469\n",
      " Regularized Logistic Regression GD (80700/99999): loss=-4817.148854340371\n",
      " Regularized Logistic Regression GD (80800/99999): loss=-4817.148858173054\n",
      " Regularized Logistic Regression GD (80900/99999): loss=-4817.148861928127\n",
      " Regularized Logistic Regression GD (81000/99999): loss=-4817.148865607162\n",
      " Regularized Logistic Regression GD (81100/99999): loss=-4817.148869211705\n",
      " Regularized Logistic Regression GD (81200/99999): loss=-4817.14887274326\n",
      " Regularized Logistic Regression GD (81300/99999): loss=-4817.148876203307\n",
      " Regularized Logistic Regression GD (81400/99999): loss=-4817.148879593297\n",
      " Regularized Logistic Regression GD (81500/99999): loss=-4817.148882914651\n",
      " Regularized Logistic Regression GD (81600/99999): loss=-4817.148886168759\n",
      " Regularized Logistic Regression GD (81700/99999): loss=-4817.148889356983\n",
      " Regularized Logistic Regression GD (81800/99999): loss=-4817.148892480659\n",
      " Regularized Logistic Regression GD (81900/99999): loss=-4817.148895541097\n",
      " Regularized Logistic Regression GD (82000/99999): loss=-4817.148898539575\n",
      " Regularized Logistic Regression GD (82100/99999): loss=-4817.148901477352\n",
      " Regularized Logistic Regression GD (82200/99999): loss=-4817.148904355658\n",
      " Regularized Logistic Regression GD (82300/99999): loss=-4817.148907175695\n",
      " Regularized Logistic Regression GD (82400/99999): loss=-4817.148909938645\n",
      " Regularized Logistic Regression GD (82500/99999): loss=-4817.148912645665\n",
      " Regularized Logistic Regression GD (82600/99999): loss=-4817.148915297894\n",
      " Regularized Logistic Regression GD (82700/99999): loss=-4817.148917896431\n",
      " Regularized Logistic Regression GD (82800/99999): loss=-4817.148920442373\n",
      " Regularized Logistic Regression GD (82900/99999): loss=-4817.1489229367835\n",
      " Regularized Logistic Regression GD (83000/99999): loss=-4817.148925380704\n",
      " Regularized Logistic Regression GD (83100/99999): loss=-4817.148927775159\n",
      " Regularized Logistic Regression GD (83200/99999): loss=-4817.148930121151\n",
      " Regularized Logistic Regression GD (83300/99999): loss=-4817.148932419662\n",
      " Regularized Logistic Regression GD (83400/99999): loss=-4817.1489346716535\n",
      " Regularized Logistic Regression GD (83500/99999): loss=-4817.14893687807\n",
      " Regularized Logistic Regression GD (83600/99999): loss=-4817.148939039835\n",
      " Regularized Logistic Regression GD (83700/99999): loss=-4817.148941157848\n",
      " Regularized Logistic Regression GD (83800/99999): loss=-4817.1489432329945\n",
      " Regularized Logistic Regression GD (83900/99999): loss=-4817.148945266153\n",
      " Regularized Logistic Regression GD (84000/99999): loss=-4817.148947258165\n",
      " Regularized Logistic Regression GD (84100/99999): loss=-4817.148949209864\n",
      " Regularized Logistic Regression GD (84200/99999): loss=-4817.1489511220725\n",
      " Regularized Logistic Regression GD (84300/99999): loss=-4817.148952995588\n",
      " Regularized Logistic Regression GD (84400/99999): loss=-4817.148954831192\n",
      " Regularized Logistic Regression GD (84500/99999): loss=-4817.148956629656\n",
      " Regularized Logistic Regression GD (84600/99999): loss=-4817.148958391727\n",
      " Regularized Logistic Regression GD (84700/99999): loss=-4817.148960118145\n",
      " Regularized Logistic Regression GD (84800/99999): loss=-4817.148961809636\n",
      " Regularized Logistic Regression GD (84900/99999): loss=-4817.148963466903\n",
      " Regularized Logistic Regression GD (85000/99999): loss=-4817.148965090639\n",
      " Regularized Logistic Regression GD (85100/99999): loss=-4817.1489666815205\n",
      " Regularized Logistic Regression GD (85200/99999): loss=-4817.148968240219\n",
      " Regularized Logistic Regression GD (85300/99999): loss=-4817.148969767387\n",
      " Regularized Logistic Regression GD (85400/99999): loss=-4817.148971263657\n",
      " Regularized Logistic Regression GD (85500/99999): loss=-4817.148972729656\n",
      " Regularized Logistic Regression GD (85600/99999): loss=-4817.148974166\n",
      " Regularized Logistic Regression GD (85700/99999): loss=-4817.148975573282\n",
      " Regularized Logistic Regression GD (85800/99999): loss=-4817.1489769521\n",
      " Regularized Logistic Regression GD (85900/99999): loss=-4817.148978303028\n",
      " Regularized Logistic Regression GD (86000/99999): loss=-4817.1489796266305\n",
      " Regularized Logistic Regression GD (86100/99999): loss=-4817.148980923455\n",
      " Regularized Logistic Regression GD (86200/99999): loss=-4817.148982194053\n",
      " Regularized Logistic Regression GD (86300/99999): loss=-4817.148983438946\n",
      " Regularized Logistic Regression GD (86400/99999): loss=-4817.14898465866\n",
      " Regularized Logistic Regression GD (86500/99999): loss=-4817.148985853705\n",
      " Regularized Logistic Regression GD (86600/99999): loss=-4817.1489870245805\n",
      " Regularized Logistic Regression GD (86700/99999): loss=-4817.14898817177\n",
      " Regularized Logistic Regression GD (86800/99999): loss=-4817.148989295762\n",
      " Regularized Logistic Regression GD (86900/99999): loss=-4817.148990397021\n",
      " Regularized Logistic Regression GD (87000/99999): loss=-4817.148991476007\n",
      " Regularized Logistic Regression GD (87100/99999): loss=-4817.148992533174\n",
      " Regularized Logistic Regression GD (87200/99999): loss=-4817.14899356896\n",
      " Regularized Logistic Regression GD (87300/99999): loss=-4817.148994583798\n",
      "Accuracy ratio = 0.654\n",
      "Test loss = -1090.215\n",
      "Train loss = -4817.149\n"
     ]
    }
   ],
   "source": [
    "lambda_ = 10000\n",
    "gamma = 1e-8\n",
    "w_rlrgd, loss_rlrgd = reg_logistic_regression(y_train, tx_train, lambda_, w_init, max_iter, gamma, pr=True, adapt_gamma = False)\n",
    "rlrgd_prediction = predict_labels(w_rlrgd, tx_test)\n",
    "acc_rlrgd = accuracy_ratio(rlrgd_prediction, y_test)\n",
    "print('Accuracy ratio = %.3f'%acc_rlrgd)\n",
    "print('Test loss = %.3f'%compute_loss_logistic(y_test, tx_test, w_rlrgd))\n",
    "print('Train loss = %.3f'%loss_rlrgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
