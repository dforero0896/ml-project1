{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T18:08:14.387486Z",
     "start_time": "2019-10-18T18:08:14.153089Z"
    }
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "from implementations import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T18:08:33.728339Z",
     "start_time": "2019-10-18T18:08:14.389345Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read train_y=LABELS, train_x=FEATURES and train_id=EVENT_IDS from dataset.\n",
    "subsamp = False\n",
    "y, x, id_ = load_csv_data('../data/train.csv', sub_sample=subsamp)\n",
    "y_out_test, x_out_test, id_out_test = load_csv_data('../data/test.csv', sub_sample=subsamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T18:28:52.668100Z",
     "start_time": "2019-10-18T18:28:51.985091Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((249975, 61), (25, 61))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean=True\n",
    "degree = 2\n",
    "feature_expansion = True\n",
    "x_train, y_train, x_test, y_test = split_data(x, y, ratio=0.9999, seed = 42)\n",
    "if clean:\n",
    "    # Clean data\n",
    "    y_train, x_train = clean_data(y_train, x_train)\n",
    "    y_test, x_test = clean_data(y_test, x_test)\n",
    "# Standardize data\n",
    "x_train_std = standardize_features(x_train)\n",
    "x_test_std = standardize_features(x_test)\n",
    "x_train = x_train_std[0]\n",
    "x_test = x_test_std[0]\n",
    "# Build data matrix\n",
    "tx_train = build_poly(x_train, degree)\n",
    "tx_test = build_poly(x_test, degree)\n",
    "tx_train.shape, tx_test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T18:29:24.387727Z",
     "start_time": "2019-10-18T18:29:03.744434Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD (0/399): loss=0.5\n",
      "GD (100/399): loss=0.33137314079292335\n",
      "GD (200/399): loss=0.32340400485531934\n",
      "GD (300/399): loss=0.31886610427104434\n",
      "Accuracy ratio = 0.720\n",
      "Test loss = 0.296\n",
      "Train loss = 0.318\n"
     ]
    }
   ],
   "source": [
    "w_init = np.array([0] * tx_train.shape[1])\n",
    "max_iter = 400\n",
    "gamma = 0.0009\n",
    "w_gd, loss_gd = least_squares_GD(y_train,\n",
    "                                 tx_train,\n",
    "                                 w_init,\n",
    "                                 max_iter,\n",
    "                                 gamma,\n",
    "                                 pr=True,\n",
    "                                 adapt_gamma=False,\n",
    "                                 kind='mse',\n",
    "                                accel=True)\n",
    "gd_prediction = predict_labels(w_gd, tx_test)\n",
    "acc_gd = accuracy_ratio(gd_prediction, y_test)\n",
    "print('Accuracy ratio = %.3f' % acc_gd)\n",
    "print('Test loss = %.3f' % compute_loss(y_test, tx_test, w_gd))\n",
    "print('Train loss = %.3f' % loss_gd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T12:37:03.486835Z",
     "start_time": "2019-10-17T12:37:03.480662Z"
    }
   },
   "source": [
    "np.savetxt('../data/w_gd_acc.dat', w_gd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T17:25:23.973086Z",
     "start_time": "2019-10-18T17:25:21.157754Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD (0/4999): loss=0.5\n",
      "SGD (100/4999): loss=0.5173437266806135\n",
      "SGD (200/4999): loss=0.33062343009174205\n",
      "SGD (300/4999): loss=0.3728576608433112\n",
      "SGD (400/4999): loss=0.24583096308033078\n",
      "SGD (500/4999): loss=0.43677362541311726\n",
      "SGD (600/4999): loss=0.26135118321843037\n",
      "SGD (700/4999): loss=0.1735789529757717\n",
      "SGD (800/4999): loss=0.21173991131173056\n",
      "SGD (900/4999): loss=0.26301353068633315\n",
      "SGD (1000/4999): loss=0.20051125485740304\n",
      "SGD (1100/4999): loss=0.6233369267651774\n",
      "SGD (1200/4999): loss=0.2667718787517875\n",
      "SGD (1300/4999): loss=0.17189326432032265\n",
      "SGD (1400/4999): loss=0.3340363100165612\n",
      "SGD (1500/4999): loss=0.8033371959348209\n",
      "SGD (1600/4999): loss=0.30138279695032477\n",
      "SGD (1700/4999): loss=0.20927274870467477\n",
      "SGD (1800/4999): loss=0.18368135633742547\n",
      "SGD (1900/4999): loss=0.35534302384458655\n",
      "SGD (2000/4999): loss=0.32953766300015486\n",
      "SGD (2100/4999): loss=0.8633827686694955\n",
      "SGD (2200/4999): loss=0.37720704000892796\n",
      "SGD (2300/4999): loss=0.17848917190100802\n",
      "SGD (2400/4999): loss=0.507070057642573\n",
      "SGD (2500/4999): loss=0.3922585946862073\n",
      "SGD (2600/4999): loss=0.19453161781845085\n",
      "SGD (2700/4999): loss=0.018337092547053326\n",
      "SGD (2800/4999): loss=0.4867868505119145\n",
      "SGD (2900/4999): loss=54.78431632365842\n",
      "SGD (3000/4999): loss=1.0464881756437974\n",
      "SGD (3100/4999): loss=0.19372460518557078\n",
      "SGD (3200/4999): loss=0.0037745551341805553\n",
      "SGD (3300/4999): loss=0.10886565507480285\n",
      "SGD (3400/4999): loss=0.11355635890517697\n",
      "SGD (3500/4999): loss=0.20134753867416919\n",
      "SGD (3600/4999): loss=0.0394786931390367\n",
      "SGD (3700/4999): loss=0.004142506445697764\n",
      "SGD (3800/4999): loss=0.5348764903340275\n",
      "SGD (3900/4999): loss=0.3480420712287957\n",
      "SGD (4000/4999): loss=0.08109354060752362\n",
      "SGD (4100/4999): loss=0.10575626259885203\n",
      "SGD (4200/4999): loss=0.45737259035693467\n",
      "SGD (4300/4999): loss=0.31331718925772667\n",
      "SGD (4400/4999): loss=2.2049821806808096\n",
      "SGD (4500/4999): loss=0.22222067622197417\n",
      "SGD (4600/4999): loss=0.7422657718033923\n",
      "SGD (4700/4999): loss=0.039479124132989256\n",
      "SGD (4800/4999): loss=1.8306220630666552\n",
      "SGD (4900/4999): loss=0.07579897067202268\n",
      "Accuracy ratio = 0.72\n",
      "Test loss = 3.63e-01\n",
      "Train loss = 3.67e-01\n"
     ]
    }
   ],
   "source": [
    "w_init = np.array([0] * tx_train.shape[1])\n",
    "max_iter = 5000\n",
    "gamma = 1e-4\n",
    "batch_size = 1\n",
    "\n",
    "w_sgd, loss_sgd = least_squares_SGD(y_train,\n",
    "                                    tx_train,\n",
    "                                    w_init,\n",
    "                                    batch_size,\n",
    "                                    max_iter,\n",
    "                                    gamma,\n",
    "                                    pr=True,\n",
    "                                    adapt_gamma=False,\n",
    "                                    choose_best=True)\n",
    "sgd_prediction = predict_labels(w_sgd, tx_test)\n",
    "acc_sgd = accuracy_ratio(sgd_prediction, y_test)\n",
    "print('Accuracy ratio = %.2f' % acc_sgd)\n",
    "print('Test loss = %.2e' % compute_loss(y_test, tx_test, w_sgd))\n",
    "print('Train loss = %.2e' % loss_sgd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T18:29:58.126363Z",
     "start_time": "2019-10-18T18:29:58.071738Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy ratio = 0.48\n",
      "Train loss = 0.32\n",
      "Test loss = 5.97e+02\n"
     ]
    }
   ],
   "source": [
    "w_lsq, loss_lsq = least_squares(y_train, tx_train)\n",
    "lsq_prediction = predict_labels(w_lsq, tx_test)\n",
    "acc_lsq = accuracy_ratio(lsq_prediction, y_test)\n",
    "print('Accuracy ratio = %.2f' % acc_lsq)\n",
    "print('Train loss = %.2f' % loss_lsq)\n",
    "print('Test loss = %.2e' % compute_loss(y_test, tx_test, w_lsq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T12:45:30.271461Z",
     "start_time": "2019-10-17T12:45:30.267909Z"
    }
   },
   "source": [
    "np.savetxt('../data/w_lsq.dat', w_lsq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T18:31:55.175315Z",
     "start_time": "2019-10-18T18:31:55.126195Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy ratio = 0.680\n",
      "Test loss = 0.317\n",
      "Train loss = 0.319\n"
     ]
    }
   ],
   "source": [
    "lambda_ = 3.3e-2\n",
    "w_rr, loss_rr = ridge_regression(y_train, tx_train, lambda_)\n",
    "rr_prediction = predict_labels(w_rr, tx_test)\n",
    "acc_rr = accuracy_ratio(rr_prediction, y_test)\n",
    "print('Accuracy ratio = %.3f'%acc_rr)\n",
    "print('Test loss = %.3f'%compute_loss(y_test, tx_test, w_rr))\n",
    "print('Train loss = %.3f'%loss_rr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T15:18:22.322364Z",
     "start_time": "2019-10-18T15:18:22.309384Z"
    }
   },
   "source": [
    "np.savetxt('../data/w_rr.dat', w_rr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T18:30:07.069000Z",
     "start_time": "2019-10-18T18:30:07.055291Z"
    }
   },
   "outputs": [],
   "source": [
    "def ridge_regression_demo(x, y, degree, ratio, seed):\n",
    "    \"\"\"ridge regression demo.\"\"\"\n",
    "    # define parameter\n",
    "    lambdas = np.logspace(-5, 3, 50)\n",
    "    # split the data, and return train and test data\n",
    "    x_train, y_train, x_test, y_test = split_data(x, y, ratio, seed)\n",
    "    #Clean\n",
    "    if clean:\n",
    "        y_train, x_train = clean_data(y_train, x_train)\n",
    "        y_test, x_test = clean_data(y_test, x_test)\n",
    "    # form train and test data with offset column\n",
    "    x_train_std = standardize_features(x_train)[0]\n",
    "    x_test_std = standardize_features(x_test)[0]\n",
    "    tx_train=build_poly(x_train_std, degree)\n",
    "    tx_test=build_poly(x_test_std, degree)\n",
    "    rmse_tr = []\n",
    "    rmse_te = []\n",
    "    accuracies = []\n",
    "    for ind, lambda_ in enumerate(lambdas):\n",
    "        # calcualte weight through least square.\n",
    "        w_train, loss_train = ridge_regression(y_train, tx_train, lambda_)\n",
    "        rmse_tr.append(np.sqrt(2*loss_train))\n",
    "        rmse_te.append(np.sqrt(2*compute_loss(y_test, tx_test, w_train, kind = 'mse')))\n",
    "        accuracies.append(accuracy_ratio(predict_labels(w_train, tx_test), y_test))\n",
    "        print(\"proportion={p}, degree={d}, lambda={l:.3e}, Training RMSE={tr:.3f}, Testing RMSE={te:.3f}, Accuracy={ac:.3f}\".format(\n",
    "               p=ratio, d=degree, l=lambda_, tr=rmse_tr[ind], te=rmse_te[ind], ac=accuracies[ind]))\n",
    "        \n",
    "    # Plot the obtained results\n",
    "    plot_train_test(rmse_tr, rmse_te, lambdas, degree)\n",
    "    plt.figure()\n",
    "    plt.semilogx(lambdas,accuracies, marker='o')\n",
    "def plot_train_test(train_errors, test_errors, lambdas, degree):\n",
    "    \"\"\"\n",
    "    train_errors, test_errors and lambas should be list (of the same size) the respective train error and test error for a given lambda,\n",
    "    * lambda[0] = 1\n",
    "    * train_errors[0] = RMSE of a ridge regression on the train set\n",
    "    * test_errors[0] = RMSE of the parameter found by ridge regression applied on the test set\n",
    "    \n",
    "    degree is just used for the title of the plot.\n",
    "    \"\"\"\n",
    "    plt.semilogx(lambdas, train_errors, color='b', marker='*', label=\"Train error\")\n",
    "    plt.semilogx(lambdas, test_errors, color='r', marker='*', label=\"Test error\")\n",
    "    plt.xlabel(\"lambda\")\n",
    "    plt.ylabel(\"RMSE\")\n",
    "    plt.title(\"Ridge regression for polynomial degree \" + str(degree))\n",
    "    leg = plt.legend(loc=1, shadow=True)\n",
    "    leg.draw_frame(False)\n",
    "    plt.savefig(\"../results/ridge_regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T18:30:11.749743Z",
     "start_time": "2019-10-18T18:30:08.141908Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proportion=0.9, degree=2, lambda=1.000e-05, Training RMSE=0.794, Testing RMSE=0.794, Accuracy=0.774\n",
      "proportion=0.9, degree=2, lambda=1.456e-05, Training RMSE=0.794, Testing RMSE=0.794, Accuracy=0.774\n",
      "proportion=0.9, degree=2, lambda=2.121e-05, Training RMSE=0.794, Testing RMSE=0.794, Accuracy=0.774\n",
      "proportion=0.9, degree=2, lambda=3.089e-05, Training RMSE=0.794, Testing RMSE=0.794, Accuracy=0.774\n",
      "proportion=0.9, degree=2, lambda=4.498e-05, Training RMSE=0.794, Testing RMSE=0.794, Accuracy=0.774\n",
      "proportion=0.9, degree=2, lambda=6.551e-05, Training RMSE=0.794, Testing RMSE=0.794, Accuracy=0.774\n",
      "proportion=0.9, degree=2, lambda=9.541e-05, Training RMSE=0.794, Testing RMSE=0.794, Accuracy=0.774\n",
      "proportion=0.9, degree=2, lambda=1.389e-04, Training RMSE=0.794, Testing RMSE=0.794, Accuracy=0.773\n",
      "proportion=0.9, degree=2, lambda=2.024e-04, Training RMSE=0.794, Testing RMSE=0.794, Accuracy=0.774\n",
      "proportion=0.9, degree=2, lambda=2.947e-04, Training RMSE=0.794, Testing RMSE=0.794, Accuracy=0.773\n",
      "proportion=0.9, degree=2, lambda=4.292e-04, Training RMSE=0.794, Testing RMSE=0.794, Accuracy=0.773\n",
      "proportion=0.9, degree=2, lambda=6.251e-04, Training RMSE=0.794, Testing RMSE=0.794, Accuracy=0.773\n",
      "proportion=0.9, degree=2, lambda=9.103e-04, Training RMSE=0.795, Testing RMSE=0.794, Accuracy=0.773\n",
      "proportion=0.9, degree=2, lambda=1.326e-03, Training RMSE=0.795, Testing RMSE=0.794, Accuracy=0.773\n",
      "proportion=0.9, degree=2, lambda=1.931e-03, Training RMSE=0.795, Testing RMSE=0.794, Accuracy=0.773\n",
      "proportion=0.9, degree=2, lambda=2.812e-03, Training RMSE=0.795, Testing RMSE=0.794, Accuracy=0.773\n",
      "proportion=0.9, degree=2, lambda=4.095e-03, Training RMSE=0.795, Testing RMSE=0.794, Accuracy=0.773\n",
      "proportion=0.9, degree=2, lambda=5.964e-03, Training RMSE=0.795, Testing RMSE=0.794, Accuracy=0.773\n",
      "proportion=0.9, degree=2, lambda=8.685e-03, Training RMSE=0.795, Testing RMSE=0.795, Accuracy=0.772\n",
      "proportion=0.9, degree=2, lambda=1.265e-02, Training RMSE=0.796, Testing RMSE=0.795, Accuracy=0.771\n",
      "proportion=0.9, degree=2, lambda=1.842e-02, Training RMSE=0.797, Testing RMSE=0.796, Accuracy=0.771\n",
      "proportion=0.9, degree=2, lambda=2.683e-02, Training RMSE=0.798, Testing RMSE=0.797, Accuracy=0.770\n",
      "proportion=0.9, degree=2, lambda=3.907e-02, Training RMSE=0.799, Testing RMSE=0.799, Accuracy=0.769\n",
      "proportion=0.9, degree=2, lambda=5.690e-02, Training RMSE=0.801, Testing RMSE=0.800, Accuracy=0.767\n",
      "proportion=0.9, degree=2, lambda=8.286e-02, Training RMSE=0.803, Testing RMSE=0.803, Accuracy=0.766\n",
      "proportion=0.9, degree=2, lambda=1.207e-01, Training RMSE=0.805, Testing RMSE=0.805, Accuracy=0.765\n",
      "proportion=0.9, degree=2, lambda=1.758e-01, Training RMSE=0.809, Testing RMSE=0.809, Accuracy=0.762\n",
      "proportion=0.9, degree=2, lambda=2.560e-01, Training RMSE=0.813, Testing RMSE=0.813, Accuracy=0.758\n",
      "proportion=0.9, degree=2, lambda=3.728e-01, Training RMSE=0.819, Testing RMSE=0.819, Accuracy=0.753\n",
      "proportion=0.9, degree=2, lambda=5.429e-01, Training RMSE=0.827, Testing RMSE=0.827, Accuracy=0.749\n",
      "proportion=0.9, degree=2, lambda=7.906e-01, Training RMSE=0.837, Testing RMSE=0.837, Accuracy=0.745\n",
      "proportion=0.9, degree=2, lambda=1.151e+00, Training RMSE=0.849, Testing RMSE=0.849, Accuracy=0.740\n",
      "proportion=0.9, degree=2, lambda=1.677e+00, Training RMSE=0.862, Testing RMSE=0.862, Accuracy=0.731\n",
      "proportion=0.9, degree=2, lambda=2.442e+00, Training RMSE=0.877, Testing RMSE=0.877, Accuracy=0.724\n",
      "proportion=0.9, degree=2, lambda=3.556e+00, Training RMSE=0.892, Testing RMSE=0.892, Accuracy=0.717\n",
      "proportion=0.9, degree=2, lambda=5.179e+00, Training RMSE=0.907, Testing RMSE=0.906, Accuracy=0.711\n",
      "proportion=0.9, degree=2, lambda=7.543e+00, Training RMSE=0.921, Testing RMSE=0.921, Accuracy=0.706\n",
      "proportion=0.9, degree=2, lambda=1.099e+01, Training RMSE=0.934, Testing RMSE=0.934, Accuracy=0.702\n",
      "proportion=0.9, degree=2, lambda=1.600e+01, Training RMSE=0.947, Testing RMSE=0.946, Accuracy=0.699\n",
      "proportion=0.9, degree=2, lambda=2.330e+01, Training RMSE=0.957, Testing RMSE=0.957, Accuracy=0.696\n",
      "proportion=0.9, degree=2, lambda=3.393e+01, Training RMSE=0.966, Testing RMSE=0.966, Accuracy=0.693\n",
      "proportion=0.9, degree=2, lambda=4.942e+01, Training RMSE=0.974, Testing RMSE=0.974, Accuracy=0.692\n",
      "proportion=0.9, degree=2, lambda=7.197e+01, Training RMSE=0.980, Testing RMSE=0.980, Accuracy=0.690\n",
      "proportion=0.9, degree=2, lambda=1.048e+02, Training RMSE=0.985, Testing RMSE=0.985, Accuracy=0.689\n",
      "proportion=0.9, degree=2, lambda=1.526e+02, Training RMSE=0.989, Testing RMSE=0.989, Accuracy=0.688\n",
      "proportion=0.9, degree=2, lambda=2.223e+02, Training RMSE=0.992, Testing RMSE=0.992, Accuracy=0.687\n",
      "proportion=0.9, degree=2, lambda=3.237e+02, Training RMSE=0.994, Testing RMSE=0.994, Accuracy=0.686\n",
      "proportion=0.9, degree=2, lambda=4.715e+02, Training RMSE=0.996, Testing RMSE=0.996, Accuracy=0.686\n",
      "proportion=0.9, degree=2, lambda=6.866e+02, Training RMSE=0.997, Testing RMSE=0.997, Accuracy=0.686\n",
      "proportion=0.9, degree=2, lambda=1.000e+03, Training RMSE=0.998, Testing RMSE=0.998, Accuracy=0.685\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEaCAYAAAA7YdFPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xl8FdX9//HXOxsJm+z7poAoCCJEXCGuiNSKS1UQ3KpFsFrt4letWhVqW9tal4papGptreBPUWmr1daKLIoQIewgi2wSkEX2sIR8fn/MBC+XrCQ3N8vn+XjcR2bOzJz5zM3N/WTOzJwjM8M555w7WgnxDsA551zV5onEOedcmXgicc45VyaeSJxzzpWJJxLnnHNl4onEOedcmXgiqWIkPS/pwSKWm6ROFRlTZVXce1WGeiXpJUnfSJpZ3vWXMpZzJK2LZwzRJLWTtEtSYgnWLVX8kiZLuqVsEbrylhTvANzhJK0CmgMHgV3Av4HbzWwXgJmNiF90VUsM36uzgQuBNma2O0b7qLLMbA1QN95xxJOk3wODgBbAV8CvzOyV+EYVO35GUjl918zqAj2BU4D74hzPYcL/yMvts1Pe9VWA9sCqo0kikvyft0ooBr+X3cB3gWOAG4CnJJ1ZzvuoNKrSH2+NY2YbgPcJEgoAkl6W9MuI+bslZUtaL+n7kdtLaizpH5J2SJol6ZeSpkUsP0HSfyRtlbRU0tWFxRI2KTwqaTqwBzhO0jGS/hzu/6uw/sRw/URJj0vaLOlLSbeHzW5JR1lfJ0kfS9oe1jkhLJekJyR9HS6bJ+mkQt6rH0haHh7vJEmtIpaZpBGSloVNVmMkqYD34WZgHHBG2HzzSAnr/qGkZcCyAursEK4zPPw9Zkv6acTyWpKeDJetD6drFVDP3ZLejCr7o6QnI97z0ZKmS9op6QNJTSLWvVTSQknbwnVPjFi2Kqx/nqTd4e+puaT3wrr+K6lh1PHk/65vkrQ4XG+lpFuP+IAVQtKFkpaEv9tnAEUt/35Y9zeS3pfUPmJZ//BzvV3Ss+Hn55Zw2Y3h+/CEpK3AwyWor8R/L2b2kJktMbM8M/sMmAqcUdLjrnLMzF+V6AWsAi4Ip9sA84GnIpa/DPwynB4AbAROAuoAfwcM6BQuHx++agNdgbXAtHBZnXD+JoImzl7AZqBbIXFNBtYA3cL1k4G3gT+FdTUDZgK3huuPABaFx9AQ+G8YW9JR1vcacD/BPz+pwNlh+UXA50ADgi+ZE4GWBbxX54XH1wuoBfwRmBJxfAb8M6ynHbAJGFDIe3Fj/vtYirr/AzQC0gqor0O4zmvhsXcP95//ORgFzAjfk6bAJ8DocNk5wLpwuiXBf8INwvkk4Gugd8R7vgI4HkgL538TLjs+3PbC8Hfxf8ByICXiczmDoNm1dVjvbIIz5lrA/4CHoo4n/3f9HaBj+PvJIPjHoVd0/AW8L02AHcD3wph+DOQCt4TLLwtjPDE81geAT6K2vSJcdidwIGLbG8O67giXpxVTX6n+XqKOIw3IppDPU3V4xT0Af0X9QoI/2F3AzvCP8cP8L4Zw+ct8++X4Yv4XQTh/fLhNJyAx/MPpErH8l3ybSK4Bpkbt+0/5XwYFxDUZGBUx3xzYR8QXIzAE+Cic/h9hEgjnL+DIRFKa+l4BxhJcl4iM6zzgC+B0ICFqWeR79WfgtxHL6obvT4dw3giTUzj/OnBvIe/FjRyeSEpS93lF/M47hOucEFH2W+DP4fQKYGDEsosImtYg6osYeA/4QTh9CbAo6nf4QMT8bcC/w+kHgdcjliUQtO2fE/G5HBqx/E3guYj5O4C3o44nqZDjfRu4s6D4o9a7HpgRMS9gHd8mg/eAm6Ni3kPQ9Hg98GnUtms5PJGsidpfUfWV6u8lar2/EFzrVEm/B6ray5u2KqfLzKwewR/ZCQT/XRWkFcEfR77VEdNNCf5zilweOd0eOC1sxtgmaRswlODiYGGit08GsiO2/xPBf80FxRY5fTT1/R/Bl8HMsPnl+wBm9j/gGWAMsFHSWEn1C9hXKyLeHwtuXthC8N91vg0R03so+QXjktRd0PFHi/5d5jePHVZ/1LJofwGGhdPDgL9GLS/sGKOPIS+MJ/IYNkZM5xQwX+D7JeliSTPCJqFtwEAK/0xHOuwzZMG3cvRn5qmIz8tWgs9I60K2jb47LPp3UlR9R/P3gqTfEbQYXB3GUC15IqnEzOxjgv+qf1/IKtlA24j5dhHTmwhO3dtElEWuuxb42MwaRLzqmtnIokKK2n4f0CRi+/pm1i0itsL2Xer6zGyDmf3AzFoBtwLPKrzN2cyeNrPeBM1kxwN3F7Cv9QRfBgBIqgM0Jvivu6xKUndJvkSif5frC6o/alm0t4Ee4XWiS4BXS7DfI/YRXh9qSxnfn/BazpsEn+HmZtYAeJeoax2FOOzzHRFTvrUEZ72Rn+E0M/uEqM9fuG3k5xGO/J0UVV+p/17C62cXA/3NbEcJjrfK8kRS+T0JXCipZwHLXgdulNRVUm3gofwFZnYQmAg8LKm2pBMITvfz/RM4XtJ1kpLD16mRF1iLYmbZwAfA45LqS0qQ1FFSRkRsd0pqLakBcE9Z6pN0laT8L4JvCL4EDoYxnyYpmaCNfy/BrdPR/g7cJKln+OX2K+AzM1tVkuMtRnnV/WD4u+pG0BY/ISx/DXhAUtPw4vgvgL8VVIGZ7QXeCGOaacGtuCXxOvAdSeeH7+VPCRL7J6U8hmgpBNdQNgG5ki4G+pdw238B3SRdEV64/xGHnwE8D9wXvl8ouFnjqohtu0u6LNz2hxRz9lBMfaX6e5F0H3AtcKGZbSnh8VZZnkgqOTPbRHB94IgH68zsPYJE8z+Ci4T/i1rldoLbDzcQNHG8RvDlgJntJPiDHkzw3+gG4DGCP/qSup7gi2IRwZf7GwQXfAFeIEgM84A5BP+F5lLwl3xJ6jsV+EzSLmASQRv7l0D9cF/fEDTNbKGAMzgz+5DgPXyT4L/VjuGxl1k51v0xwe/xQ+D3ZvZBWP5LIJPgvZxPcJH7lwXWEPgLwQX76GatQpnZUoKmsD8SXET+LsFt6PtLeQzR9e4kSACvE/yOriX4/ZVk283AVcBvCH6vnYHpEcvfIvjMjpe0A1hAcAYQue1vw227EryH+4rYX1H1lfbv5VcEZ47LFNzdt0vSz0ty3FWRqnGznYsi6TGghZndEId9Xww8b2bti125hpHUAfgSSDaz3HKorx2whOB3Xa2bVEpKwXNK6whuGPgo3vFUN35GUo2F9733UKAPcDPwVgXtO03SQElJkloTNLtVyL5rsvAL8yfA+JqeRCRdJKlB2Nz4c4LrMjPiHFa15E/ZVm/1CJqzWhHc9/848E4F7VvAIwTt/DkEbda/qKB910jhRf6NBE18A+IcTmVwBsG1ovzm0svMLCe+IVVP3rTlnHOuTLxpyznnXJl4InHOOVcmNeIaSZMmTaxDhw7xDsM556qUzz//fLOZNS1uvRqRSDp06EBmZma8w3DOuSpF0uri1/KmLeecc2XkicQ551yZxDSRSHpRwYBDCwpZLklPKxgQaJ6kXhHLblAwyNAySTdElPeWND/c5umwMzbnnHNxEuszkpcp+sGoiwn6z+kMDAeeA5DUiOBJ6NOAPsBDCkdfC9cZHrGdP3jlnHNxFNNEYmZTCPr0L8wg4BULzAAaSGpJMHDPf8xsq5l9QzC63IBwWX0z+zTs2/8VglHNnHPORdiYlU1Wgwy+nreh+JXLKN7XSFpz+OAy68KyosrXFVDunHM1UmEJY8mw0XTfPo3F146KeQzxTiQFXd+woyg/smJpuKRMSZmbNm0qQ4jOORd/JU0YOUoDiYyFz5FIHhkLnwMpKI+ReCeSdRw+4lkbgr7+iypvU0D5EcxsrJmlm1l606bFPk/jnHMF2rJlCz179qRnz560aNGC1q1bH5rfv79kw7XcdNNNLF26tExxlDRhpLL3sO1ySGV6h6HsnPtlmfZflHgnkknA9eHdW6cD28OR8t4H+ktqGF5k7w+8Hy7bKen08G6t66m43mydc1VIdjZkZMCGMl4iaNy4MVlZWWRlZTFixAh+/OMfH5pPSUkBwMzIy8srtI6XXnqJLl26lGh/0WcehSWMpKiEsR/YqBZMP+465tc9nTzEPlJIYT+5derTrEeLEsV6NGJ9++9rwKdAF0nrJN0saYSkEeEq7wIrCUaFewG4DcDMtgKjgVnha1RYBjASGBduswJ4L5bH4JyrmkaPhmnTYFSMLhEsX76ck046iREjRtCrVy+ys7MZPnw46enpdOvWjVEROz777LPJysoiNzeXBg0acO+993LyySdzxhln8PXXXx9W75Jhozl2+1RuOPMMTmzcgZ6JyUwkEQi++AYDF5LCGbVb81ydk7kAuIoEegNLul7OJ7eexKC9C+lYqyGjb/wx07qN4Ouvj4y1XJlZtX/17t3bnHPVw513mmVkFP5KSDCDI18JCYVvc+edJdv3Qw89ZL/73e/MzGzZsmUmyWbOnHlo+ZYtW8zM7MCBA3b22WfbwoULzczsrLPOsjlz5tiBAwcMsHfffdc2zFlvQ1Na2/0/+rmZme0h9VCwd4O9Fk5vButAsu0Ge5Ykawf2jxO+b2ZmTzU6y1ITkux/Y96zyd1us3GNM6xHjx62e/du27Fjh51wwgk2d+7cAmMtCSDTSvAdG++mLeecK1d9+kCzZpAQfrslJATzp51W/vvq2LEjp5566qH51157jV69etGrVy8WL17MokWLjtgmLS2Niy++mCXDRnPR/vUsGDeeyb1/yk7VB4K7hz4Afgkcn1SfjOO7kpOQxMRO17Jl+M/p3uBEGm//BoCuEx7m7PPO4dzbBpCxYAzb7vsuV155JbVr16ZevXpcdtllTJs2rcBYy1ON6LTROVd9PPlk8euMHAljx0JqKuzfD1deCc8+W/6x1KlT59D0smXLeOqpp5g5cyYNGjRg2LBh7N2794htUnJygmsewHigyZ6VnDP7DxwkgcxGF3LK1g/Jw5gIZHcZSsaCbwMfN24cndL6c0bEmxAZgxUxUGHkeuXNz0icc9XOxo0wYgTMmBH8LOsF95LYsWMH9erVo379+mRnZ/P++++zffU2shpkcGB3cHfX+llfkZuQwoHwmgfAQcQnrb/HlrlfcaBWXaZ1G0Gf717PzxudRMrWIPA5c+aUKIZ+/frx1ltvkZOTw65du3jnnXfo27dv+R9sFD8jcc5VOxMnfjs9ZkzF7LNXr1507dqVk046ieOOO46zzjqL9c+9Tfft08hZ3JjMy0Zz7epJJJHLuuTOtD+wnP0kkcABDjRoSrMeLWi2Pgg8ffdu7rrrLobPmEFet2506tSJd94p/gbVPn36MGTIkENNWCNHjqR79+4sX748psdeI8ZsT09PNx+PxDlXUXKURhpHNmvlksiGKctYe81P2d+oJS1+MZwNo8aSsjWbM9ZPLKCm+JL0uZmlF7een5E451w52/TxYnL6D+L4ffMQcIAkMltdSsf3xtCmRwvaRCSNLldX0ClTDPk1EuecK6PIhwgzH/+YAxdcTJd98wDYSy0SyGN/w+aHHgqsbvyMxDnnymjJsNGcvX0aq9PPIf3AUtYmdWBe3TP4pv0phzVfVVd+jcQ5545SYddC9pJKquXEIaLyVdJrJN605ZxzR2nztKWsSP62D638DhJ3xLCDxMrIE4lzzh2F9XM2suni6+l4YClGkESiO0isKTyROOdcESK7kW/WpBnNEmrRuflxXNirLcft/IxFtdOZ0u021kyYwbRuIw49RBjtxRdfZENFPBkZB36x3TlXPWVnw+DBMGECtDj6M4T8buQBbmx6Kt0skx9/vZrs5PZ8M2EiXS/veWjdom7lffHFF+nVqxctjjKW3NxckpKSCp0v6Xax4InEOVc9RfYjX8aOtvIvqncgGKY1iTzaHviSF67owwunBgNcnXnmmTzzzDPk5eVx0003kZWVhZkxfPhwmjdvTlZWFtdccw1paWnMnDnz0FgmEPTTdfvtt7N582bq1KnDuHHjOP744xk2bBjNmzdn9uzZnHrqqaSkpLBp0yZWrlxJixYtGDt2LCNGjGD27NkkJyfz5JNP0q9fP8aNG8d///tfdu3axb59+/jPf/5TpuMvjicS51zVctddEJ4hFGjqVIgcuOm554JXQgIU1u9Uz55F9ga5+ZNlWN++cHAVAHtI481W5zGxSy6ffPBPkpKSGD58OOPHj6djx45s3ryZ+fPnA7Bt2zYaNGjAH//4R5555hl69ux5RP3Dhw9n3LhxdOzYkenTp3P77bfzwQcfALBixQo+/PBDEhISeOCBB5gzZw5TpkwhNTWVxx57jJSUFObPn8/ChQsZOHAgy5YtA+DTTz8lKyuLhg0bFvFmlo+YJhJJA4CngERgnJn9Jmp5e+BFoCmwFRhmZusknQs8EbHqCcBgM3tb0stABrA9XHajmRXxqXLO1Sh9+sDKlbB5c5BQEhKgSRPo2PGoqsvZmcvyK/6Pcw+uwgieUq/FPj7L+4Z5S1eRnh7cHZuTk0Pbtm256KKLWLp0KXfeeScDBw6kf//+Rda/bds2ZsyYwZVXXnmoLDc399D0VVddRULCt5ezBw0aRGpqKgDTpk3j7rvvBqBbt260atXqUL9a/fv3r5AkAjFMJJISgTHAhQRjrc+SNMnMIjvo/z3wipn9RdJ5wK+B68zsI6BnWE8jgtEQP4jY7m4zeyNWsTvnKrEK7Ed+97YDZB5/LedueoPlqd1YXS+NZhf1ZdqcfSSumcr37/w+o0ePPmK7efPm8d577/H000/z5ptvMnbs2EL3YWY0adLk0HWYaNHdv1eGbuOjxfKurT7AcjNbaWb7CbreHxS1Tlfgw3D6owKWA3wPeM/M9sQsUudc9VLGfuQ3ZmWTVb8vczsMImPTG2Re+wc65Sygw23fodnJrchYMIZbpr/K66+/zubNm4Hg7q41a9awadMmzIyrrrqKRx55hNmzZwNQr149du7cecS+GjZsSMuWLXnrrbcAyMvLY+7cuSWKs1+/frz66qsALF68mOzsbDp16lSqYy0PsWzaag2sjZhfB0SPUTYXuJKg+etyoJ6kxma2JWKdwcAforZ7VNIvCJLQvWa2r1wjd85VbWXsR37x4IfJ2DkNAbNvfJr0l+44Yp3u3bvz0EMPccEFF5CXl0dycjLPP/88iYmJ3HzzzZgZknjssccAuOmmm7jlllsKvNg+fvx4Ro4cycMPP8z+/fsZNmwYJ598crFx3nHHHdx66610796d5ORkXnnllcPqrSgx6yJF0lXARWZ2Szh/HdDHzO6IWKcV8AxwLDCFIKl0M7Pt4fKWwDyglZkdiCjbAKQAY4EVZjaqgP0PB4YDtGvXrvfq1atjcpzOueqjsC5PckglrRp0eVJalaGLlHVA24j5NsD6yBXMbL2ZXWFmpwD3h2XbI1a5GngrP4mEy7PDcen3AS8RNKEdwczGmlm6maU3bdq0fI7IOVetfTNrBV8mfntRfg+1md5hKDtrWJcnpRXLRDIL6CzpWEkpBE1UkyJXkNREUn4M9xHcwRVpCPBa1DYtw58CLgMWxCB251wNNPMn4zn24AryEDmkUou9NbLLk9KKWSIxs1zgduB9YDHwupktlDRK0qXhaucASyV9ATQHHs3fXlIHgjOaj6OqflXSfGA+0AT4ZayOwTlXc/xr5D+4dOrP2JDYimndRhTb5Yn7lncj75yr8aY/N48et53FhmO6cNyaj0msX3G3zlZmPtSuc84VY2NWNhvPvoL2u9eSk1Sflp+940nkKHgicc7VWIuufohzds8gl0S2vDOTul1axzukKskTiXOuxsm/zffccD6Zg7T4Tu8ae5tvWfl4JM65GmfHnJXMr9X70Lzf5ls2nkicczXO0k+3cuK+OYdGNvTbfMvGE4lzrkbZuGYfjX90LQdJ4pPjb/TbfMuBXyNxztUYZjD9nPu5Inceq8b8k7Nu+w5Q9MiGrnh+RuKcqzHe+dGHXPHl48w7+zY6hEnElZ0nEudcjbB4+lbSn7mBtXVOoPu/fxfvcKoVb9pyzlVrG7Oyyc4YzNaD9enI1+x8+x+oTu14h1WteCJxzlVrS4aNpu+OqSRgLLnpMU644JR4h1TteF9bzrlqyccWKbvKMB6Jc87FzY45K5nadggHw6+5HFL9ocMY8UTinKuWmvdsSdK2zSSSxwGSSGG/P3QYI55InHPV0vI5Ozl551S2JDRm5Wuf+UOHMeQX251z1Y4ZZF0xik7sZc87H9Hlkl50Gdwr3mFVWzE9I5E0QNJSScsl3VvA8vaSPpQ0T9JkSW0ilh2UlBW+JkWUHyvpM0nLJE0Ih/F1zrlD/vPkQgatepKFZ9xCk0tOj3c41V7MEomkRGAMcDHQFRgiqWvUar8HXjGzHsAo4NcRy3LMrGf4ujSi/DHgCTPrDHwD3ByrY3DOVT07dxj1772N3Yn16TLx18Vv4MoslmckfYDlZrbSzPYD44FBUet0BT4Mpz8qYPlhJAk4D3gjLPoLcFm5Reycq/L+MfhVTt8/ha13/4akFk3iHU6NEMtE0hpYGzG/LiyLNBe4Mpy+HKgnqXE4nyopU9IMSfnJojGwzcxyi6gTAEnDw+0zN23aVNZjcc5VctnZcE7PbZz/3k9Z2ew0jnvUGysqSiwTiQooi3768WdAhqQ5QAbwFZCfJNqFD8JcCzwpqWMJ6wwKzcaaWbqZpTdt2vSoDsA5V3U8eU82r87tRhM20ei1ZyHBb0qtKLG8a2sd0DZivg2wPnIFM1sPXAEgqS5wpZltj1iGma2UNBk4BXgTaCApKTwrOaJO51zNkpYGe/fCBH5EK9azgJPocX4vUlMhxx9grxCxTNmzgM7hXVYpwGBgUuQKkppIyo/hPuDFsLyhpFr56wBnAYss6M/lI+B74TY3AO/E8Bicc5XcHkvDEFfzBgK6swBD7LG0eIdWY8QskYRnDLcD7wOLgdfNbKGkUZLy78I6B1gq6QugOfBoWH4ikClpLkHi+I2ZLQqX3QP8RNJygmsmf47VMTjnKj99uZLMehmH5ndTm5mdh6JV3hVKRYnpA4lm9i7wblTZLyKm3+DbO7Ai1/kE6F5InSsJ7ghzzjm+SWlOh53zMcBSUknbv5ctufWhhXeFUlH8apRzrkr7903jacJWdp1+AQkzZ5Bw2wgu7uldoVQk70beOVdlffXlfvZ1PJHEBvVov3m236lVzrwbeedctTfl+nEcZyup9fivPYnEkb/zzrkqaXnWLs6dNorlrfrR4sYB8Q6nRvNE4pyrkjKve4oWbKTR2N+ACnpW2VUUTyTOuSon68MtXLzgtyw+fhCNvnNGvMOp8TyROOeqlI1Z2TS9sCf12EGbvzxa/AYu5jyROOeqlAWX3EMrW8fa1C7UO71bvMNx+AiJzrkqIkdppLGX88P59nuXgkQOqaSZd6oVT35G4pyrEnbMWcm0xpce6u57D7WZ3mEoO+d6Vyjx5mckzrkqoXnPlnyzbTkAe6lFLfaSW6c+zXp4Vyjx5mckzrkqYdakbDofXMKqtBNZPeEzpnUbQcpW7wqlMvAzEudclbD2rsfpBTT/7B/U7t6RLlePiXdILuRnJM65Sm/+5C30//J5FvUYQu3uHeMdjoviicQ5V+ktvf1p6rKbds/dF+9QXAFimkgkDZC0VNJySfcWsLy9pA8lzZM0WVKbsLynpE8lLQyXXROxzcuSvpSUFb56xvIYnHPxtXz2Ds5f+DQLO1/GMWf6cyOVUcwSiaREYAxwMdAVGCKpa9RqvwdeMbMewCjg12H5HuB6M+sGDACelNQgYru7zaxn+MqK1TE45+Iv69bnaMg2Wvzx/niH4goRyzOSPsByM1tpZvuB8cCgqHW6Ah+G0x/lLzezL8xsWTi9HvgaaBrDWJ1zldDaL3Lom/kHFrftT+OLih0Ww8VJLBNJa2BtxPy6sCzSXODKcPpyoJ6kxpErSOoDpAArIoofDZu8npBUq6CdSxouKVNS5qZNm8pyHM65OJnxgz/TnK9p9LifjVRmsUwkBfXrHD0c48+ADElzgAzgKyD3UAVSS+CvwE1mlhcW3wecAJwKNALuKWjnZjbWzNLNLL1pUz+Zca6qWfL+ar475Wcsa9SH5lf1i3c4rgixfI5kHdA2Yr4NsD5yhbDZ6goASXWBK81sezhfH/gX8ICZzYjYJjuc3CfpJYJk5JyrZnZ+70ZS2cfOWv6PYGUXyzOSWUBnScdKSgEGA5MiV5DURFJ+DPcBL4blKcBbBBfi/1/UNi3DnwIuAxbE8BiccxUsR2kgcequyQD0yv5X0Dmj0uIbmCtUzBKJmeUCtwPvA4uB181soaRRki4NVzsHWCrpC6A5kD+4wNVAP+DGAm7zfVXSfGA+0AT4ZayOwTlX8XbMWcnsOn0PzXvnjJWfzKIvW1Q/6enplpmZGe8wnHMlkJsLm5Nb0JyN7CWVFPYzrdutZCx4Nt6h1TiSPjezYm+X8yfbnXOVyuTfzKAFG1nY4CzWTJjhnTNWAd5po3Ou0jADnvgD2xMa0HX1v0moX9c7Z6wC/IzEOVdpzJ64inO3vsny828loX7deIfjSsgTiXOu0si+9ynySODEZ26PdyiuFDyROOcqhdVzt5GxfBwLug2m9vFt4h2OKwVPJM65SiHrjnHUYxetH/9JvENxpeSJxDkXd9s3H6D3tKdY1Pxcml10SrzDcaXkicQ5F3fT7vx/tLF1pNzrZyNVkScS51xc5R4w2v2/x1mT1oVOPxoY73DcUfBE4pyLm41Z2Syv15PuB2bzzY0/hgT/SqqK/LfmnIubJUNH02XfPPZSi+6/uz7e4bijVGQikXRexPSxUcuuiFVQzrnqLb+H34xFzyEglX0k1K3tPfxWUcWdkfw+YvrNqGUPlHMszrkaYseclUxvfy0Hwl6ackjzHn6rsOISiQqZLmjeOedKpHnPlhxIrEUSueSSSAr7yK1Tn2Y9WsQ7NHcUiuu00QqZLmjeOedKrOW6TATMu/tv7Hx3Kilbs4vdxlVOxZ2RHCdpkqR/REznzx9bzLZIGiBpqaTlku4tYHl7SR9KmidpsqQ2EctukLQsfN3ykhs2AAAdFklEQVQQUd5b0vywzqfDkRKdc1XI3t0HSc3dxcJGZ9Prt4PJWDCGM9ZPjHdY7igVd0YyKGL691HLoucPIykRGANcSDB++yxJk8xsUVQdr5jZX8IL+78GrpPUCHgISCc48/k83PYb4DlgODADeBcYALxXzHE45yqRT37+T87L+5J5tz0W71BcOSgykZjZx5HzkpKBk4CvzOzrYuruAyw3s5XhtuMJElNkIukK/Dic/gh4O5y+CPiPmW0Nt/0PMEDSZKC+mX0alr9CMG67JxLnqggzqPvS02QntaX7Ly6PdziuHBR3++/zkrqF08cAc4FXgDmShhRTd2tgbcT8urAs0lzgynD6cqCepMZFbNs6nC6qzvzYh0vKlJS5adOmYkJ1zlWUuX+bT5+d/2P1JT9EyT62XnVQ3DWSvma2MJy+CfjCzLoDvYH/K2bbgq5dRF+g/xmQIWkOkAF8BeQWsW1J6gwKzcaaWbqZpTdt2rSYUJ1zFWXLw0+zhzS6P/2DeIfiyklxiWR/xPSFhE1PZlaSAZTXAW0j5tsA6yNXMLP1ZnaFmZ0C3B+WbS9i23XhdKF1Oucqr+wFWzhz5d+Y230Yddo2inc4rpwUl0i2SbpE0inAWcC/ASQlAcU9gjoL6CzpWEkpwGBgUuQKkppIyo/hPuDFcPp9oL+khpIaAv2B980sG9gp6fTwbq3rgXdKdKTOubib/6MXSGMvbR77UbxDceWouERyK3A78BJwV8SZyPnAv4ra0Mxyw23fBxYDr5vZQkmjJF0arnYOsFTSF0Bz4NFw263AaIJkNAsYlX/hHRgJjAOWAyvwC+3OVQl7dx6g28djmNvkfNpefFK8w3HlSGbV/7nC9PR0y8zMjHcYztVYG7Oy2dnnfDodWEzWqEn0fPC78Q7JlYCkz80svbj1irxlQtLTRS03Mz8/dc4Va8nQ0fQ7sJgd1OPk+3zMkeqmuHvvRgALgNcJLmr7U+TOuRLLURpp7CUjnK/PTkhOIodU0iwnrrG58lPcNZKWwFiCBwSvA5KBSWb2FzP7S6yDc85Vbfm9/OaSCMAe7+W3WioykZjZFjN73szOBW4EGgALJV1XEcE556q25j1bkpeQRCIHySWRWt7Lb7VUosdKJfUChhA8S/Ie8Hksg3LOVR8t181EwJwHJrLnrfe9l99qqLiL7Y8AlxDcvjseuC+8rdc554q1b8c+6ud+w6xmAzl19KUw+tLiN3JVTnFnJA8CK4GTw9evwl7bBZiZ9YhteM65qmzW3a9ztm1k7V13xjsUF0PFJZJixxxxzrmCWJ7R6G9PsSLlRHrdc2G8w3ExVFw38qsLKg/HGhkMFLjcOecWvvAJJ+35nCmDn6Vjgj85UJ0V1418fUn3SXpGUn8F7iBo7rq6YkJ0zlVFO3/1NNtoQK8nr493KC7GinuO5K9AF2A+cAvwAfA9YJCZDSpqQ+dczbVh1lpOXfMmc3rfQt3mdeIdjoux4q6RHBeOP4KkccBmoJ2Z7Yx5ZM65KuuLu56lKcZxj/8w3qG4ClDcGcmB/AkzOwh86UnEOVeUfd/s4aRPx/JZi0G0z+gQ73BcBSjujORkSTvCaQFp4Xz+7b/1Yxqdc67Kmf3TVznDtpL8U7/lt6Yo7q6txIoKxDlX9W2YvZ4eL93FF8ldSf9Jv3iH4ypIcU1bZSJpgKSlkpZLureA5e0kfSRpjqR5kgaG5UMlZUW88iT1DJdNDuvMX9YslsfgnCu5Nd8ZQR32sDOtGfJbfmuMEvW1dTTCZ03GEPTPtQ6YJWmSmS2KWO0BgpETn5PUFXgX6GBmrwKvhvV0B94xs6yI7YaamY9U5Vwlkd9dfJ9wvveOySB5d/E1RCzPSPoAy81spZntJ+irK/qWYQPyr7McQzDmSbQhwGsxi9I5V2Y75qxkVvNLyB9vdQ+1vbv4GiSWiaQ1sDZifl1YFulhYJikdQRnI3cUUM81HJlIXgqbtR5U2PlXNEnDJWVKyty0adNRHYBzrmSa92xJw20rAdhLLWqx17uLr0FimUgK+oKPHiB+CPCymbUBBgJ/lXQoJkmnAXvMbEHENkPDZ1v6hq8Cx0Yxs7Fmlm5m6U2bNi3LcTjnirFr9RY67FvCiuQTWD3hM6Z1G0HK1g3xDstVkFgmknVA24j5NhzZdHUzwTC+mNmnQCrQJGL5YKLORszsq/DnTuDvcKhZ1jkXJwtuf44k8tj54ht0ufpkMhaM4Yz1E+MdlqsgsUwks4DOko6VlEKQFCZFrbMGOB9A0okEiWRTOJ8AXEVwbYWwLElSk3A6mWCslAU45+ImL2cfHd97hk+PuYieQ7vFOxwXBzFLJOEAWLcD7xMMjPW6mS2UNEpS/ug2PwV+IGkuwZnHjWaW3/zVD1hnZisjqq0FvC9pHpAFfAW8EKtjcM4Vb+49f6fpwY3s++FPKfiKpavu9O33dvWVnp5umZl+t7Bz5c6MlfV6sG+/6Lx7LknJnkmqE0mfm1l6cevF9IFE51z1tuzZ/3Dc7gWsuvwnnkRqME8kzrmjlvPo42xQC854eki8Q3Fx5InEOXdUNn64gB7ZH5B5+h00aF4r3uG4OPJE4pw7Kmt/8gf2kMZJf7w13qG4OPNE4pwrtbX/nEuveS8zvfU1dOjdON7huDjzROKcK7Xdg29GGCnsi3corhLwROKcK7EcpYHECbs/R0DGV68FvfwqLd6huTjyROKcK7Edc1aytPYph+a9l18HMRyPxDlX/TTr0pB6e5ZgwF5SvZdfB/gZiXOuFBbd/RK1yWF640GsmTDDe/l1gHeR4pwrqQMHyK7Xma+sFT12TCellj/JXt15FynOuXK19Bev0nLfarJvut+TiDuMJxLnXPEOHqTO079mflJPLvjDwHhH4yoZTyTOuWKteOwN2uz5guVX309abT8bcYfzROKcK5oZib/9FUsTTuC8Z66IdzSuEoppIpE0QNJSScsl3VvA8naSPpI0R9I8SQPD8g6SciRlha/nI7bpLWl+WOfTkg+l41wsrXn2n3TYPo8Fl9zHMQ39f093pJh9KiQlAmOAi4GuwBBJXaNWe4Bg5MRTCIbifTZi2Qoz6xm+RkSUPwcMBzqHrwGxOgbnajwz9j/8KKvUgYznvat4V7BY/nvRB1huZivNbD/B2OuDotYxoH44fQywvqgKJbUE6pvZp+GQvK8Al5Vv2M65fMsefZ1Omz8j89SRNGmZHO9wXCUVy0TSGlgbMb8uLIv0MDBM0jrgXeCOiGXHhk1eH0vqG1HnumLqdM6Vk/oP/xgDGm1dEe9QXCUWy0RS0LWL6KcfhwAvm1kbYCDwV0kJQDbQLmzy+gnwd0n1S1hnsHNpuKRMSZmbNm066oNwribK75yx+cFsBJy3fKx3zugKFctEsg5oGzHfhiObrm4GXgcws0+BVKCJme0zsy1h+efACuD4sM42xdRJuN1YM0s3s/SmTZuWw+E4V3PsmL2CrQmNyAvnvXNGV5RYJpJZQGdJx0pKIbiYPilqnTXA+QCSTiRIJJskNQ0v1iPpOIKL6ivNLBvYKen08G6t64F3YngMztVIByZPp1HeVkDkeOeMrhgxSyRmlgvcDrwPLCa4O2uhpFGSLg1X+ynwA0lzgdeAG8OL6P2AeWH5G8AIM9sabjMSGAcsJzhTeS9Wx+BcjXTgAAkP/pzt1GNyl1u9c0ZXLO+00Tl3mBU/GUPHJ25n/LB/Mviv34l3OC6OvNNG51yp2Y6dNHrmET5JzuC7z3mfWq5kPJE45w5ZOvz3NDywiQ0/+S116nqnEa5kPJE45wDIXbeBdq8/znt1r+K7o/vEOxxXhXgicc4BsGzYIyTbPhJ+8yuS/SF2Vwo+ZrtzNdzGrGy29r2Uzrtm83bzkVx5W6d4h+SqGD8jca6GWzJsNCfsyiSPBFo//yDen7YrLT8jca6GylEaaewlI5xPIZczLm9BDqmkWU5cY3NVi5+ROFdD7Zizkk/aDSYv7MJuD2neDYo7Kn5G4lwN1bxnS7ZvX0YCxn6SqcU+7wbFHRU/I3GuhsrJXEjH7bNZrk4s/ess7wbFHTU/I3GuJjp4kI2X3kJdGrFx4iecdVlTGDYm3lG5KsrPSJyrgVbfM4YO2TOYdO6TQRJxrgw8kThXw+xftpqmT/ycD2tdzJUTh8Y7HFcNeCJxriYxY+3AW8nLAxvzHMc08IdGXNl5InGuhtiYlc2qtBPpuPx9/t8pv+aCm9vHOyRXTcQ0kUgaIGmppOWS7i1geTtJH0maI2mepIFh+YWSPpc0P/x5XsQ2k8M6s8JXs1geg3PVxYprfk77fUvZQHO+86/b4h2Oq0ZidtdWOFTuGOBCgrHWZ0maZGaLIlZ7gGDkxOckdQXeBToAm4Hvmtl6SScRjLLYOmK7oWbmI1U5VwL5T7CfGc63YCO0SvIn2F25ieUZSR9guZmtNLP9wHhgUNQ6BtQPp48B1gOY2RwzWx+WLwRSJdWKYazOVVs75qzky9rdDs3vobY/we7KVSwTSWtgbcT8Og4/qwB4GBgmaR3B2cgdBdRzJTDHzPZFlL0UNms9KHkXc84Vafp0jt2zEANySKUWe/0JdleuYplICvqCjx4gfgjwspm1AQYCf5V0KCZJ3YDHgFsjthlqZt2BvuHrugJ3Lg2XlCkpc9OmTWU4DOeqrt2fzqPeHTewhYb8r9OtrJkww59gd+Uulk+2rwPaRsy3IWy6inAzMADAzD6VlAo0Ab6W1AZ4C7jezFbkb2BmX4U/d0r6O0ET2ivROzezscBYgPT09OgE5ly1l7dpCzsvuIyD1oCVEzI5/+qWAHS52p9gd+Urlmcks4DOko6VlAIMBiZFrbMGOB9A0olAKrBJUgPgX8B9ZjY9f2VJSZKahNPJwCXAghgeg3NVT3Y29OvHml6DaLjnK6b9eCJ9wyTiXCzE7IzEzHIl3U5wx1Ui8KKZLZQ0Csg0s0nAT4EXJP2YoNnrRjOzcLtOwIOSHgyr7A/sBt4Pk0gi8F/ghVgdg3NV0e57R1N76lQ6AC/2fYmbHj8t3iG5ak5m1b/VJz093TIz/W5hV82lpcHevUeWp6ZCjt/m60pP0udmll7cev5ku3PVxLG2kk84/dAdLbtJ428MpYP5bb4utjyROFdNvH3dm5zJDCC4zTeVfbToXJ8Zq/w2XxdbnkicqwaW3PBrTh53B9lqyZ8YTkbKDP7ECBrnbqCF5xEXYz6wlXNV1MasbLIzBqOe3Tl5yhj+3ehanu3zMm2PS+aF4TB27Bj+mw0T4x2oq/Y8kThXRS0dOoq+O6agKVOY1OpWzl30LAOO+baRYYw/LuIqiCcS56qY/E4Y+0WUXbr+T+Q0+At4J4wuDvwaiXNVzJd/eJsd1Ds0750wunjzROJcFWG5B/nssl/T+SeXkMBB8pB3wugqBW/acq6S25iVzaazL0MJ4rSdn/Fx86tIPbiHvc3b0+IXw9kwaiwpW7PjHaarwTyROFeJWZ6x7rzr6LV7JgdI4n/Xv8y5L12PEr7tXNs7YXTx5k1bzlVCZrBPtVBiAr2/+RABKeRy3is3sjexdrzDc+4wnkicqwQ2ZmWT1SCDjXM38OnY+UxrfCm12E8OqewnGfCL6q7y8qYt5yqBJcNG03f7VFb2zuC0g8vYqfrMvOxX7Fu0gjO/eMkvqrtKzc9InKtA+WceX88LRijMURpIZCx8jgSMTge/IAGjlu2lz1v3kbRzK9O6jfCRDV2l5mckzlWgJcNGc/b2afz3uw/yVas+9KEjJxGMpy6CzhZnd7iSzu/8nlTgjPXfdnDiF9VdZeWJxLkY2JiVTfY5g2k1ZQLNerQ49DR6Rrj8ojXjYM048hCLGvely5bp7CeFFPZ785WrcmLatCVpgKSlkpZLureA5e0kfSRpjqR5kgZGLLsv3G6ppItKWqdzlcGSYaPpvn0anw96hD/cMJfX6g5nB3UPLc8lkbnNL2Rz1ldsT2nizVeuSovZCImSEoEvgAuBdQRjuA8xs0UR64wF5pjZc5K6Au+aWYdw+jWgD9CKYEjd48PNiqyzID5CoouVws48ohmwKaU1TfavZz8pJHOAad1uJWPBsxUftHMlVBlGSOwDLDezlWa2HxgPDIpax4D64fQxwPpwehAw3sz2mdmXwPKwvpLU6Vy5ys6GjAzYUMCJwsIhwZnHpxn3cH/rl3mPAewl5dDygyTwRcM+bP5oPisa92Fqt5GsnvCZn3m4aiWW10haA2sj5tcBp0Wt8zDwgaQ7gDrABRHbzojatnU4XVydAEgaDgwHaNeuXemjdzVOdjYMHgwTJnDYYFBP3ZvNqCmDefzuCQy+qwUzZ8JNtwUjEJ4XrjNo2ysM2vYKBqyv25mWu5YfOvPIbtWb4885iaZ+4dxVU7E8I1EBZdHtaEOAl82sDTAQ+KukhCK2LUmdQaHZWDNLN7P0pk2bliJsV10UdiZRWHmQMDJ44p4NbNgAtWqBBMe+8hB9mcqlf/seX6QPYeBt7UllH/Dth28/ycxpNZBNc75iTb2T/MzD1SixTCTrgLYR8234tukq383A6wBm9imQCjQpYtuS1FluSvtFVFQTSHnVFc99VNZ9Rz+bkS8/MTx5b8Hlj96xgalTISUlSBjtX3mEvkwl45WbeKblL9mzPxFD3MoLJGD0ZTpDGE+7hHXYE08yu8MV5JFADqkkcpAdDdvTrGcrzlg/kYwFY+hy9clkLBhz2C28zlVLZhaTF0Gz2UrgWCAFmAt0i1rnPeDGcPpEgqQgoFu4fq1w+5VAYknqLOjVu3dvOxr3XL/eJtPP7rkhu0zl5VlXPPdRWfc9uetIyyXB/tN5pM2da5aSYgZmYwjKxzDSwCyRA9aSr2wC37ODyKZypv2RH1ouCcEGUa88sM21W9s+kszAdpNqn3UeapYdxPBJy8ttcrfbbMmELJvc7Tb7pOXlR8TmXFUGZFpJvu9LstLRvgiaq74AVgD3h2WjgEvD6a7A9DAhZAH9I7a9P9xuKXBxUXUW9yptIklNLfiLKP9V0vKkpOBVHnXFcx9F7TsxsWL3/SdusaZstPZ8aTmkFJoASlu+uU5b2x8mjBxq2ayOV5tlZ9t7HUbYQRLsYEqqHSTB3j12ZKk+S85VZSVNJDG7/bcyKe3tv5aahvYVfAtnaS7e5JIIQBIHy1xXPPdRWPnBsGU0kbwK33dxDiiZRMslASOXRLbW70Czweez6q3ZtNg0j1T2s4c0FnS+nD5THuffZzxC/1Vjg3au/ft5/9hbuXjls3DFFdCyJQwfDmPHBm1sE72pytUMleH23ypLX65kZqdrD31J5yH2JtVBrVqxN6kOeeFXW3HlSW1bkdS2dNuodetSlSe1a01SuyOX5STVDbepW7Ly5LqoTRtykqPL66F27chJrndY+Z7k+iR27khi547sSa5/2LLdKQ3Q8cezO6UhB8PygySwM7UJOvlkdqQ2PZSEDpLAN7VboX792FqnzaH3PJdENtbvhK65huwGJ3AgvMFwP8msapoOTzwBEyYwv+0A8hD7SCEPMf2E70NuLh+2vxkQeSmpJGB83rg//OlPLKmTTgq55KUEd11tyT0GWrRgwCkbSbhtBAkzZ5Bw2wgu7hleV5k4EcaMgZNPDn56EnHuCJ5ICtKyJVtz65OAkZeSCoiP2l4PX33F5DbXkf8FVWz5mjWwZk3ptlm3rnTlq1fD6tVHLJvc9rpwm2ElK29zHaxdy8eto8uHwerVfNx66GHlH7cZCl98AV98wZTW1x62bErrIbB0KVNbXYPCcgHTWl4FWVl80uJKBIfKZzQfBB9/zMymlxx6zxMwZje+EMaPZ26Dc0gkj7yUVJI4yOK6p8Jdd8HVV7MrN42p3UayasJMpnYbScL2byAxsdDE4AnDufLnTVuFKaxJo7Tl5VlXPPdRWfftnIuZkjZteSJxzjlXIL9G4pxzrkJ4InHOOVcmnkicc86ViScS55xzZeKJxDnnXJl4InHOOVcmNeL2X0nbgWURRccA26OmC/vZBNhcwl1F1luSZdFlsYqrqNg8roqNK7+MShpXflmyxxWXuIqLo7C4CoqxPOJqb2bFj8NRkg65qvoLGFvYfP50ET9L1GlZQfspbllFxVVUbB5XxcaVP11Z44qIz+OKQ1zFxVFYDAXFWN5xFfWqKU1b/yhi/h/F/CzLfopbVlFxFbWdx1WxceVPV9a4itqHxxX7uIqLo7AYCoqnvOMqVI1o2ioLSZlWgic7K5rHVToeV+l4XKVT0+OqKWckZTE23gEUwuMqHY+rdDyu0qnRcfkZiXPOuTLxMxLnnHNl4onEOedcmXgicc45VyaeSMpA0jmSpkp6XtI58Y4nkqQ6kj6XdEm8Y8kn6cTwvXpD0sh4x5NP0mWSXpD0jqT+8Y4nn6TjJP1Z0huVIJY6kv4Svk9D4x1Pvsr0HkWqxJ+pmPwN1thEIulFSV9LWhBVPkDSUknLJd1bTDUG7AJSgXWVKC6Ae4DXyyOm8orLzBab2QjgaqBcbkksp7jeNrMfADcC11SiuFaa2c3lEU85xHgF8Eb4Pl0aq5hKG1es36MyxFXun6lyiqvc/wYJK66RL6Af0AtYEFGWCKwAjgNSgLlAV6A78M+oVzMgIdyuOfBqJYrrAmAwwYf4ksoSV7jNpcAnwLWVKa5wu8eBXpUwrjcqwd/AfUDPcJ2/xyKeo4kr1u9ROcRVbp+p8oqrvP8GzYwkaigzmyKpQ1RxH2C5ma0EkDQeGGRmvwaKaiL6BqhVWeKSdC5Qh+ALIEfSu2aWF++4wnomAZMk/Qv4e1liKq+4JAn4DfCemc0ua0zlFVeslSZGgjPuNkAWMW7JKGVci2IZy9HGJWkx5fyZKo+4gEXl/TcI1NxEUojWwNqI+XXAaYWtLOkK4CKgAfBMZYnLzO4P47sR2FzWJFJecYXXka4gSLrvxiimUscF3EFwFneMpE5m9nxliEtSY+BR4BRJ94UJJ9YKi/Fp4BlJ3+Hou3cp97ji9B4VGxcV95kqVVyx+hv0RHI4FVBW6BObZjYRmBi7cA4pVVyHVjB7ufxDOUxp36/JwORYBROhtHE9TfBFGWuljWsLMCJ24RSowBjNbDdwUwXHEqmwuOLxHkUqLK6K+kwVprC4JhODv8Eae7G9EOuAthHzbYD1cYolksdVOh7X0ausMXpcpVOhcXkiOdwsoLOkYyWlEFywnhTnmMDjKi2P6+hV1hg9rtKp2LhifadDZX0BrwHZwAGC7H1zWD4Q+ILgjof7PS6PqzrFVRVi9LiqXlzeaaNzzrky8aYt55xzZeKJxDnnXJl4InHOOVcmnkicc86ViScS55xzZeKJxDnnXJl4InHuKEnaVU71PCzpZyVY72VJ3yuPfTpXnjyROOecKxNPJM6VkaS6kj6UNFvSfEmDwvIOkpZIGidpgaRXJV0gabqkZZL6RFRzsqT/heU/CLeXpGckLQq7/G4Wsc9fSJoV1js27ArfubjwROJc2e0FLjezXsC5wOMRX+ydgKeAHsAJwLXA2cDPgJ9H1NED+A5wBvALSa2Ay4EuBANf/QA4M2L9Z8zsVDM7CUgjDuOZOJfPu5F3ruwE/EpSPyCPYCyI5uGyL81sPoCkhcCHZmaS5gMdIup4x8xyCAYi+4hgYKJ+wGtmdhBYL+l/EeufK+n/gNpAI2Ah8RknxDlPJM6Vg6FAU6C3mR2QtApIDZfti1gvL2I+j8P//qI7vbNCypGUCjwLpJvZWkkPR+zPuQrnTVvOld0xwNdhEjkXaH8UdQySlBqO+HcOQTfgU4DBkhIltSRoNoNvk8ZmSXUBv5PLxZWfkThXdq8C/5CUSTCm+ZKjqGMm8C+gHTDazNZLegs4D5hP0B34xwBmtk3SC2H5KoKk41zceDfyzjnnysSbtpxzzpWJJxLnnHNl4onEOedcmXgicc45VyaeSJxzzpWJJxLnnHNl4onEOedcmXgicc45Vyb/H+FBBHW3Fyk6AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "seed = 42\n",
    "degree = 2\n",
    "split_ratio = 0.9\n",
    "ridge_regression_demo(x, y, degree, split_ratio, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T18:30:20.729742Z",
     "start_time": "2019-10-18T18:30:20.721124Z"
    }
   },
   "outputs": [],
   "source": [
    "def cross_validation(y, x, k_indices, k, lambda_, degree):\n",
    "    \"\"\"return the loss of ridge regression.\"\"\"\n",
    "    # get k'th subgroup in test, others in train\n",
    "    id_test = k_indices[k]\n",
    "    id_train = k_indices[~(np.arange(k_indices.shape[0]) == k)].reshape(-1)\n",
    "    x_test = x[id_test]\n",
    "    x_train = x[id_train]\n",
    "    y_test = y[id_test]\n",
    "    y_train = y[id_train]\n",
    "    #Clean\n",
    "    if clean:\n",
    "        y_train, x_train = clean_data(y_train, x_train)\n",
    "        y_test, x_test = clean_data(y_test, x_test)\n",
    "    # Standardize\n",
    "    x_train_std = standardize(x_train)[0]\n",
    "    x_test_std = standardize(x_test)[0]\n",
    "    # Define feature matrix\n",
    "    tx_train = build_poly(x_train_std, degree)\n",
    "    tx_test = build_poly(x_test_std, degree)\n",
    "    # ridge regression\n",
    "    weight, loss_tr = ridge_regression(y_train, tx_train, lambda_)\n",
    "    # calculate the loss for train and test data\n",
    "    loss_te = compute_loss(y_test, tx_test, weight)\n",
    "    accuracy = accuracy_ratio(predict_labels(weight, tx_test), y_test)\n",
    "\n",
    "    return loss_tr, loss_te, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T18:31:38.710840Z",
     "start_time": "2019-10-18T18:30:26.506760Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best lambda from accuracy: 1.00e-07\n",
      "Best lambda from error: 3.29e-02\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def cross_validation_visualization(lambds, mse_tr, mse_te):\n",
    "    \"\"\"visualization the curves of mse_tr and mse_te.\"\"\"\n",
    "    best_l_err = lambds[np.argmin(mse_te)]\n",
    "    print('Best lambda from error: %.2e'%best_l_err)\n",
    "    plt.semilogx(lambds, mse_tr, marker=\".\", color='b', label='train error')\n",
    "    plt.semilogx(lambds, mse_te, marker=\".\", color='r', label='test error')\n",
    "    plt.axvline(best_l_err, c = 'g', label = '$\\lambda^*_{rmse}=%.1e$'%best_l_err, ls = ':')\n",
    "    plt.xlabel(\"lambda\")\n",
    "    plt.ylabel(\"rmse\")\n",
    "    plt.title(\"cross validation\")\n",
    "    plt.legend(loc=0)\n",
    "    plt.grid(True)\n",
    "    plt.savefig(\"../results/cross_validation\")\n",
    "def cross_validation_visualization_accuracy(lambdas, accuracies):\n",
    "    \"\"\"visualization the curves of mse_tr and mse_te.\"\"\"\n",
    "    plt.semilogx(lambdas, accuracies, lw =2, marker = '*', label = 'Accuracy ratio')\n",
    "    best_l_acc = lambdas[np.argmax(accuracies)]\n",
    "    plt.axvline(best_l_acc, c= 'k', label = '$\\lambda^*_{acc}=%.1e$'%best_l_acc, ls = ':')\n",
    "    print('Best lambda from accuracy: %.2e'%best_l_acc)\n",
    "    plt.xlabel(\"lambda\")\n",
    "    plt.ylabel(\"accuracy\")\n",
    "    plt.title(\"cross validation\")\n",
    "    plt.legend(loc=0)\n",
    "    plt.grid(True)\n",
    "    plt.savefig(\"../results/cross_validation_accuracies\")\n",
    "def cross_validation_demo():\n",
    "    seed = 42\n",
    "    degree = 2\n",
    "    k_fold = 4\n",
    "    lambdas = np.logspace(-7, 3, 30)\n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    # define lists to store the loss of training data and test data\n",
    "    rmse_tr = []\n",
    "    rmse_te = []\n",
    "    std_tr = []\n",
    "    std_te = []\n",
    "    accuracies = []\n",
    "    # cross validation\n",
    "    for ind, lambda_ in enumerate(lambdas):\n",
    "        x_validation = np.array([cross_validation(y, x, k_indices, k, lambda_, degree) for k in range(k_fold)])\n",
    "        rmse_tr.append(np.mean(np.sqrt(2 * x_validation[:, 0])))\n",
    "        rmse_te.append(np.mean(np.sqrt(2 * x_validation[:, 1])))\n",
    "        std_tr.append(np.std(np.sqrt(2 * x_validation[:, 0])))\n",
    "        std_te.append(np.std(np.sqrt(2 * x_validation[:, 1])))\n",
    "        accuracies.append(np.mean(x_validation[:,2]))\n",
    "    cross_validation_visualization_accuracy(lambdas, accuracies)\n",
    "    plt.figure()\n",
    "    cross_validation_visualization(lambdas, rmse_tr, rmse_te)\n",
    "\n",
    "cross_validation_demo()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Bias-Variance test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T12:53:17.453735Z",
     "start_time": "2019-10-17T12:50:27.446578Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-19e5c47428f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m \u001b[0mbias_variance_demo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-31-19e5c47428f6>\u001b[0m in \u001b[0;36mbias_variance_demo\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mindex_degrees\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdegree\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdegrees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mtx_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_poly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_std\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0mtx_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_poly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test_std\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_tr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mridge_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.89e-05\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mloss_te\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mse'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gdrive/EPFL/2019-2020/MachineLearning/Project/ml-project1/scripts/implementations.py\u001b[0m in \u001b[0;36mbuild_poly\u001b[0;34m(x, degree)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbuild_poly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;34m\"\"\"polynomial basis functions for input data x, for j=0 up to j=degree.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m     \u001b[0mphi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdegree\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m     \u001b[0mphi_off\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mphi_off\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gdrive/EPFL/2019-2020/MachineLearning/Project/ml-project1/scripts/implementations.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbuild_poly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;34m\"\"\"polynomial basis functions for input data x, for j=0 up to j=degree.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m     \u001b[0mphi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdegree\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m     \u001b[0mphi_off\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mphi_off\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def bias_variance_decomposition_visualization(degrees, rmse_tr, rmse_te):\n",
    "    \"\"\"visualize the bias variance decomposition.\"\"\"\n",
    "    rmse_tr_mean = np.expand_dims(np.mean(rmse_tr, axis=0), axis=0)\n",
    "    rmse_te_mean = np.expand_dims(np.mean(rmse_te, axis=0), axis=0)\n",
    "    print(rmse_te_mean, rmse_tr_mean)\n",
    "    plt.plot(degrees,\n",
    "             rmse_tr.T,\n",
    "             'b',\n",
    "             linestyle=\"-\",\n",
    "             color=([0.7, 0.7, 1]),\n",
    "             label='train',\n",
    "             linewidth=0.3)\n",
    "    plt.plot(degrees,\n",
    "             rmse_te.T,\n",
    "             'r',\n",
    "             linestyle=\"-\",\n",
    "             color=[1, 0.7, 0.7],\n",
    "             label='test',\n",
    "             linewidth=0.3)\n",
    "    plt.plot(degrees,\n",
    "             rmse_tr_mean.T,\n",
    "             'b',\n",
    "             linestyle=\"-\",\n",
    "             label='train',\n",
    "             linewidth=3)\n",
    "    plt.plot(degrees,\n",
    "             rmse_te_mean.T,\n",
    "             'r',\n",
    "             linestyle=\"-\",\n",
    "             label='test',\n",
    "             linewidth=3)\n",
    "    plt.ylim(0.7, 1)\n",
    "    plt.xlabel(\"degree\")\n",
    "    plt.ylabel(\"error\")\n",
    "    plt.title(\"Bias-Variance Decomposition\")\n",
    "    plt.savefig(\"bias_variance\")\n",
    "\n",
    "\n",
    "def bias_variance_demo():\n",
    "    \"\"\"The entry.\"\"\"\n",
    "    # define parameters\n",
    "    seeds = range(100)\n",
    "    ratio_train = 0.5\n",
    "    degrees = range(1, 8)\n",
    "    # define list to store the variable\n",
    "    rmse_tr = np.empty((len(seeds), len(degrees)))\n",
    "    rmse_te = np.empty((len(seeds), len(degrees)))\n",
    "    for index_seed, seed in enumerate(seeds):\n",
    "        np.random.seed(seed)\n",
    "        # split data with a specific seed\n",
    "        x_train, y_train, x_test, y_test = split_data(x, y, ratio_train, seed)\n",
    "        x_train_std = standardize(x_train)[0]\n",
    "        x_test_std = standardize(x_test)[0]\n",
    "        for index_degrees, degree in enumerate(degrees):\n",
    "            tx_train = build_poly(x_train_std, degree)\n",
    "            tx_test = build_poly(x_test_std, degree)\n",
    "            weight, loss_tr = ridge_regression(y_train, tx_train, 1.89e-05 )\n",
    "            loss_te = compute_loss(y_test, tx_test, weight, kind='mse')\n",
    "            rmse_tr[index_seed, index_degrees] = np.sqrt(2 * loss_tr)\n",
    "            rmse_te[index_seed, index_degrees] = np.sqrt(2 * loss_te)\n",
    "    bias_variance_decomposition_visualization(degrees, rmse_tr, rmse_te)\n",
    "\n",
    "\n",
    "bias_variance_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T12:54:53.029395Z",
     "start_time": "2019-10-17T12:54:47.011563Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(568238,)\n"
     ]
    }
   ],
   "source": [
    "print(id_out_test.shape)\n",
    "x_out_test_std = standardize_features(x_out_test)\n",
    "x_out = x_out_test_std[0]\n",
    "tx_out = build_poly(x_out, 2)\n",
    "\n",
    "create_csv_submission(id_out_test, predict_labels(w_rr, tx_out) , '../results/rr_pred.csv')\n",
    "create_csv_submission(id_out_test, predict_labels(w_gd, tx_out) , '../results/gd_pred_accel.csv')\n",
    "create_csv_submission(id_out_test, predict_labels(w_lsq, tx_out) , '../results/lsq_pred.csv')\n",
    "#create_csv_submission(id_out_test, predict_labels(w_sgd, tx_out) , '../results/sgd_pred_noadapt.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T18:32:32.794589Z",
     "start_time": "2019-10-18T18:32:32.777297Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_labels_log(weights, data):\n",
    "    \"\"\"Generates class predictions given weights, and a test data matrix\"\"\"\n",
    "    y_pred = np.dot(data, weights)\n",
    "    y_pred[np.where(y_pred <= 0.5)] = -1\n",
    "    y_pred[np.where(y_pred > 0.5)] = 1\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T18:32:38.319778Z",
     "start_time": "2019-10-18T18:32:38.223130Z"
    }
   },
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'w_bar' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-0be42fef88c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m                                         \u001b[0mpr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                                         \u001b[0madapt_gamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m                                        accel=True)\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mlrgd_prediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_labels_log\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_lrgd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gdrive/EPFL/2019-2020/MachineLearning/Project/ml-project1/scripts/implementations.py\u001b[0m in \u001b[0;36mlogistic_regression\u001b[0;34m(y, tx, initial_w, max_iters, gamma, threshold, adapt_gamma, pr, accel)\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_loss_logistic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;31m# update w by gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0madapt_gamma\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m             \u001b[0mgamma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgamma_0\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_iter\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0maccel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'w_bar' referenced before assignment"
     ]
    }
   ],
   "source": [
    "y_train_log = np.copy(y_train)\n",
    "y_train_log[y_train == -1] = 0\n",
    "\n",
    "y_test_log = np.copy(y_test)\n",
    "y_test_log[y_test == -1] = 0\n",
    "\n",
    "w_init = np.array([0] * tx_train.shape[1])\n",
    "max_iter = 400\n",
    "gamma = 1e-6\n",
    "w_lrgd, loss_lrgd = logistic_regression(y_train_log,\n",
    "                                        tx_train,\n",
    "                                        w_init,\n",
    "                                        max_iter,\n",
    "                                        gamma,\n",
    "                                        pr=True,\n",
    "                                        adapt_gamma=False,\n",
    "                                       accel=True)\n",
    "\n",
    "lrgd_prediction = predict_labels_log(w_lrgd, tx_test)\n",
    "print(tx_test.dot(w_lrgd))\n",
    "acc_lrgd = accuracy_ratio(lrgd_prediction, y_test)\n",
    "\n",
    "print('Accuracy ratio = %.3f' % acc_lrgd)\n",
    "print('Test loss = %.3f' % compute_loss_logistic(y_test_log, tx_test, w_lrgd))\n",
    "print('Train loss = %.3f' % loss_lrgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T18:33:13.633702Z",
     "start_time": "2019-10-18T18:32:41.270424Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Regularized Logistic Regression GD (0/399): loss=173269.466460989\n",
      " Regularized Logistic Regression GD (100/399): loss=125009.66602998153\n",
      " Regularized Logistic Regression GD (200/399): loss=119068.38676597021\n",
      " Regularized Logistic Regression GD (300/399): loss=116520.99870399895\n",
      "Accuracy ratio = 0.760\n",
      "Test loss = 10.842\n",
      "Train loss = 115192.766\n"
     ]
    }
   ],
   "source": [
    "lambda_ = 500\n",
    "gamma = 1e-7\n",
    "w_rlrgd, loss_rlrgd = reg_logistic_regression(y_train_log,\n",
    "                                              tx_train,\n",
    "                                              lambda_,\n",
    "                                              w_init,\n",
    "                                              max_iter,\n",
    "                                              gamma,\n",
    "                                              pr=True,\n",
    "                                              adapt_gamma=False, \n",
    "                                              accel=False)\n",
    "rlrgd_prediction = predict_labels_log(w_rlrgd, tx_test)\n",
    "acc_rlrgd = accuracy_ratio(rlrgd_prediction, y_test)\n",
    "print('Accuracy ratio = %.3f' % acc_rlrgd)\n",
    "print('Test loss = %.3f' % compute_loss_logistic(y_test_log, tx_test, w_rlrgd))\n",
    "print('Train loss = %.3f' % loss_rlrgd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T17:51:14.763658Z",
     "start_time": "2019-10-18T17:51:14.741402Z"
    }
   },
   "source": [
    "def cross_validation(y, x, k_indices, k, lambda_, degree):\n",
    "    \"\"\"return the loss of ridge regression.\"\"\"\n",
    "    gamma = 1e-8\n",
    "    # get k'th subgroup in test, others in train\n",
    "    id_test = k_indices[k]\n",
    "    id_train = k_indices[~(np.arange(k_indices.shape[0]) == k)].reshape(-1)\n",
    "    x_test = x[id_test]\n",
    "    x_train = x[id_train]\n",
    "    y_test = y[id_test]\n",
    "    y_train = y[id_train]\n",
    "    #Clean\n",
    "    if clean:\n",
    "        y_train, x_train = clean_data(y_train, x_train)\n",
    "        y_test, x_test = clean_data(y_test, x_test)\n",
    "    # Standardize\n",
    "    x_train_std = standardize(x_train)[0]\n",
    "    x_test_std = standardize(x_test)[0]\n",
    "    # Define feature matrix\n",
    "    tx_train = build_poly(x_train_std, degree)\n",
    "    tx_test = build_poly(x_test_std, degree)\n",
    "    y_train_log = np.copy(y_train)\n",
    "    y_train_log[y_train == -1] = 0\n",
    "\n",
    "    y_test_log = np.copy(y_test)\n",
    "    y_test_log[y_test == -1] = 0\n",
    "    # logistic regression\n",
    "    weight, loss_tr = reg_logistic_regression(y_train_log,\n",
    "                                              tx_train,\n",
    "                                              lambda_,\n",
    "                                              w_init,\n",
    "                                              max_iter,\n",
    "                                              gamma,\n",
    "                                              pr=True,\n",
    "                                              adapt_gamma=False)\n",
    "    # calculate the loss for train and test data\n",
    "    loss_te = compute_loss_logistic(y_test, tx_test, weight)\n",
    "    accuracy = accuracy_ratio(predict_labels(weight, tx_test), y_test)\n",
    "\n",
    "    return loss_tr, loss_te, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T17:57:34.213341Z",
     "start_time": "2019-10-18T17:51:15.297351Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2590.424676956365\n",
      " Regularized Logistic Regression GD (200/3999): loss=2582.527937577399\n",
      " Regularized Logistic Regression GD (300/3999): loss=2575.368552847789\n",
      " Regularized Logistic Regression GD (400/3999): loss=2568.780904032578\n",
      " Regularized Logistic Regression GD (500/3999): loss=2562.6448385416898\n",
      " Regularized Logistic Regression GD (600/3999): loss=2556.8724341413717\n",
      " Regularized Logistic Regression GD (700/3999): loss=2551.398675044787\n",
      " Regularized Logistic Regression GD (800/3999): loss=2546.174884591752\n",
      " Regularized Logistic Regression GD (900/3999): loss=2541.1641459193565\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2536.338101679504\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2531.67469777888\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2527.1565742708344\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2522.7699036668982\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2518.5035421832426\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2514.3484026436186\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2510.2969863799462\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2506.343030570849\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2502.4812403438923\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2498.7070837675974\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2495.01663394787\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2491.4064467084545\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2487.873465360162\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2484.4149462333644\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2481.0284002225717\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2477.711546745117\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2474.4622773692267\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2471.278627004066\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2468.158751023333\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2465.100907058168\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2462.1034404722886\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2459.1647727466284\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2456.283392165365\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2453.4578463241414\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2450.686736081433\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2447.968710653399\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2445.3024636138985\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2442.686729611312\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2440.1202816523096\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2437.601928833603\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2590.0469296126385\n",
      " Regularized Logistic Regression GD (200/3999): loss=2582.023647994793\n",
      " Regularized Logistic Regression GD (300/3999): loss=2574.771041365607\n",
      " Regularized Logistic Regression GD (400/3999): loss=2568.103895059401\n",
      " Regularized Logistic Regression GD (500/3999): loss=2561.8956293314463\n",
      " Regularized Logistic Regression GD (600/3999): loss=2556.055111975363\n",
      " Regularized Logistic Regression GD (700/3999): loss=2550.5157229927668\n",
      " Regularized Logistic Regression GD (800/3999): loss=2545.228044802811\n",
      " Regularized Logistic Regression GD (900/3999): loss=2540.15487498903\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2535.267801800964\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2530.54482608884\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2525.9686870167156\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2521.525666340385\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2517.2047226654354\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2512.9968566126217\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2508.894639922504\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2504.8918625616984\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2500.983265850376\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2497.164339029226\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2493.4311631047167\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2489.780290261239\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2486.208650254292\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2482.7134774235333\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2479.2922535662715\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2475.9426630792095\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2472.662557634964\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2469.449928298811\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2466.3028834700094\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2463.2196313944887\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2460.1984662722994\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2457.2377571951088\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2454.3359393136075\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2451.4915067619218\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2448.7030069661787\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2445.969036042561\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2443.2882350521545\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2440.659286928503\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2438.080913932732\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2435.551875521721\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2588.636489675893\n",
      " Regularized Logistic Regression GD (200/3999): loss=2579.2932120083005\n",
      " Regularized Logistic Regression GD (300/3999): loss=2570.9708957736707\n",
      " Regularized Logistic Regression GD (400/3999): loss=2563.450808399197\n",
      " Regularized Logistic Regression GD (500/3999): loss=2556.568387786644\n",
      " Regularized Logistic Regression GD (600/3999): loss=2550.199533595191\n",
      " Regularized Logistic Regression GD (700/3999): loss=2544.250139570605\n",
      " Regularized Logistic Regression GD (800/3999): loss=2538.6482616491558\n",
      " Regularized Logistic Regression GD (900/3999): loss=2533.3383842683625\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2528.2772521400993\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2523.4308357617356\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2518.772109429008\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2514.2794112275546\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2509.93522184275\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2505.7252469974787\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2501.6377219170995\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2497.6628796366886\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2493.7925413225653\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2490.0197982704576\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2486.338763375142\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2482.744375670959\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2479.2322457226514\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2475.7985326829007\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2472.4398460594325\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2469.153166879997\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2465.9357841711308\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2462.785243588212\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2459.699305732652\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2456.675912224854\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2453.7131580100768\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2450.8092686912596\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2447.9625819281714\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2445.1715321352685\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2442.4346378619534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Regularized Logistic Regression GD (3500/3999): loss=2439.750491358589\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2437.117749927169\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2434.535128731349\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2432.0013948015985\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2429.5153620201713\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2587.984238979349\n",
      " Regularized Logistic Regression GD (200/3999): loss=2578.347880933269\n",
      " Regularized Logistic Regression GD (300/3999): loss=2569.895495219469\n",
      " Regularized Logistic Regression GD (400/3999): loss=2562.333773595912\n",
      " Regularized Logistic Regression GD (500/3999): loss=2555.4575690830497\n",
      " Regularized Logistic Regression GD (600/3999): loss=2549.1208224311563\n",
      " Regularized Logistic Regression GD (700/3999): loss=2543.2176122334013\n",
      " Regularized Logistic Regression GD (800/3999): loss=2537.6696319147436\n",
      " Regularized Logistic Regression GD (900/3999): loss=2532.417937389024\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2527.4174343918285\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2522.63312197121\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2518.037478604247\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2513.6086070471288\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2509.328893856436\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2505.184025210883\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2501.1622540589897\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2497.2538475733236\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2493.4506659421677\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2489.745838140752\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2486.1335101926197\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2482.608648212489\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2479.166883258706\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2475.804388378917\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2472.517780643359\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2469.3040427127758\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2466.16045977716\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2463.0845686592274\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2460.0741165955524\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2457.127027752667\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2454.241375950526\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2451.4153623861457\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2448.6472973975756\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2445.935585501742\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2443.278713091694\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2440.675238298419\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2438.1237826176025\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2435.6230239775346\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2433.171690985138\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2430.768558136069\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2590.4246769563656\n",
      " Regularized Logistic Regression GD (200/3999): loss=2582.527937577403\n",
      " Regularized Logistic Regression GD (300/3999): loss=2575.3685528477977\n",
      " Regularized Logistic Regression GD (400/3999): loss=2568.7809040325947\n",
      " Regularized Logistic Regression GD (500/3999): loss=2562.644838541708\n",
      " Regularized Logistic Regression GD (600/3999): loss=2556.872434141401\n",
      " Regularized Logistic Regression GD (700/3999): loss=2551.3986750448203\n",
      " Regularized Logistic Regression GD (800/3999): loss=2546.1748845918\n",
      " Regularized Logistic Regression GD (900/3999): loss=2541.1641459194125\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2536.3381016795693\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2531.674697778946\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2527.1565742709217\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2522.7699036670074\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2518.5035421833622\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2514.348402643754\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2510.296986380094\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2506.343030571012\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2502.4812403440687\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2498.7070837677975\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2495.01663394808\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2491.4064467086996\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2487.873465360422\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2484.4149462336395\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2481.028400222874\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2477.7115467454255\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2474.462277369564\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2471.278627004408\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2468.158751023709\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2465.1009070585633\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2462.1034404727006\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2459.1647727470613\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2456.283392165839\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2453.4578463246235\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2450.6867360819506\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2447.9687106539327\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2445.302463614448\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2442.6867296118953\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2440.1202816529108\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2437.6019288342163\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2590.04692961264\n",
      " Regularized Logistic Regression GD (200/3999): loss=2582.0236479947953\n",
      " Regularized Logistic Regression GD (300/3999): loss=2574.771041365614\n",
      " Regularized Logistic Regression GD (400/3999): loss=2568.103895059415\n",
      " Regularized Logistic Regression GD (500/3999): loss=2561.895629331467\n",
      " Regularized Logistic Regression GD (600/3999): loss=2556.0551119753964\n",
      " Regularized Logistic Regression GD (700/3999): loss=2550.515722992802\n",
      " Regularized Logistic Regression GD (800/3999): loss=2545.228044802859\n",
      " Regularized Logistic Regression GD (900/3999): loss=2540.154874989088\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2535.2678018010283\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2530.544826088921\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2525.9686870168084\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2521.525666340495\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2517.204722665556\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2512.996856612756\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2508.8946399226534\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2504.8918625618703\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2500.9832658505698\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2497.164339029427\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2493.4311631049404\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2489.780290261481\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2486.208650254557\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2482.713477423804\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2479.292253566564\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2475.9426630795233\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2472.6625576352985\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2469.4499282991674\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2466.3028834703746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Regularized Logistic Regression GD (2900/3999): loss=2463.21963139489\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2460.1984662727155\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2457.237757195549\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2454.3359393140768\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2451.4915067624197\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2448.703006966689\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2445.9690360430977\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2443.288235052711\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2440.659286929084\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2438.080913933334\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2435.551875522341\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2588.6364896758946\n",
      " Regularized Logistic Regression GD (200/3999): loss=2579.293212008306\n",
      " Regularized Logistic Regression GD (300/3999): loss=2570.97089577368\n",
      " Regularized Logistic Regression GD (400/3999): loss=2563.4508083992096\n",
      " Regularized Logistic Regression GD (500/3999): loss=2556.5683877866686\n",
      " Regularized Logistic Regression GD (600/3999): loss=2550.199533595226\n",
      " Regularized Logistic Regression GD (700/3999): loss=2544.250139570643\n",
      " Regularized Logistic Regression GD (800/3999): loss=2538.648261649213\n",
      " Regularized Logistic Regression GD (900/3999): loss=2533.3383842684257\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2528.2772521401725\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2523.430835761826\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2518.7721094291123\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2514.279411227661\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2509.935221842871\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2505.7252469976197\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2501.63772191726\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2497.6628796368705\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2493.7925413227604\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2490.019798270675\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2486.338763375368\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2482.7443756712014\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2479.232245722906\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2475.798532683189\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2472.4398460597363\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2469.1531668803177\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2465.9357841714755\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2462.7852435885657\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2459.699305733039\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2456.675912225251\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2453.7131580105006\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2450.809268691704\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2447.9625819286325\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2445.1715321357565\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2442.4346378624637\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2439.7504913591197\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2437.117749927716\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2434.5351287319345\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2432.0013948021997\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2429.515362020797\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2587.984238979351\n",
      " Regularized Logistic Regression GD (200/3999): loss=2578.347880933273\n",
      " Regularized Logistic Regression GD (300/3999): loss=2569.8954952194813\n",
      " Regularized Logistic Regression GD (400/3999): loss=2562.333773595931\n",
      " Regularized Logistic Regression GD (500/3999): loss=2555.457569083073\n",
      " Regularized Logistic Regression GD (600/3999): loss=2549.120822431189\n",
      " Regularized Logistic Regression GD (700/3999): loss=2543.217612233443\n",
      " Regularized Logistic Regression GD (800/3999): loss=2537.669631914795\n",
      " Regularized Logistic Regression GD (900/3999): loss=2532.4179373890915\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2527.4174343919003\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2522.6331219712915\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2518.037478604348\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2513.6086070472406\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2509.328893856561\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2505.184025211023\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2501.162254059144\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2497.2538475735005\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2493.4506659423455\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2489.7458381409683\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2486.133510192837\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2482.6086482127275\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2479.1668832589676\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2475.8043883791984\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2472.517780643656\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2469.3040427130863\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2466.1604597774904\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2463.084568659575\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2460.074116595935\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2457.127027753066\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2454.241375950938\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2451.415362386581\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2448.6472973980335\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2445.9355855022222\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2443.278713092195\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2440.675238298944\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2438.1237826181437\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2435.6230239781\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2433.1716909857255\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2430.7685581366854\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2590.424676956369\n",
      " Regularized Logistic Regression GD (200/3999): loss=2582.527937577414\n",
      " Regularized Logistic Regression GD (300/3999): loss=2575.368552847817\n",
      " Regularized Logistic Regression GD (400/3999): loss=2568.780904032628\n",
      " Regularized Logistic Regression GD (500/3999): loss=2562.644838541748\n",
      " Regularized Logistic Regression GD (600/3999): loss=2556.8724341414636\n",
      " Regularized Logistic Regression GD (700/3999): loss=2551.3986750448958\n",
      " Regularized Logistic Regression GD (800/3999): loss=2546.174884591899\n",
      " Regularized Logistic Regression GD (900/3999): loss=2541.16414591954\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2536.3381016797225\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2531.674697779127\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2527.1565742711314\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2522.769903667237\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2518.5035421836224\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2514.3484026440537\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2510.2969863804283\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2506.3430305713805\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2502.481240344471\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2498.7070837682304\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2495.016633948569\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2491.406446709208\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2487.873465360979\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Regularized Logistic Regression GD (2300/3999): loss=2484.4149462342357\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2481.0284002235217\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2477.711546746118\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2474.4622773703013\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2471.2786270052015\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2468.1587510245354\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2465.1009070594387\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2462.103440473617\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2459.1647727480354\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2456.283392166841\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2453.4578463256876\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2450.686736083066\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2447.968710655101\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2445.302463615667\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2442.6867296131663\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2440.1202816542404\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2437.601928835593\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2590.0469296126425\n",
      " Regularized Logistic Regression GD (200/3999): loss=2582.0236479948067\n",
      " Regularized Logistic Regression GD (300/3999): loss=2574.771041365635\n",
      " Regularized Logistic Regression GD (400/3999): loss=2568.1038950594448\n",
      " Regularized Logistic Regression GD (500/3999): loss=2561.895629331516\n",
      " Regularized Logistic Regression GD (600/3999): loss=2556.05511197546\n",
      " Regularized Logistic Regression GD (700/3999): loss=2550.5157229928836\n",
      " Regularized Logistic Regression GD (800/3999): loss=2545.2280448029624\n",
      " Regularized Logistic Regression GD (900/3999): loss=2540.1548749892177\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2535.2678018011784\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2530.544826089095\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2525.968687017015\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2521.525666340717\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2517.2047226658224\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2512.9968566130447\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2508.8946399229935\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2504.8918625622373\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2500.9832658509754\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2497.1643390298727\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2493.4311631054206\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2489.780290262009\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2486.208650255122\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2482.713477424414\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2479.29225356722\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2475.94266308022\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2472.662557636038\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2469.4499282999554\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2466.3028834712163\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2463.2196313957625\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2460.1984662736486\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2457.2377571965385\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2454.335939315108\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2451.4915067634884\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2448.7030069678194\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2445.9690360442723\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2443.288235053945\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2440.659286930372\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2438.080913934676\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2435.5518755237413\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2588.6364896758964\n",
      " Regularized Logistic Regression GD (200/3999): loss=2579.2932120083133\n",
      " Regularized Logistic Regression GD (300/3999): loss=2570.9708957737043\n",
      " Regularized Logistic Regression GD (400/3999): loss=2563.4508083992446\n",
      " Regularized Logistic Regression GD (500/3999): loss=2556.5683877867177\n",
      " Regularized Logistic Regression GD (600/3999): loss=2550.199533595295\n",
      " Regularized Logistic Regression GD (700/3999): loss=2544.2501395707327\n",
      " Regularized Logistic Regression GD (800/3999): loss=2538.6482616493345\n",
      " Regularized Logistic Regression GD (900/3999): loss=2533.3383842685753\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2528.2772521403367\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2523.4308357620125\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2518.772109429326\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2514.279411227917\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2509.9352218431545\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2505.725246997944\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2501.6377219176134\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2497.6628796372484\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2493.792541323187\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2490.0197982711315\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2486.338763375874\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2482.7443756717425\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2479.232245723492\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2475.79853268381\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2472.4398460603984\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2469.1531668810217\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2465.935784172224\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2462.7852435893665\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2459.6993057338796\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2456.675912226142\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2453.7131580114324\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2450.8092686926875\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2447.9625819296666\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2445.1715321368415\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2442.4346378635955\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2439.7504913603007\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2437.1177499289506\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2434.535128733217\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2432.001394803532\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2429.51536202219\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2587.9842389793544\n",
      " Regularized Logistic Regression GD (200/3999): loss=2578.3478809332823\n",
      " Regularized Logistic Regression GD (300/3999): loss=2569.895495219503\n",
      " Regularized Logistic Regression GD (400/3999): loss=2562.3337735959617\n",
      " Regularized Logistic Regression GD (500/3999): loss=2555.457569083128\n",
      " Regularized Logistic Regression GD (600/3999): loss=2549.120822431261\n",
      " Regularized Logistic Regression GD (700/3999): loss=2543.217612233534\n",
      " Regularized Logistic Regression GD (800/3999): loss=2537.669631914909\n",
      " Regularized Logistic Regression GD (900/3999): loss=2532.417937389236\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2527.4174343920618\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2522.6331219714884\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2518.0374786045704\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2513.608607047499\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2509.3288938568517\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2505.1840252113398\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2501.1622540594976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Regularized Logistic Regression GD (1700/3999): loss=2497.253847573885\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2493.4506659427707\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2489.7458381414126\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2486.133510193332\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2482.60864821326\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2479.1668832595387\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2475.8043883798114\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2472.517780644318\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2469.3040427137903\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2466.1604597782352\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2463.0845686603566\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2460.0741165967615\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2457.1270277539425\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2454.241375951863\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2451.415362387554\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2448.6472973990417\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2445.935585503285\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2443.2787130933084\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2440.6752383001062\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2438.123782619355\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2435.623023979354\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2433.1716909870356\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2430.7685581380415\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2590.4246769563742\n",
      " Regularized Logistic Regression GD (200/3999): loss=2582.5279375774303\n",
      " Regularized Logistic Regression GD (300/3999): loss=2575.3685528478572\n",
      " Regularized Logistic Regression GD (400/3999): loss=2568.780904032693\n",
      " Regularized Logistic Regression GD (500/3999): loss=2562.6448385418407\n",
      " Regularized Logistic Regression GD (600/3999): loss=2556.8724341415946\n",
      " Regularized Logistic Regression GD (700/3999): loss=2551.39867504508\n",
      " Regularized Logistic Regression GD (800/3999): loss=2546.1748845921247\n",
      " Regularized Logistic Regression GD (900/3999): loss=2541.16414591982\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2536.3381016800545\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2531.67469777953\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2527.156574271592\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2522.7699036677645\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2518.503542184212\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2514.348402644705\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2510.296986381162\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2506.3430305721836\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2502.481240345357\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2498.707083769206\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2495.016633949625\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2491.406446710365\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2487.873465362218\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2484.4149462355635\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2481.028400224941\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2477.7115467476433\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2474.4622773719148\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2471.278627006912\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2468.15875102636\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2465.100907061368\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2462.103440475664\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2459.164772750175\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2456.2833921691085\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2453.4578463280545\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2450.6867360855367\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2447.968710657689\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2445.302463618363\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2442.6867296159708\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2440.120281657166\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2437.6019288386456\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2590.0469296126485\n",
      " Regularized Logistic Regression GD (200/3999): loss=2582.02364799483\n",
      " Regularized Logistic Regression GD (300/3999): loss=2574.7710413656764\n",
      " Regularized Logistic Regression GD (400/3999): loss=2568.1038950595134\n",
      " Regularized Logistic Regression GD (500/3999): loss=2561.895629331612\n",
      " Regularized Logistic Regression GD (600/3999): loss=2556.0551119755955\n",
      " Regularized Logistic Regression GD (700/3999): loss=2550.5157229930596\n",
      " Regularized Logistic Regression GD (800/3999): loss=2545.228044803196\n",
      " Regularized Logistic Regression GD (900/3999): loss=2540.154874989498\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2535.2678018015163\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2530.544826089491\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2525.9686870174755\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2521.525666341245\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2517.204722666423\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2512.9968566137145\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2508.8946399237293\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2504.891862563064\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2500.9832658518794\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2497.164339030856\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2493.4311631064948\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2489.7802902631724\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2486.2086502563716\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2482.713477425761\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2479.2922535686616\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2475.9426630817634\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2472.662557637682\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2469.4499283017003\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2466.302883473065\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2463.2196313977206\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2460.1984662757095\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2457.2377571987013\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2454.335939317378\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2451.491506765889\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2448.70300697032\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2445.969036046899\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2443.2882350566692\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2440.6592869332108\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2438.0809139376315\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2435.5518755268154\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2588.6364896759014\n",
      " Regularized Logistic Regression GD (200/3999): loss=2579.2932120083356\n",
      " Regularized Logistic Regression GD (300/3999): loss=2570.9708957737516\n",
      " Regularized Logistic Regression GD (400/3999): loss=2563.4508083993214\n",
      " Regularized Logistic Regression GD (500/3999): loss=2556.5683877868346\n",
      " Regularized Logistic Regression GD (600/3999): loss=2550.19953359545\n",
      " Regularized Logistic Regression GD (700/3999): loss=2544.250139570931\n",
      " Regularized Logistic Regression GD (800/3999): loss=2538.6482616495823\n",
      " Regularized Logistic Regression GD (900/3999): loss=2533.3383842688813\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2528.2772521407137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Regularized Logistic Regression GD (1100/3999): loss=2523.4308357624386\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2518.7721094298176\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2514.279411228471\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2509.9352218437857\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2505.7252469986365\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2501.637721918378\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2497.6628796381083\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2493.7925413241164\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2490.019798272147\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2486.3387633769717\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2482.7443756729353\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2479.2322457247724\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2475.7985326851817\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2472.439846061868\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2469.1531668825874\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2465.935784173888\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2462.785243591131\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2459.699305735744\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2456.675912228107\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2453.713158013509\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2450.809268694869\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2447.962581931952\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2445.1715321392317\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2442.4346378661016\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2439.7504913629227\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2437.117749931681\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2434.5351287360672\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2432.0013948064925\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2429.515362025261\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2587.9842389793603\n",
      " Regularized Logistic Regression GD (200/3999): loss=2578.3478809333023\n",
      " Regularized Logistic Regression GD (300/3999): loss=2569.895495219551\n",
      " Regularized Logistic Regression GD (400/3999): loss=2562.333773596041\n",
      " Regularized Logistic Regression GD (500/3999): loss=2555.4575690832494\n",
      " Regularized Logistic Regression GD (600/3999): loss=2549.1208224314264\n",
      " Regularized Logistic Regression GD (700/3999): loss=2543.2176122337437\n",
      " Regularized Logistic Regression GD (800/3999): loss=2537.669631915158\n",
      " Regularized Logistic Regression GD (900/3999): loss=2532.4179373895204\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2527.4174343924356\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2522.633121971909\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2518.037478605055\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2513.6086070480455\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2509.328893857467\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2505.184025212027\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2501.162254060263\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2497.2538475747237\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2493.450665943698\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2489.745838142416\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2486.1335101944283\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2482.608648214434\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2479.1668832607998\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2475.8043883811642\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2472.517780645759\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2469.3040427153333\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2466.1604597798714\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2463.0845686620987\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2460.0741165985983\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2457.127027755872\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2454.2413759539\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2451.415362389692\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2448.647297401295\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2445.9355855056383\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2443.2787130957618\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2440.675238302676\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2438.123782622038\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2435.623023982145\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2433.1716909899346\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2430.768558141048\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2590.424676956386\n",
      " Regularized Logistic Regression GD (200/3999): loss=2582.5279375774767\n",
      " Regularized Logistic Regression GD (300/3999): loss=2575.368552847946\n",
      " Regularized Logistic Regression GD (400/3999): loss=2568.7809040328502\n",
      " Regularized Logistic Regression GD (500/3999): loss=2562.6448385420767\n",
      " Regularized Logistic Regression GD (600/3999): loss=2556.8724341419043\n",
      " Regularized Logistic Regression GD (700/3999): loss=2551.398675045473\n",
      " Regularized Logistic Regression GD (800/3999): loss=2546.174884592617\n",
      " Regularized Logistic Regression GD (900/3999): loss=2541.164145920436\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2536.3381016807875\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2531.6746977803823\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2527.1565742725834\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2522.769903668897\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2518.5035421855014\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2514.3484026461597\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2510.2969863827734\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2506.343030573989\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2502.4812403473247\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2498.7070837713536\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2495.0166339519683\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2491.406446712897\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2487.873465364959\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2484.414946238519\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2481.028400228096\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2477.711546751014\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2474.462277375509\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2471.2786270107235\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2468.1587510304053\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2465.100907065638\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2462.103440480158\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2459.164772754917\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2456.283392174086\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2453.4578463332723\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2450.6867360910096\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2447.9687106634087\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2445.302463624332\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2442.6867296222085\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2440.1202816636464\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2437.601928845385\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2590.0469296126635\n",
      " Regularized Logistic Regression GD (200/3999): loss=2582.023647994865\n",
      " Regularized Logistic Regression GD (300/3999): loss=2574.7710413657646\n",
      " Regularized Logistic Regression GD (400/3999): loss=2568.1038950596553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Regularized Logistic Regression GD (500/3999): loss=2561.895629331835\n",
      " Regularized Logistic Regression GD (600/3999): loss=2556.055111975908\n",
      " Regularized Logistic Regression GD (700/3999): loss=2550.515722993471\n",
      " Regularized Logistic Regression GD (800/3999): loss=2545.228044803704\n",
      " Regularized Logistic Regression GD (900/3999): loss=2540.154874990118\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2535.2678018022666\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2530.5448260903695\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2525.968687018486\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2521.5256663424084\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2517.2047226677355\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2512.9968566151915\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2508.8946399253678\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2504.8918625648716\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2500.9832658538708\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2497.1643390330482\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2493.431163108874\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2489.7802902657545\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2486.2086502591496\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2482.713477428743\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2479.2922535718576\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2475.942663085178\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2472.6625576413207\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2469.4499283055566\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2466.3028834771508\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2463.219631402038\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2460.198466280261\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2457.237757203498\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2454.335939322422\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2451.4915067711636\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2448.703006975855\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2445.969036052674\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2443.288235062704\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2440.6592869395013\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2438.0809139441794\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2435.5518755336175\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2588.636489675917\n",
      " Regularized Logistic Regression GD (200/3999): loss=2579.29321200839\n",
      " Regularized Logistic Regression GD (300/3999): loss=2570.9708957738567\n",
      " Regularized Logistic Regression GD (400/3999): loss=2563.450808399491\n",
      " Regularized Logistic Regression GD (500/3999): loss=2556.5683877870915\n",
      " Regularized Logistic Regression GD (600/3999): loss=2550.199533595786\n",
      " Regularized Logistic Regression GD (700/3999): loss=2544.2501395713803\n",
      " Regularized Logistic Regression GD (800/3999): loss=2538.648261650142\n",
      " Regularized Logistic Regression GD (900/3999): loss=2533.338384269556\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2528.2772521415113\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2523.43083576338\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2518.7721094308995\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2514.279411229703\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2509.9352218451754\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2505.72524700018\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2501.637721920102\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2497.662879639988\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2493.7925413261814\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2490.0197982744003\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2486.3387633794146\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2482.744375675566\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2479.2322457276055\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2475.7985326882176\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2472.43984606511\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2469.153166886047\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2465.9357841775663\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2462.785243595028\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2459.6993057398695\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2456.6759122324597\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2453.7131580180976\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2450.809268699686\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2447.9625819370112\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2445.17153214453\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2442.4346378716455\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2439.750491368709\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2437.11774993773\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2434.535128742344\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2432.0013948130345\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2429.515362032057\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2587.9842389793757\n",
      " Regularized Logistic Regression GD (200/3999): loss=2578.3478809333487\n",
      " Regularized Logistic Regression GD (300/3999): loss=2569.895495219649\n",
      " Regularized Logistic Regression GD (400/3999): loss=2562.33377359622\n",
      " Regularized Logistic Regression GD (500/3999): loss=2555.4575690835063\n",
      " Regularized Logistic Regression GD (600/3999): loss=2549.1208224317716\n",
      " Regularized Logistic Regression GD (700/3999): loss=2543.2176122342\n",
      " Regularized Logistic Regression GD (800/3999): loss=2537.669631915721\n",
      " Regularized Logistic Regression GD (900/3999): loss=2532.417937390206\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2527.4174343932455\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2522.63312197285\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2518.0374786061343\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2513.608607049272\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2509.3288938588416\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2505.184025213558\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2501.162254061959\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2497.2538475765996\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2493.450665945743\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2489.745838144634\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2486.1335101968307\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2482.608648217046\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2479.166883263601\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2475.804388384156\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2472.517780648961\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2469.3040427187375\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2466.1604597835003\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2463.084568665923\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2460.07411660266\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2457.1270277601566\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2454.2413759584056\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2451.4153623944294\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2448.6472974062594\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2445.9355855108292\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2443.278713101202\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2440.6752383083453\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2438.123782627946\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2435.623023988307\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2433.171690996332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Regularized Logistic Regression GD (3900/3999): loss=2430.7685581476962\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2590.4246769564097\n",
      " Regularized Logistic Regression GD (200/3999): loss=2582.5279375775704\n",
      " Regularized Logistic Regression GD (300/3999): loss=2575.368552848139\n",
      " Regularized Logistic Regression GD (400/3999): loss=2568.780904033176\n",
      " Regularized Logistic Regression GD (500/3999): loss=2562.6448385425656\n",
      " Regularized Logistic Regression GD (600/3999): loss=2556.8724341425905\n",
      " Regularized Logistic Regression GD (700/3999): loss=2551.398675046368\n",
      " Regularized Logistic Regression GD (800/3999): loss=2546.17488459373\n",
      " Regularized Logistic Regression GD (900/3999): loss=2541.1641459217935\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2536.3381016824187\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2531.6746977823045\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2527.1565742747953\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2522.7699036714403\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2518.503542188375\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2514.3484026493743\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2510.2969863863646\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2506.3430305779457\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2502.4812403516885\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2498.7070837761253\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2495.016633957162\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2491.406446718515\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2487.873465371015\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2484.414946245027\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2481.028400235073\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2477.7115467584617\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2474.462277383444\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2471.2786270191577\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2468.1587510393374\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2465.1009070750792\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2462.10344049013\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2459.1647727654026\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2456.2833921851093\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2453.457846344835\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2450.686736103111\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2447.9687106760584\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2445.3024636375385\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2442.6867296359683\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2440.1202816779846\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2437.6019288602774\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2590.0469296126835\n",
      " Regularized Logistic Regression GD (200/3999): loss=2582.0236479949635\n",
      " Regularized Logistic Regression GD (300/3999): loss=2574.7710413659756\n",
      " Regularized Logistic Regression GD (400/3999): loss=2568.1038950599936\n",
      " Regularized Logistic Regression GD (500/3999): loss=2561.8956293323367\n",
      " Regularized Logistic Regression GD (600/3999): loss=2556.0551119765823\n",
      " Regularized Logistic Regression GD (700/3999): loss=2550.5157229943625\n",
      " Regularized Logistic Regression GD (800/3999): loss=2545.2280448048214\n",
      " Regularized Logistic Regression GD (900/3999): loss=2540.1548749914905\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2535.2678018039096\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2530.5448260923026\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2525.968687020727\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2521.525666344975\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2517.2047226706395\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2512.996856618457\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2508.8946399289994\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2504.8918625688966\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2500.9832658582823\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2497.164339037874\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2493.431163114121\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2489.7802902714384\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2486.208650265284\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2482.7134774353376\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2479.292253578926\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2475.94266309272\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2472.6625576493534\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2469.4499283140926\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2466.3028834861966\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2463.2196314115995\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2460.19846629034\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2457.2377572141095\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2454.3359393335722\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2451.491506782861\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2448.703006988088\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2445.9690360654627\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2443.2882350760588\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2440.6592869534184\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2438.0809139586663\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2435.55187554868\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2588.636489675948\n",
      " Regularized Logistic Regression GD (200/3999): loss=2579.293212008505\n",
      " Regularized Logistic Regression GD (300/3999): loss=2570.9708957740845\n",
      " Regularized Logistic Regression GD (400/3999): loss=2563.450808399874\n",
      " Regularized Logistic Regression GD (500/3999): loss=2556.568387787659\n",
      " Regularized Logistic Regression GD (600/3999): loss=2550.199533596555\n",
      " Regularized Logistic Regression GD (700/3999): loss=2544.250139572381\n",
      " Regularized Logistic Regression GD (800/3999): loss=2538.6482616513786\n",
      " Regularized Logistic Regression GD (900/3999): loss=2533.338384271062\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2528.277252143299\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2523.430835765457\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2518.7721094332924\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2514.279411232421\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2509.935221848236\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2505.725247003599\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2501.6377219238834\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2497.662879644162\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2493.7925413307535\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2490.0197982793698\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2486.3387633848033\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2482.744375681396\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2479.232245733877\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2475.7985326949456\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2472.439846072298\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2469.153166893701\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2465.9357841857027\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2462.7852436036505\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2459.699305748995\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2456.675912242095\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2453.7131580282344\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2450.8092687103513\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2447.9625819482\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2445.1715321562583\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2442.43463788391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Regularized Logistic Regression GD (3500/3999): loss=2439.750491381521\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2437.1177499510823\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2434.5351287562635\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2432.0013948275055\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2429.515362047088\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2587.9842389794053\n",
      " Regularized Logistic Regression GD (200/3999): loss=2578.3478809334715\n",
      " Regularized Logistic Regression GD (300/3999): loss=2569.8954952198837\n",
      " Regularized Logistic Regression GD (400/3999): loss=2562.3337735966047\n",
      " Regularized Logistic Regression GD (500/3999): loss=2555.4575690840784\n",
      " Regularized Logistic Regression GD (600/3999): loss=2549.1208224325455\n",
      " Regularized Logistic Regression GD (700/3999): loss=2543.2176122351866\n",
      " Regularized Logistic Regression GD (800/3999): loss=2537.669631916953\n",
      " Regularized Logistic Regression GD (900/3999): loss=2532.4179373917173\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2527.41743439502\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2522.6331219749272\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2518.037478608507\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2513.6086070519764\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2509.328893861884\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2505.1840252169604\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2501.1622540657113\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2497.2538475807332\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2493.4506659502526\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2489.745838149565\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2486.133510202167\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2482.6086482227906\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2479.1668832697874\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2475.8043883907935\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2472.517780656041\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2469.3040427262913\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2466.1604597915116\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2463.0845686744237\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2460.0741166116263\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2457.127027769622\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2454.241375968375\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2451.415362404899\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2448.647297417236\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2445.9355855223343\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2443.2787131132263\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2440.6752383208996\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2438.1237826410406\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2435.623024001929\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2433.1716910104997\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2430.768558162412\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2590.424676956459\n",
      " Regularized Logistic Regression GD (200/3999): loss=2582.5279375777823\n",
      " Regularized Logistic Regression GD (300/3999): loss=2575.3685528485826\n",
      " Regularized Logistic Regression GD (400/3999): loss=2568.780904033907\n",
      " Regularized Logistic Regression GD (500/3999): loss=2562.6448385436493\n",
      " Regularized Logistic Regression GD (600/3999): loss=2556.8724341440748\n",
      " Regularized Logistic Regression GD (700/3999): loss=2551.3986750483123\n",
      " Regularized Logistic Regression GD (800/3999): loss=2546.174884596188\n",
      " Regularized Logistic Regression GD (900/3999): loss=2541.164145924799\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2536.3381016860144\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2531.674697786529\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2527.1565742796984\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2522.769903677038\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2518.5035421947155\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2514.348402656503\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2510.2969863942885\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2506.3430305867196\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2502.481240361335\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2498.707083786671\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2495.0166339686216\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2491.4064467309513\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2487.8734653844194\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2484.414946259442\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2481.0284002505127\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2477.711546774959\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2474.4622774010086\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2471.2786270378133\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2468.1587510590994\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2465.100907095976\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2462.1034405121623\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2459.1647727886057\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2456.2833922094865\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2453.4578463703956\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2450.686736129886\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2447.9687107040486\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2445.302463666764\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2442.6867296664304\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2440.1202817096896\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2437.601928893247\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2590.046929612747\n",
      " Regularized Logistic Regression GD (200/3999): loss=2582.0236479951745\n",
      " Regularized Logistic Regression GD (300/3999): loss=2574.771041366418\n",
      " Regularized Logistic Regression GD (400/3999): loss=2568.103895060747\n",
      " Regularized Logistic Regression GD (500/3999): loss=2561.8956293334377\n",
      " Regularized Logistic Regression GD (600/3999): loss=2556.0551119780944\n",
      " Regularized Logistic Regression GD (700/3999): loss=2550.515722996336\n",
      " Regularized Logistic Regression GD (800/3999): loss=2545.2280448073216\n",
      " Regularized Logistic Regression GD (900/3999): loss=2540.154874994537\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2535.2678018075453\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2530.5448260965863\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2525.9686870256933\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2521.5256663506516\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2517.204722677061\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2512.996856625667\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2508.8946399370443\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2504.891862577781\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2500.983265868054\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2497.1643390485506\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2493.4311631257515\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2489.7802902840212\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2486.2086502788643\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2482.713477449942\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2479.2922535945672\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2475.9426631094198\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2472.662557667141\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2469.449928332981\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2466.302883506203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Regularized Logistic Regression GD (2900/3999): loss=2463.2196314327507\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2460.1984663126473\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2457.237757237581\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2454.335939358225\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2451.4915068087153\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2448.7030070151636\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2445.9690360937702\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2443.2882351055964\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2440.6592869842\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2438.0809139907064\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2435.55187558199\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2588.636489676009\n",
      " Regularized Logistic Regression GD (200/3999): loss=2579.293212008747\n",
      " Regularized Logistic Regression GD (300/3999): loss=2570.9708957745956\n",
      " Regularized Logistic Regression GD (400/3999): loss=2563.450808400731\n",
      " Regularized Logistic Regression GD (500/3999): loss=2556.5683877889073\n",
      " Regularized Logistic Regression GD (600/3999): loss=2550.19953359825\n",
      " Regularized Logistic Regression GD (700/3999): loss=2544.250139574576\n",
      " Regularized Logistic Regression GD (800/3999): loss=2538.648261654126\n",
      " Regularized Logistic Regression GD (900/3999): loss=2533.3383842743856\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2528.277252147241\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2523.430835770049\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2518.7721094385806\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2514.2794112384404\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2509.935221855004\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2505.7252470111675\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2501.6377219322676\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2497.662879653393\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2493.7925413408493\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2490.0197982903796\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2486.338763396745\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2482.74437569429\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2479.2322457477467\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2475.7985327098113\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2472.4398460881926\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2469.1531669106384\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2465.9357842037107\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2462.785243622737\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2459.69930576918\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2456.6759122633944\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2453.7131580506816\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2450.8092687339454\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2447.9625819729613\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2445.1715321821916\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2442.4346379110393\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2439.7504914098527\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2437.1177499806313\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2434.5351287870367\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2432.0013948595197\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2429.5153620803567\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2587.9842389794776\n",
      " Regularized Logistic Regression GD (200/3999): loss=2578.347880933713\n",
      " Regularized Logistic Regression GD (300/3999): loss=2569.8954952204167\n",
      " Regularized Logistic Regression GD (400/3999): loss=2562.333773597465\n",
      " Regularized Logistic Regression GD (500/3999): loss=2555.457569085338\n",
      " Regularized Logistic Regression GD (600/3999): loss=2549.120822434258\n",
      " Regularized Logistic Regression GD (700/3999): loss=2543.2176122374017\n",
      " Regularized Logistic Regression GD (800/3999): loss=2537.669631919708\n",
      " Regularized Logistic Regression GD (900/3999): loss=2532.4179373950274\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2527.417434398946\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2522.6331219795043\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2518.0374786137813\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2513.6086070579545\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2509.3288938686046\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2505.1840252244565\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2501.162254074022\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2497.253847589865\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2493.4506659602525\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2489.7458381604524\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2486.133510213968\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2482.608648235525\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2479.1668832834766\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2475.8043884054655\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2472.5177806717097\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2469.3040427429783\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2466.1604598092363\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2463.084568693202\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2460.0741166314815\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2457.1270277905683\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2454.2413759904252\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2451.415362428068\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2448.6472974415506\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2445.935585547786\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2443.2787131398372\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2440.6752383486755\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2438.1237826699935\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2435.6230240320697\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2433.1716910418436\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2430.7685581949586\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2590.42467695659\n",
      " Regularized Logistic Regression GD (200/3999): loss=2582.527937578242\n",
      " Regularized Logistic Regression GD (300/3999): loss=2575.3685528495553\n",
      " Regularized Logistic Regression GD (400/3999): loss=2568.780904035529\n",
      " Regularized Logistic Regression GD (500/3999): loss=2562.6448385460512\n",
      " Regularized Logistic Regression GD (600/3999): loss=2556.8724341473853\n",
      " Regularized Logistic Regression GD (700/3999): loss=2551.398675052631\n",
      " Regularized Logistic Regression GD (800/3999): loss=2546.1748846016235\n",
      " Regularized Logistic Regression GD (900/3999): loss=2541.1641459314465\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2536.3381016939725\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2531.674697795873\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2527.1565742905264\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2522.7699036894337\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2518.5035422087503\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2514.3484026722554\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2510.296986411834\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2506.3430306061164\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2502.481240382664\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2498.7070838099976\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2495.016633993999\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2491.4064467584353\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2487.8734654140794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Regularized Logistic Regression GD (2300/3999): loss=2484.4149462913256\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2481.028400284677\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2477.7115468114416\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2474.462277439867\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2471.278627079086\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2468.1587511028333\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2465.1009071421977\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2462.10344056092\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2459.164772839927\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2456.2833922634127\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2453.457846426972\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2450.6867361891113\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2447.9687107659624\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2445.3024637313993\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2442.6867297338067\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2440.1202817798467\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2437.601928966194\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2590.046929612868\n",
      " Regularized Logistic Regression GD (200/3999): loss=2582.02364799563\n",
      " Regularized Logistic Regression GD (300/3999): loss=2574.771041367386\n",
      " Regularized Logistic Regression GD (400/3999): loss=2568.1038950623847\n",
      " Regularized Logistic Regression GD (500/3999): loss=2561.8956293358774\n",
      " Regularized Logistic Regression GD (600/3999): loss=2556.0551119814536\n",
      " Regularized Logistic Regression GD (700/3999): loss=2550.515723000711\n",
      " Regularized Logistic Regression GD (800/3999): loss=2545.2280448128276\n",
      " Regularized Logistic Regression GD (900/3999): loss=2540.1548750012676\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2535.2678018156003\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2530.5448261060624\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2525.968687036664\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2521.5256663631985\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2517.204722691292\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2512.9968566416237\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2508.8946399548104\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2504.8918625974375\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2500.9832658896617\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2497.1643390721724\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2493.431163151455\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2489.780290311867\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2486.2086503089204\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2482.7134774822234\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2479.292253629153\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2475.942663146366\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2472.6625577064806\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2469.449928374753\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2466.3028835504583\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2463.2196314795265\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2460.198466361982\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2457.237757289512\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2454.3359394127824\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2451.4915068659234\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2448.703007075054\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2445.9690361563726\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2443.2882351709454\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2440.659287052301\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2438.0809140615906\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2435.551875655673\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2588.636489676159\n",
      " Regularized Logistic Regression GD (200/3999): loss=2579.293212009277\n",
      " Regularized Logistic Regression GD (300/3999): loss=2570.970895775729\n",
      " Regularized Logistic Regression GD (400/3999): loss=2563.4508084026106\n",
      " Regularized Logistic Regression GD (500/3999): loss=2556.5683877916636\n",
      " Regularized Logistic Regression GD (600/3999): loss=2550.199533602012\n",
      " Regularized Logistic Regression GD (700/3999): loss=2544.25013957944\n",
      " Regularized Logistic Regression GD (800/3999): loss=2538.648261660182\n",
      " Regularized Logistic Regression GD (900/3999): loss=2533.3383842817307\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2528.277252155959\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2523.4308357802324\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2518.7721094502854\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2514.2794112517536\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2509.935221869999\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2505.7252470278977\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2501.637721950812\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2497.6628796738164\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2493.7925413632192\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2490.0197983147386\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2486.3387634231512\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2482.7443757228075\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2479.2322457784303\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2475.79853274271\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2472.4398461233545\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2469.153166948112\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2465.9357842435265\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2462.785243664956\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2459.6993058138332\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2456.6759123105176\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2453.7131581003155\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2450.8092687861295\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2447.962582027721\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2445.17153223957\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2442.434637971044\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2439.7504914725228\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2437.117750045995\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2434.5351288551215\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2432.00139493034\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2429.515362153936\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2587.984238979627\n",
      " Regularized Logistic Regression GD (200/3999): loss=2578.347880934286\n",
      " Regularized Logistic Regression GD (300/3999): loss=2569.8954952215768\n",
      " Regularized Logistic Regression GD (400/3999): loss=2562.3337735993805\n",
      " Regularized Logistic Regression GD (500/3999): loss=2555.457569088137\n",
      " Regularized Logistic Regression GD (600/3999): loss=2549.120822438043\n",
      " Regularized Logistic Regression GD (700/3999): loss=2543.21761224229\n",
      " Regularized Logistic Regression GD (800/3999): loss=2537.6696319257876\n",
      " Regularized Logistic Regression GD (900/3999): loss=2532.417937402387\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2527.4174344076605\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2522.6331219896433\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2518.0374786254383\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2513.6086070711926\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2509.3288938834958\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2505.184025241066\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2501.162254092399\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Regularized Logistic Regression GD (1700/3999): loss=2497.253847610101\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2493.4506659823837\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2489.7458381845413\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2486.133510240068\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2482.608648263684\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2479.1668833137674\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2475.8043884379113\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2472.517780706376\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2469.3040427798883\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2466.160459848445\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2463.0845687347564\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2460.0741166754024\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2457.127027836909\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2454.241376039206\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2451.4153624793257\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2448.647297495311\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2445.935585604085\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2443.278713198695\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2440.6752384101214\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2438.1237827340483\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2435.623024098762\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2433.1716911111753\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2430.7685582669583\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2590.424676956851\n",
      " Regularized Logistic Regression GD (200/3999): loss=2582.52793757925\n",
      " Regularized Logistic Regression GD (300/3999): loss=2575.368552851682\n",
      " Regularized Logistic Regression GD (400/3999): loss=2568.7809040391144\n",
      " Regularized Logistic Regression GD (500/3999): loss=2562.6448385513813\n",
      " Regularized Logistic Regression GD (600/3999): loss=2556.8724341546986\n",
      " Regularized Logistic Regression GD (700/3999): loss=2551.3986750621893\n",
      " Regularized Logistic Regression GD (800/3999): loss=2546.1748846136547\n",
      " Regularized Logistic Regression GD (900/3999): loss=2541.1641459461553\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2536.338101711562\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2531.674697816556\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2527.156574314485\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2522.7699037168422\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2518.503542239798\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2514.3484027070963\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2510.2969864506445\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2506.3430306490395\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2502.4812404298477\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2498.707083861598\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2495.016634050138\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2491.406446819252\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2487.873465479698\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2484.414946361866\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2481.0284003602465\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2477.711546892157\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2474.462277525819\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2471.2786271703844\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2468.1587511995526\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2465.10090724445\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2462.103440668778\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2459.164772953474\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2456.2833923827175\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2453.4578465520985\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2450.6867363201372\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2447.9687109029533\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2445.3024638744055\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2442.686729882875\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2440.1202819350374\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2437.601929127553\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2590.0469296131446\n",
      " Regularized Logistic Regression GD (200/3999): loss=2582.023647996665\n",
      " Regularized Logistic Regression GD (300/3999): loss=2574.771041369558\n",
      " Regularized Logistic Regression GD (400/3999): loss=2568.103895066011\n",
      " Regularized Logistic Regression GD (500/3999): loss=2561.8956293412625\n",
      " Regularized Logistic Regression GD (600/3999): loss=2556.0551119888632\n",
      " Regularized Logistic Regression GD (700/3999): loss=2550.5157230103946\n",
      " Regularized Logistic Regression GD (800/3999): loss=2545.2280448250112\n",
      " Regularized Logistic Regression GD (900/3999): loss=2540.1548750161714\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2535.267801833431\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2530.5448261270017\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2525.9686870609326\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2521.525666390979\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2517.2047227227345\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2512.9968566769267\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2508.8946399941306\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2504.8918626409186\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2500.983265937479\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2497.1643391244393\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2493.431163208327\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2489.7802903734732\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2486.208650375377\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2482.7134775536774\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2479.2922537056797\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2475.942663228094\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2472.662557793506\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2469.449928467177\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2466.3028836483713\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2463.2196315830147\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2460.1984664711304\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2457.237757404381\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2454.335939533468\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2451.4915069924855\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2448.7030072075468\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2445.969036294866\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2443.2882353154823\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2440.6592872029564\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2438.0809142184057\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2435.551875818692\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2588.6364896764926\n",
      " Regularized Logistic Regression GD (200/3999): loss=2579.2932120104874\n",
      " Regularized Logistic Regression GD (300/3999): loss=2570.9708957782336\n",
      " Regularized Logistic Regression GD (400/3999): loss=2563.4508084067634\n",
      " Regularized Logistic Regression GD (500/3999): loss=2556.568387797764\n",
      " Regularized Logistic Regression GD (600/3999): loss=2550.1995336103187\n",
      " Regularized Logistic Regression GD (700/3999): loss=2544.250139590187\n",
      " Regularized Logistic Regression GD (800/3999): loss=2538.6482616735825\n",
      " Regularized Logistic Regression GD (900/3999): loss=2533.3383842979906\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2528.2772521752486\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Regularized Logistic Regression GD (1100/3999): loss=2523.430835802737\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2518.7721094761846\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2514.279411281194\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2509.9352219031434\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2505.725247064907\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2501.637721991829\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2497.6628797189896\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2493.792541412678\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2490.0197983686157\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2486.338763481587\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2482.7443757859032\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2479.232245846302\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2475.7985328154805\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2472.4398462011345\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2469.153167031\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2465.9357843316197\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2462.785243758353\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2459.6993059126135\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2456.6759124147884\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2453.713158210127\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2450.809268901566\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2447.9625821488708\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2445.171532366485\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2442.4346381038076\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2439.7504916111798\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2437.1177501906122\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2434.535129005743\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2432.001395087015\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2429.515362316712\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2587.984238979974\n",
      " Regularized Logistic Regression GD (200/3999): loss=2578.3478809355283\n",
      " Regularized Logistic Regression GD (300/3999): loss=2569.895495224149\n",
      " Regularized Logistic Regression GD (400/3999): loss=2562.33377360362\n",
      " Regularized Logistic Regression GD (500/3999): loss=2555.4575690943166\n",
      " Regularized Logistic Regression GD (600/3999): loss=2549.1208224464363\n",
      " Regularized Logistic Regression GD (700/3999): loss=2543.2176122530955\n",
      " Regularized Logistic Regression GD (800/3999): loss=2537.669631939218\n",
      " Regularized Logistic Regression GD (900/3999): loss=2532.417937418644\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2527.417434426916\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2522.6331220120846\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2518.037478651204\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2513.6086071004665\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2509.3288939164227\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2505.184025277778\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2501.162254133069\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2497.2538476548348\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2493.4506660313345\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2489.745838237816\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2486.133510297805\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2482.608648326007\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2479.166883380748\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2475.804388509695\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2472.5177807830464\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2469.3040428615627\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2466.160459935196\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2463.084568826683\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2460.0741167725823\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2457.127027939411\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2454.2413761471225\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2451.415362592723\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2448.6472976142504\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2445.935585728635\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2443.278713328903\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2440.675238546045\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2438.1237828757367\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2435.6230242462675\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2433.1716912645543\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2430.7685584262367\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2590.424676957458\n",
      " Regularized Logistic Regression GD (200/3999): loss=2582.527937581484\n",
      " Regularized Logistic Regression GD (300/3999): loss=2575.368552856395\n",
      " Regularized Logistic Regression GD (400/3999): loss=2568.780904047034\n",
      " Regularized Logistic Regression GD (500/3999): loss=2562.644838563137\n",
      " Regularized Logistic Regression GD (600/3999): loss=2556.8724341708885\n",
      " Regularized Logistic Regression GD (700/3999): loss=2551.398675083327\n",
      " Regularized Logistic Regression GD (800/3999): loss=2546.1748846402543\n",
      " Regularized Logistic Regression GD (900/3999): loss=2541.1641459786883\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2536.3381017504926\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2531.674697862317\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2527.156574367485\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2522.769903777498\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2518.5035423084882\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2514.348402784189\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2510.296986536485\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2506.3430307440008\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2502.481240534245\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2498.7070839757334\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2495.016634174342\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2491.406446953787\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2487.8734656248535\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2484.4149465179003\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2481.0284005274148\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2477.711547070701\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2474.4622777159707\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2471.278627372348\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2468.1587514135554\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2465.1009074706576\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2462.1034409073854\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2459.164773204657\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2456.2833926466355\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2453.4578468289096\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2450.6867366099937\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2447.9687112059823\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2445.3024641907377\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2442.686730212648\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2440.120282278341\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2437.6019294845123\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2590.046929613767\n",
      " Regularized Logistic Regression GD (200/3999): loss=2582.023647998943\n",
      " Regularized Logistic Regression GD (300/3999): loss=2574.771041374352\n",
      " Regularized Logistic Regression GD (400/3999): loss=2568.1038950740653\n",
      " Regularized Logistic Regression GD (500/3999): loss=2561.8956293531915\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Regularized Logistic Regression GD (600/3999): loss=2556.0551120052646\n",
      " Regularized Logistic Regression GD (700/3999): loss=2550.5157230318173\n",
      " Regularized Logistic Regression GD (800/3999): loss=2545.2280448519523\n",
      " Regularized Logistic Regression GD (900/3999): loss=2540.1548750491384\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2535.2678018728643\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2530.5448261733577\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2525.9686871146223\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2521.52566645242\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2517.204722792321\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2512.9968567550277\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2508.894640081109\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2504.89186273712\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2500.983266043231\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2497.1643392400915\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2493.4311633341367\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2489.780290509757\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2486.2086505224042\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2482.7134777117144\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2479.292253874984\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2475.9426634088964\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2472.6625579860242\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2469.4499286716436\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2466.3028838649766\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2463.2196318119486\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2460.1984667125735\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2457.237757658522\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2454.335939800439\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2451.4915072724543\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2448.7030075006573\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2445.969036601246\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2443.288235635256\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2440.6592875362394\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2438.080914565308\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2435.551876179317\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2588.636489677207\n",
      " Regularized Logistic Regression GD (200/3999): loss=2579.2932120131354\n",
      " Regularized Logistic Regression GD (300/3999): loss=2570.9708957837747\n",
      " Regularized Logistic Regression GD (400/3999): loss=2563.450808415953\n",
      " Regularized Logistic Regression GD (500/3999): loss=2556.56838781128\n",
      " Regularized Logistic Regression GD (600/3999): loss=2550.1995336287205\n",
      " Regularized Logistic Regression GD (700/3999): loss=2544.2501396139805\n",
      " Regularized Logistic Regression GD (800/3999): loss=2538.648261703236\n",
      " Regularized Logistic Regression GD (900/3999): loss=2533.3383843339398\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2528.2772522179307\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2523.430835852537\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2518.7721095334714\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2514.2794113463387\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2509.93522197649\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2505.725247146806\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2501.6377220825875\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2497.6628798189276\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2493.7925415220966\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2490.0197984878178\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2486.338763610825\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2482.7443759254766\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2479.2322459964735\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2475.798532976481\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2472.439846373205\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2469.153167214359\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2465.935784526508\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2462.785243964961\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2459.699306131147\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2456.6759126454244\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2453.713158453064\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2450.809269156955\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2447.9625824168725\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2445.171532647264\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2442.434638397494\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2439.7504919179137\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2437.1177505105065\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2434.5351293389376\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2432.0013954336164\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2429.515362676818\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2587.984238980737\n",
      " Regularized Logistic Regression GD (200/3999): loss=2578.3478809382837\n",
      " Regularized Logistic Regression GD (300/3999): loss=2569.895495229823\n",
      " Regularized Logistic Regression GD (400/3999): loss=2562.3337736129747\n",
      " Regularized Logistic Regression GD (500/3999): loss=2555.4575691080067\n",
      " Regularized Logistic Regression GD (600/3999): loss=2549.120822464991\n",
      " Regularized Logistic Regression GD (700/3999): loss=2543.217612277\n",
      " Regularized Logistic Regression GD (800/3999): loss=2537.6696319689477\n",
      " Regularized Logistic Regression GD (900/3999): loss=2532.417937454616\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2527.417434469527\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2522.633122061727\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2518.037478708241\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2513.608607165232\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2509.32889398926\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2505.184025359032\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2501.16225422303\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2497.253847753816\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2493.4506661396213\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2489.7458383556973\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2486.1335104255304\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2482.60864846385\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2479.166883528963\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2475.804388668511\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2472.5177809526795\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2469.304043042222\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2466.160460127107\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2463.084569030031\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2460.074116987561\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2457.1270281661955\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2454.2413763858526\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2451.415362843575\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2448.647297877371\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2445.9355860041487\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2443.2787136169504\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2440.6752388467435\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2438.1237831892117\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2435.6230245725974\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2433.1716916038536\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2430.768558778588\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Regularized Logistic Regression GD (100/3999): loss=2590.424676958793\n",
      " Regularized Logistic Regression GD (200/3999): loss=2582.527937586437\n",
      " Regularized Logistic Regression GD (300/3999): loss=2575.3685528668466\n",
      " Regularized Logistic Regression GD (400/3999): loss=2568.7809040645598\n",
      " Regularized Logistic Regression GD (500/3999): loss=2562.6448385891617\n",
      " Regularized Logistic Regression GD (600/3999): loss=2556.8724342067003\n",
      " Regularized Logistic Regression GD (700/3999): loss=2551.3986751301118\n",
      " Regularized Logistic Regression GD (800/3999): loss=2546.174884699101\n",
      " Regularized Logistic Regression GD (900/3999): loss=2541.164146050674\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2536.338101836605\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2531.6746979635386\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2527.156574484733\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2522.7699039116537\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2518.503542460419\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2514.3484029547185\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2510.296986726402\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2506.3430309540495\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2502.4812407651802\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2498.7070842282383\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2495.016634449086\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2491.406447251423\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2487.8734659459733\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2484.4149468631026\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2481.0284008972444\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2477.7115474656885\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2474.4622781366197\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2471.278627819149\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2468.1587518869314\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2465.1009079710707\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2462.1034414352416\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2459.164773760337\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2456.2833932304934\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2453.4578474412883\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2450.68673725122\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2447.9687118763636\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2445.302464890557\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2442.686730942166\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2440.1202830378134\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2437.6019302741724\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2590.0469296151496\n",
      " Regularized Logistic Regression GD (200/3999): loss=2582.023648004001\n",
      " Regularized Logistic Regression GD (300/3999): loss=2574.771041384966\n",
      " Regularized Logistic Regression GD (400/3999): loss=2568.1038950918464\n",
      " Regularized Logistic Regression GD (500/3999): loss=2561.8956293795827\n",
      " Regularized Logistic Regression GD (600/3999): loss=2556.0551120415544\n",
      " Regularized Logistic Regression GD (700/3999): loss=2550.515723079207\n",
      " Regularized Logistic Regression GD (800/3999): loss=2545.2280449115688\n",
      " Regularized Logistic Regression GD (900/3999): loss=2540.154875122046\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2535.2678019601076\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2530.5448262758982\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2525.9686872333987\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2521.5256665883408\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2517.2047229462455\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2512.996856927798\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2508.8946402735082\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2504.8918629499426\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2500.983266277194\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2497.1643394958855\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2493.4311636124676\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2489.780290811237\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2486.2086508476586\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2482.713478061333\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2479.292254249506\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2475.942663808858\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2472.6625584119274\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2469.449929123955\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2466.302884344144\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2463.219632318396\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2460.1984672467074\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2457.2377582207096\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2454.3359403910517\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2451.4915078918084\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2448.7030081490825\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2445.9690372790105\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2443.28823634267\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2440.65928827354\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2438.08091533273\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2435.551876977091\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2588.6364896787977\n",
      " Regularized Logistic Regression GD (200/3999): loss=2579.293212019008\n",
      " Regularized Logistic Regression GD (300/3999): loss=2570.97089579602\n",
      " Regularized Logistic Regression GD (400/3999): loss=2563.4508084363138\n",
      " Regularized Logistic Regression GD (500/3999): loss=2556.568387841162\n",
      " Regularized Logistic Regression GD (600/3999): loss=2550.1995336694044\n",
      " Regularized Logistic Regression GD (700/3999): loss=2544.250139666599\n",
      " Regularized Logistic Regression GD (800/3999): loss=2538.648261768828\n",
      " Regularized Logistic Regression GD (900/3999): loss=2533.338384413489\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2528.2772523123454\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2523.4308359626907\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2518.772109660197\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2514.279411490439\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2509.9352221387535\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2505.7252473279545\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2501.6377222833635\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2497.662880040006\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2493.7925417641677\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2490.019798751502\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2486.3387638967674\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2482.7443762342587\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2479.2322463286573\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2475.798533332636\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2472.4398467538495\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2469.1531676200184\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2465.935784957629\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2462.7852444220243\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2459.6993066145847\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2456.6759131556564\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2453.7131589904716\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2450.809269721937\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2447.962583009759\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2445.17153326839\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2442.434639047186\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2439.7504925964677\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2437.1177512182103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Regularized Logistic Regression GD (3700/3999): loss=2434.535130076032\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2432.0013962003663\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2429.5153634734456\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2587.9842389824125\n",
      " Regularized Logistic Regression GD (200/3999): loss=2578.347880944373\n",
      " Regularized Logistic Regression GD (300/3999): loss=2569.8954952424065\n",
      " Regularized Logistic Regression GD (400/3999): loss=2562.333773633714\n",
      " Regularized Logistic Regression GD (500/3999): loss=2555.4575691382684\n",
      " Regularized Logistic Regression GD (600/3999): loss=2549.1208225060186\n",
      " Regularized Logistic Regression GD (700/3999): loss=2543.2176123299027\n",
      " Regularized Logistic Regression GD (800/3999): loss=2537.669632034723\n",
      " Regularized Logistic Regression GD (900/3999): loss=2532.4179375341923\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2527.4174345638007\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2522.6331221715322\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2518.03747883439\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2513.6086073085007\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2509.3288941504065\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2505.184025538764\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2501.162254422039\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2497.253847972775\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2493.450666379169\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2489.7458386164594\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2486.133510708096\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2482.6086487687944\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2479.1668838568257\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2475.8043890198196\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2472.517781327933\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2469.3040434419004\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2466.160460551668\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2463.0845694798936\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2460.0741174631216\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2457.127028667849\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2454.2413769139857\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2451.415363398509\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2448.6472984594334\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2445.9355866136525\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2443.2787142541843\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2440.6752395119593\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2438.1237838826655\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2435.6230252945306\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2433.1716923544623\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2430.768559558083\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2590.4246769617343\n",
      " Regularized Logistic Regression GD (200/3999): loss=2582.527937597386\n",
      " Regularized Logistic Regression GD (300/3999): loss=2575.368552889933\n",
      " Regularized Logistic Regression GD (400/3999): loss=2568.7809041033356\n",
      " Regularized Logistic Regression GD (500/3999): loss=2562.6448386467423\n",
      " Regularized Logistic Regression GD (600/3999): loss=2556.8724342858973\n",
      " Regularized Logistic Regression GD (700/3999): loss=2551.39867523357\n",
      " Regularized Logistic Regression GD (800/3999): loss=2546.1748848292764\n",
      " Regularized Logistic Regression GD (900/3999): loss=2541.1641462099\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2536.338102027125\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2531.6746981874535\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2527.1565747441064\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2522.7699042084514\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2518.503542796528\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2514.348403331955\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2510.2969871465402\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2506.343031418761\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2502.481241276056\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2498.7070847868385\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2495.0166350568797\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2491.406447909824\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2487.87346665637\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2484.4149476267457\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2481.0284017153895\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2477.711548339485\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2474.462279067195\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2471.2786288075545\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2468.1587529341846\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2465.10090907811\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2462.1034426029573\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2459.164774989597\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2456.2833945220927\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2453.4578487960016\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2450.6867386697477\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2447.9687133593766\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2445.3024664386885\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2442.6867325560133\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2440.1202847179334\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2437.601932021079\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2590.0469296181723\n",
      " Regularized Logistic Regression GD (200/3999): loss=2582.0236480151643\n",
      " Regularized Logistic Regression GD (300/3999): loss=2574.7710414084418\n",
      " Regularized Logistic Regression GD (400/3999): loss=2568.103895131199\n",
      " Regularized Logistic Regression GD (500/3999): loss=2561.8956294379627\n",
      " Regularized Logistic Regression GD (600/3999): loss=2556.0551121218314\n",
      " Regularized Logistic Regression GD (700/3999): loss=2550.5157231840317\n",
      " Regularized Logistic Regression GD (800/3999): loss=2545.22804504345\n",
      " Regularized Logistic Regression GD (900/3999): loss=2540.1548752833496\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2535.2678021530933\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2530.544826502736\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2525.968687496161\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2521.5256668890274\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2517.204723286772\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2512.9968573100127\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2508.8946406991777\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2504.8918634207444\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2500.9832667947735\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2497.1643400617954\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2493.4311642281878\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2489.780291478196\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2486.208651567208\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2482.713478834747\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2479.2922550780368\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2475.9426646936595\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2472.662559354108\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2469.4499301245573\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2466.302885404163\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2463.219633438778\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2460.1984684283316\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2457.237759464408\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Regularized Logistic Regression GD (3200/3999): loss=2454.335941697605\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2451.4915092619594\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2448.703009583527\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2445.969038778396\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2443.2882379076045\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2440.6592899045972\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2438.080917030439\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2435.551878741945\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2588.6364896823397\n",
      " Regularized Logistic Regression GD (200/3999): loss=2579.2932120319965\n",
      " Regularized Logistic Regression GD (300/3999): loss=2570.970895823131\n",
      " Regularized Logistic Regression GD (400/3999): loss=2563.450808481317\n",
      " Regularized Logistic Regression GD (500/3999): loss=2556.5683879072594\n",
      " Regularized Logistic Regression GD (600/3999): loss=2550.1995337594003\n",
      " Regularized Logistic Regression GD (700/3999): loss=2544.2501397829974\n",
      " Regularized Logistic Regression GD (800/3999): loss=2538.6482619139497\n",
      " Regularized Logistic Regression GD (900/3999): loss=2533.338384589466\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2528.2772525212004\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2523.4308362063757\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2518.772109940543\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2514.2794118092315\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2509.935222497701\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2505.725247728703\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2501.637722727505\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2497.6628805290848\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2493.7925422996386\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2490.019799334831\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2486.3387645293087\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2482.7443769173196\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2479.2322470635363\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2475.798534120532\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2472.4398475959374\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2469.153168517402\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2465.9357859113875\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2462.7852454331587\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2459.6993076840636\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2456.6759142843966\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2453.7131601793526\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2450.809270971783\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2447.962584321362\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2445.1715346424926\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2442.4346404844655\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2439.750494097595\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2437.117752783798\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2434.5351317066606\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2432.001397896574\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2429.515365235753\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2587.9842389861337\n",
      " Regularized Logistic Regression GD (200/3999): loss=2578.3478809578446\n",
      " Regularized Logistic Regression GD (300/3999): loss=2569.8954952702243\n",
      " Regularized Logistic Regression GD (400/3999): loss=2562.333773679564\n",
      " Regularized Logistic Regression GD (500/3999): loss=2555.457569205242\n",
      " Regularized Logistic Regression GD (600/3999): loss=2549.1208225968057\n",
      " Regularized Logistic Regression GD (700/3999): loss=2543.217612446935\n",
      " Regularized Logistic Regression GD (800/3999): loss=2537.669632180219\n",
      " Regularized Logistic Regression GD (900/3999): loss=2532.417937710224\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2527.417434772347\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2522.6331224144524\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2518.0374791134736\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2513.608607625459\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2509.3288945068907\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2505.18402593637\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2501.162254862298\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2497.253848457161\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2493.4506669091197\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2489.74583919332\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2486.1335113332148\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2482.608649443396\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2479.1668845821478\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2475.8043897969987\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2472.5177821580833\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2469.3040443260784\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2466.1604614908597\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2463.0845704750586\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2460.074118515187\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2457.1270297776496\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2454.241378082315\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2451.4153646261466\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2448.647299747095\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2445.9355879620116\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2443.2787156638574\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2440.6752409835653\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2438.1237854167352\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2435.6230268915738\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2433.1716940149627\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2430.7685612824826\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2590.424676968239\n",
      " Regularized Logistic Regression GD (200/3999): loss=2582.52793762161\n",
      " Regularized Logistic Regression GD (300/3999): loss=2575.368552941034\n",
      " Regularized Logistic Regression GD (400/3999): loss=2568.780904189114\n",
      " Regularized Logistic Regression GD (500/3999): loss=2562.644838774123\n",
      " Regularized Logistic Regression GD (600/3999): loss=2556.872434461127\n",
      " Regularized Logistic Regression GD (700/3999): loss=2551.3986754624525\n",
      " Regularized Logistic Regression GD (800/3999): loss=2546.174885117268\n",
      " Regularized Logistic Regression GD (900/3999): loss=2541.1641465621683\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2536.3381024485902\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2531.67469868282\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2527.1565753179134\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2522.7699048650416\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2518.5035435400946\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2514.348404166535\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2510.2969880759697\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2506.3430324467777\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2502.4812424062166\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2498.707086022568\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2495.0166364014567\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2491.4064493663836\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2487.873468227892\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2484.4149493160976\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2481.028403525277\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2477.711550272525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Regularized Logistic Regression GD (2600/3999): loss=2474.4622811258296\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2471.278630994124\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2468.15875525092\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2465.100911527114\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2462.1034451862206\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2459.164777708996\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2456.2833973794136\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2453.45785179292\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2450.6867418078436\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2447.968716640141\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2445.302469863507\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2442.6867361262075\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2440.120288434728\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2437.6019358856183\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2590.046929624896\n",
      " Regularized Logistic Regression GD (200/3999): loss=2582.0236480398776\n",
      " Regularized Logistic Regression GD (300/3999): loss=2574.7710414603794\n",
      " Regularized Logistic Regression GD (400/3999): loss=2568.103895218251\n",
      " Regularized Logistic Regression GD (500/3999): loss=2561.895629567102\n",
      " Regularized Logistic Regression GD (600/3999): loss=2556.0551122994143\n",
      " Regularized Logistic Regression GD (700/3999): loss=2550.5157234159287\n",
      " Regularized Logistic Regression GD (800/3999): loss=2545.2280453351987\n",
      " Regularized Logistic Regression GD (900/3999): loss=2540.1548756401926\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2535.2678025800283\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2530.544827004551\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2525.9686880774557\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2521.5256675542023\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2517.204724040084\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2512.99685815553\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2508.894641640817\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2504.891864462263\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2500.983267939757\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2497.1643413136926\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2493.431165590282\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2489.7802929536365\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2486.2086531589903\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2482.713480545731\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2479.2922569109332\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2475.9426666510317\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2472.6625614384147\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2469.4499323381215\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2466.3028877491734\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2463.219635917302\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2460.1984710423326\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2457.23776221573\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2454.335944587994\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2451.491512293039\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2448.7030127568205\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2445.9690420953707\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2443.28824136958\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2440.659293512844\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2438.08092078614\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2435.551882646187\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2588.636489690128\n",
      " Regularized Logistic Regression GD (200/3999): loss=2579.293212060751\n",
      " Regularized Logistic Regression GD (300/3999): loss=2570.9708958831043\n",
      " Regularized Logistic Regression GD (400/3999): loss=2563.4508085808748\n",
      " Regularized Logistic Regression GD (500/3999): loss=2556.5683880535094\n",
      " Regularized Logistic Regression GD (600/3999): loss=2550.1995339584983\n",
      " Regularized Logistic Regression GD (700/3999): loss=2544.2501400405113\n",
      " Regularized Logistic Regression GD (800/3999): loss=2538.6482622349763\n",
      " Regularized Logistic Regression GD (900/3999): loss=2533.3383849787656\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2528.2772529832523\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2523.430836745449\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2518.772110560746\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2514.27941251448\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2509.9352232917686\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2505.7252486152515\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2501.6377237100555\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2497.662881611029\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2493.7925434842605\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2490.0198006252776\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2486.3387659286204\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2482.744378428441\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2479.232248689254\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2475.7985358635447\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2472.439849458808\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2469.153170502611\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2465.935788021277\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2462.785247669984\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2459.6993100499667\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2456.675916781421\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2453.7131628094116\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2450.809273736714\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2447.9625872228935\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2445.1715376822604\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2442.4346436640267\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2439.750497418395\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2437.1177562472053\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2434.535135313967\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2432.001401648974\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2429.515369134362\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2587.984238994348\n",
      " Regularized Logistic Regression GD (200/3999): loss=2578.3478809876465\n",
      " Regularized Logistic Regression GD (300/3999): loss=2569.895495331766\n",
      " Regularized Logistic Regression GD (400/3999): loss=2562.3337737809843\n",
      " Regularized Logistic Regression GD (500/3999): loss=2555.4575693533816\n",
      " Regularized Logistic Regression GD (600/3999): loss=2549.1208227976313\n",
      " Regularized Logistic Regression GD (700/3999): loss=2543.217612705809\n",
      " Regularized Logistic Regression GD (800/3999): loss=2537.6696325020725\n",
      " Regularized Logistic Regression GD (900/3999): loss=2532.41793809966\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2527.4174352336854\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2522.633122951839\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2518.0374797308514\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2513.608608326635\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2509.3288952955004\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2505.184026815947\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2501.1622558362415\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2497.253849528734\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2493.450668081462\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2489.7458404694753\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2486.133512716078\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2482.608650935767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Regularized Logistic Regression GD (2200/3999): loss=2479.166886186689\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2475.8043915162843\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2472.5177839945563\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2469.3040462820586\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2466.160463568579\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2463.0845726766042\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2460.0741208425607\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2457.1270322327405\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2454.24138066693\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2451.415367341957\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2448.6473025956816\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2445.9355909448677\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2443.2787187823888\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2440.6752442390703\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2438.123788810429\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2435.6230304245983\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2433.171697688363\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2430.7685650972285\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2590.4246769826445\n",
      " Regularized Logistic Regression GD (200/3999): loss=2582.5279376751955\n",
      " Regularized Logistic Regression GD (300/3999): loss=2575.368553054062\n",
      " Regularized Logistic Regression GD (400/3999): loss=2568.780904378888\n",
      " Regularized Logistic Regression GD (500/3999): loss=2562.644839055883\n",
      " Regularized Logistic Regression GD (600/3999): loss=2556.872434848763\n",
      " Regularized Logistic Regression GD (700/3999): loss=2551.398675968799\n",
      " Regularized Logistic Regression GD (800/3999): loss=2546.1748857543726\n",
      " Regularized Logistic Regression GD (900/3999): loss=2541.1641473414475\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2536.338103380927\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2531.674699778673\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2527.1565765872833\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2522.769906317562\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2518.503545185028\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2514.3484060127626\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2510.296990132068\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2506.343034720955\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2502.4812449063884\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2498.707088756289\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2495.0166393759714\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2491.4064525886097\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2487.873471704442\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2484.4149530533036\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2481.0284075291597\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2477.711554548813\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2474.462285679972\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2471.2786358312997\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2468.1587603760463\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2465.1009169448294\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2462.1034509009505\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2459.164783724891\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2456.2834037004186\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2453.4578584227456\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2450.686748749979\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2447.968723897876\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2445.3024774399446\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2442.686744024235\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2440.1202966570722\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2437.601944434819\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2590.0469296397346\n",
      " Regularized Logistic Regression GD (200/3999): loss=2582.023648094532\n",
      " Regularized Logistic Regression GD (300/3999): loss=2574.771041575279\n",
      " Regularized Logistic Regression GD (400/3999): loss=2568.1038954108335\n",
      " Regularized Logistic Regression GD (500/3999): loss=2561.895629852801\n",
      " Regularized Logistic Regression GD (600/3999): loss=2556.055112692252\n",
      " Regularized Logistic Regression GD (700/3999): loss=2550.5157239289465\n",
      " Regularized Logistic Regression GD (800/3999): loss=2545.2280459806107\n",
      " Regularized Logistic Regression GD (900/3999): loss=2540.154876429607\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2535.267803524497\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2530.5448281146773\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2525.9686893634075\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2521.5256690257415\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2517.2047257065847\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2512.9968600260077\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2508.894643723932\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2504.8918667663265\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2500.9832704727014\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2497.1643440831604\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2493.431168603538\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2489.780296217645\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2486.208656680373\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2482.7134843307917\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2479.292260965672\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2475.9426709811764\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2472.6625660493637\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2469.449937235\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2466.3028929368425\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2463.219641400331\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2460.1984768250677\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2457.237768302245\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2454.3359509821234\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2451.4915189984295\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2448.703019776842\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2445.969049433208\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2443.288249028219\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2440.6593014950786\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2438.080929094554\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2435.5518912832126\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2588.6364897074022\n",
      " Regularized Logistic Regression GD (200/3999): loss=2579.2932121243334\n",
      " Regularized Logistic Regression GD (300/3999): loss=2570.9708960157723\n",
      " Regularized Logistic Regression GD (400/3999): loss=2563.450808801143\n",
      " Regularized Logistic Regression GD (500/3999): loss=2556.5683883770225\n",
      " Regularized Logistic Regression GD (600/3999): loss=2550.199534398956\n",
      " Regularized Logistic Regression GD (700/3999): loss=2544.2501406101987\n",
      " Regularized Logistic Regression GD (800/3999): loss=2538.648262945163\n",
      " Regularized Logistic Regression GD (900/3999): loss=2533.3383858399698\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2528.2772540054125\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2523.430837938005\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2518.7721119327502\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2514.2794140746128\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2509.935225048421\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2505.7252505764886\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Regularized Logistic Regression GD (1600/3999): loss=2501.6377258836615\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2497.662884004521\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2493.7925461048903\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2490.019803480031\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2486.3387690242243\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2482.7443817713465\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2479.232252285671\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2475.7985397194348\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2472.439853579873\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2469.153174894313\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2465.9357926888138\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2462.7852526183397\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2459.6993152838736\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2456.67592230536\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2453.7131686276734\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2450.8092798533394\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2447.9625936417237\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2445.171544406911\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2442.4346506979123\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2439.7505047647346\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2437.117763909031\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2434.5351432941115\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2432.001409950085\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2429.5153777589267\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2587.9842390125245\n",
      " Regularized Logistic Regression GD (200/3999): loss=2578.347881053592\n",
      " Regularized Logistic Regression GD (300/3999): loss=2569.895495467921\n",
      " Regularized Logistic Regression GD (400/3999): loss=2562.333774005347\n",
      " Regularized Logistic Regression GD (500/3999): loss=2555.4575696811007\n",
      " Regularized Logistic Regression GD (600/3999): loss=2549.120823241902\n",
      " Regularized Logistic Regression GD (700/3999): loss=2543.2176132785107\n",
      " Regularized Logistic Regression GD (800/3999): loss=2537.6696332140914\n",
      " Regularized Logistic Regression GD (900/3999): loss=2532.4179389611686\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2527.417436254294\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2522.6331241406597\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2518.037481096644\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2513.608609877788\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2509.328897040108\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2505.184028761776\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2501.1622579908153\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2497.2538518992833\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2493.450670674942\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2489.7458432925946\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2486.133515775272\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2482.608654237214\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2479.166889736315\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2475.804395319727\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2472.517788057231\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2469.304050609118\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2466.1604681649137\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2463.0845775469\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2460.0741259912274\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2457.1270376639623\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2454.241386384649\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2451.415373349906\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2448.6473088973808\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2445.9355975435997\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2443.27872568124\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2440.675251440943\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2438.1237963180147\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2435.6230382403987\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2433.1717058147156\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2430.7685735362847\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2590.42467701449\n",
      " Regularized Logistic Regression GD (200/3999): loss=2582.52793779374\n",
      " Regularized Logistic Regression GD (300/3999): loss=2575.368553304133\n",
      " Regularized Logistic Regression GD (400/3999): loss=2568.780904798686\n",
      " Regularized Logistic Regression GD (500/3999): loss=2562.644839679233\n",
      " Regularized Logistic Regression GD (600/3999): loss=2556.8724357063015\n",
      " Regularized Logistic Regression GD (700/3999): loss=2551.3986770889464\n",
      " Regularized Logistic Regression GD (800/3999): loss=2546.174887163787\n",
      " Regularized Logistic Regression GD (900/3999): loss=2541.1641490653824\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2536.3381054435004\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2531.674702202938\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2527.156579395406\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2522.7699095308494\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2518.503548823943\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2514.3484100970168\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2510.296994680608\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2506.3430397519605\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2502.481250437307\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2498.707094803858\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2495.016645956216\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2491.4064597168613\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2487.8734793953427\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2484.4149613208137\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2481.0284163866095\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2477.7115640088773\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2474.4622957547194\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2471.278646532188\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2468.1587717139255\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2465.100928930004\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2462.1034635431643\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2459.164797033373\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2456.28341768386\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2453.4578730893495\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2450.6867641074964\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2447.968739953575\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2445.3024942006678\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2442.686761496396\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2440.1203148466752\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2437.6019633475053\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2590.046929672595\n",
      " Regularized Logistic Regression GD (200/3999): loss=2582.023648215457\n",
      " Regularized Logistic Regression GD (300/3999): loss=2574.771041829461\n",
      " Regularized Logistic Regression GD (400/3999): loss=2568.1038958368563\n",
      " Regularized Logistic Regression GD (500/3999): loss=2561.8956304848184\n",
      " Regularized Logistic Regression GD (600/3999): loss=2556.055113561319\n",
      " Regularized Logistic Regression GD (700/3999): loss=2550.5157250638517\n",
      " Regularized Logistic Regression GD (800/3999): loss=2545.2280474084105\n",
      " Regularized Logistic Regression GD (900/3999): loss=2540.1548781759443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Regularized Logistic Regression GD (1000/3999): loss=2535.26780561388\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2530.544830570518\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2525.9686922081883\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2521.5256722810786\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2517.2047293932405\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2512.996864163915\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2508.894648332248\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2504.8918718634095\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2500.9832760761547\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2497.1643502098186\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2493.4311752695367\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2489.7803034383287\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2486.2086644704104\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2482.7134927041666\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2479.292269935664\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2475.9426805603835\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2472.6625762497747\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2469.4499480679533\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2466.3029044130676\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2463.2196535299922\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2460.1984896177246\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2457.2377817669462\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2454.3359651273645\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2451.4915338321875\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2448.7030353066584\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2445.9690656661096\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2443.2882659707952\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2440.659319153498\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2438.08094747457\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2435.551910390178\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2588.636489745594\n",
      " Regularized Logistic Regression GD (200/3999): loss=2579.293212265013\n",
      " Regularized Logistic Regression GD (300/3999): loss=2570.9708963092557\n",
      " Regularized Logistic Regression GD (400/3999): loss=2563.450809288419\n",
      " Regularized Logistic Regression GD (500/3999): loss=2556.568389092713\n",
      " Regularized Logistic Regression GD (600/3999): loss=2550.1995353733396\n",
      " Regularized Logistic Regression GD (700/3999): loss=2544.250141870457\n",
      " Regularized Logistic Regression GD (800/3999): loss=2538.648264516234\n",
      " Regularized Logistic Regression GD (900/3999): loss=2533.3383877451456\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2528.2772562666505\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2523.430840576204\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2518.772114967917\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2514.279417525984\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2509.93522893451\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2505.7252549151563\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2501.637730692146\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2497.6628892994527\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2493.792551902299\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2490.019809795354\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2486.338775872358\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2482.7443891665785\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2479.2322602417335\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2475.7985482495137\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2472.4398626965794\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2469.1531846097205\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2465.9358030144444\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2462.785263565185\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2459.6993268623887\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2456.675934525524\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2453.713181498924\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2450.809293384642\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2447.962607841544\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2445.1715592832784\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2442.4346662583816\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2439.750521016426\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2437.1177808586235\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2434.5351609478907\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2432.0014283139258\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2429.5153968383256\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2587.984239052755\n",
      " Regularized Logistic Regression GD (200/3999): loss=2578.3478811994687\n",
      " Regularized Logistic Regression GD (300/3999): loss=2569.8954957691312\n",
      " Regularized Logistic Regression GD (400/3999): loss=2562.3337745016993\n",
      " Regularized Logistic Regression GD (500/3999): loss=2555.457570406078\n",
      " Regularized Logistic Regression GD (600/3999): loss=2549.1208242247503\n",
      " Regularized Logistic Regression GD (700/3999): loss=2543.217614545451\n",
      " Regularized Logistic Regression GD (800/3999): loss=2537.6696347892384\n",
      " Regularized Logistic Regression GD (900/3999): loss=2532.417940867013\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2527.4174385120637\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2522.6331267705823\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2518.037484118046\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2513.6086133092626\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2509.3289008995253\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2505.184033066369\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2501.162262757189\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2497.253857143438\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2493.4506764122884\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2489.7458495379506\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2486.1335225428716\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2482.608661540733\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2479.1668975888256\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2475.804403733762\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2472.517797044739\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2469.3040601814987\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2466.1604783330304\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2463.084588321045\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2460.07413738118\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2457.127049678977\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2454.241399033467\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2451.415386640793\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2448.647322838088\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2445.935612141422\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2443.278740943015\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2440.6752673730443\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2438.1238129264075\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2435.623055530643\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2433.171723791963\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2430.7685922052924\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2590.4246770849677\n",
      " Regularized Logistic Regression GD (200/3999): loss=2582.5279380559805\n",
      " Regularized Logistic Regression GD (300/3999): loss=2575.368553857311\n",
      " Regularized Logistic Regression GD (400/3999): loss=2568.7809057273885\n",
      " Regularized Logistic Regression GD (500/3999): loss=2562.6448410581806\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Regularized Logistic Regression GD (600/3999): loss=2556.8724376033697\n",
      " Regularized Logistic Regression GD (700/3999): loss=2551.398679566945\n",
      " Regularized Logistic Regression GD (800/3999): loss=2546.1748902816917\n",
      " Regularized Logistic Regression GD (900/3999): loss=2541.1641528790956\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2536.3381100063198\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2531.6747075659346\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2527.156585607585\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2522.769916639318\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2518.503556874032\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2514.3484191322977\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2510.29700474296\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2506.3430508816227\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2502.4812626728985\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2498.7071081823915\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2495.0166605131353\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2491.406475486076\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2487.8734964092487\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2484.4149796103343\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2481.0284359812053\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2477.7115849365928\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2474.462318042235\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2471.2786702048393\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2468.158796795753\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2465.1009554437996\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2462.103491510469\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2459.16482647458\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2456.2834486182387\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2453.457905535052\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2450.686798081622\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2447.968775472241\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2445.302531278988\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2442.6868001485627\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2440.1203550860073\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2437.6020051864393\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2590.0469297452732\n",
      " Regularized Logistic Regression GD (200/3999): loss=2582.0236484829757\n",
      " Regularized Logistic Regression GD (300/3999): loss=2574.771042391764\n",
      " Regularized Logistic Regression GD (400/3999): loss=2568.1038967793106\n",
      " Regularized Logistic Regression GD (500/3999): loss=2561.8956318829764\n",
      " Regularized Logistic Regression GD (600/3999): loss=2556.0551154838763\n",
      " Regularized Logistic Regression GD (700/3999): loss=2550.5157275745028\n",
      " Regularized Logistic Regression GD (800/3999): loss=2545.2280505670046\n",
      " Regularized Logistic Regression GD (900/3999): loss=2540.1548820392586\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2535.2678102360364\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2530.5448360033547\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2525.968698501472\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2521.525679482597\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2517.2047375489124\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2512.996873317864\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2508.894658526821\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2504.8918831392384\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2500.9832884721955\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2497.1643637633206\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2493.4311900161197\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2489.780319412028\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2486.2086817036643\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2482.713511227867\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2479.2922897791955\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2475.942701751665\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2472.662598815284\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2469.44997203279\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2466.302929800971\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2463.2196803634133\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2460.1985179178487\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2457.237811553753\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2454.335996419672\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2451.4915666476936\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2448.703069661952\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2445.9691015767853\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2443.288303451431\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2440.659358217713\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2438.080988135115\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2435.5519526589073\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2588.6364898300767\n",
      " Regularized Logistic Regression GD (200/3999): loss=2579.2932125762063\n",
      " Regularized Logistic Regression GD (300/3999): loss=2570.9708969585236\n",
      " Regularized Logistic Regression GD (400/3999): loss=2563.4508103663834\n",
      " Regularized Logistic Regression GD (500/3999): loss=2556.568390675989\n",
      " Regularized Logistic Regression GD (600/3999): loss=2550.1995375288816\n",
      " Regularized Logistic Regression GD (700/3999): loss=2544.250144658403\n",
      " Regularized Logistic Regression GD (800/3999): loss=2538.6482679918136\n",
      " Regularized Logistic Regression GD (900/3999): loss=2533.3383919598073\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2528.277261268989\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2523.430846412464\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2518.772121682363\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2514.2794251611676\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2509.935237531372\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2505.7252645132253\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2501.637741329555\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2497.662901012971\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2493.7925647274133\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2490.01982376623\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2486.3387910218858\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2482.7444055264164\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2479.232277842269\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2475.798567119882\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2472.4398828646913\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2469.153206102263\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2465.9358258569505\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2462.7852877819487\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2459.6993524765794\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2456.6759615591495\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2453.7132099728983\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2450.8093233187847\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2447.9626392546274\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2445.1715921930167\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2442.4347006814974\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2439.7505569686614\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2437.1178183547877\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2434.535200001862\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2432.0014689387176\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2429.515439046078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2587.984239141743\n",
      " Regularized Logistic Regression GD (200/3999): loss=2578.347881522157\n",
      " Regularized Logistic Regression GD (300/3999): loss=2569.895496435434\n",
      " Regularized Logistic Regression GD (400/3999): loss=2562.3337755997495\n",
      " Regularized Logistic Regression GD (500/3999): loss=2555.457572009912\n",
      " Regularized Logistic Regression GD (600/3999): loss=2549.120826399009\n",
      " Regularized Logistic Regression GD (700/3999): loss=2543.2176173482007\n",
      " Regularized Logistic Regression GD (800/3999): loss=2537.6696382738023\n",
      " Regularized Logistic Regression GD (900/3999): loss=2532.417945083169\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2527.4174435067684\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2522.633132588553\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2518.037490802073\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2513.6086209004393\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2509.328909437403\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2505.184042589073\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2501.1622733014383\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2497.253868744651\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2493.4506891045457\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2489.745863354033\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2486.13353751428\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2482.608677697681\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2479.1669149602812\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2475.804422347417\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2472.5178169270557\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2469.304081357685\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2466.1605008270794\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2463.084612155773\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2460.0741625782107\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2457.127076258785\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2454.2414270154045\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2451.4154160430926\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2448.6473536779267\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2445.935644434948\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2443.278774705324\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2440.6753026182887\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2438.123849667764\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2435.6230937803957\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2433.1717635614905\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2430.768633505156\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2590.4246772408633\n",
      " Regularized Logistic Regression GD (200/3999): loss=2582.5279386361285\n",
      " Regularized Logistic Regression GD (300/3999): loss=2575.368555081056\n",
      " Regularized Logistic Regression GD (400/3999): loss=2568.780907781871\n",
      " Regularized Logistic Regression GD (500/3999): loss=2562.6448441087264\n",
      " Regularized Logistic Regression GD (600/3999): loss=2556.8724418000957\n",
      " Regularized Logistic Regression GD (700/3999): loss=2551.398685048825\n",
      " Regularized Logistic Regression GD (800/3999): loss=2546.174897179187\n",
      " Regularized Logistic Regression GD (900/3999): loss=2541.1641613158595\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2536.3381201002817\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2531.6747194300415\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2527.156599350265\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2522.769932364804\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2518.503574682552\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2514.3484391202737\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2510.29702700305\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2506.3430755028385\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2502.4812897406564\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2498.70713777858\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2495.016692716198\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2491.4065103709877\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2487.8735340476956\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2484.4150200706945\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2481.02847932867\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2477.711631233211\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2474.462367347025\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2471.2787225738443\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2468.1588522821567\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2465.1010140979943\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2462.103553380172\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2459.1648916048857\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2456.283517051731\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2453.4579773119194\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2450.686873239715\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2447.9688540471616\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2445.3026133042154\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2442.686885655501\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2440.1204441040522\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2437.60209774315\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2590.046929906051\n",
      " Regularized Logistic Regression GD (200/3999): loss=2582.023649074765\n",
      " Regularized Logistic Regression GD (300/3999): loss=2574.771043635674\n",
      " Regularized Logistic Regression GD (400/3999): loss=2568.103898864218\n",
      " Regularized Logistic Regression GD (500/3999): loss=2561.895634976011\n",
      " Regularized Logistic Regression GD (600/3999): loss=2556.055119736994\n",
      " Regularized Logistic Regression GD (700/3999): loss=2550.5157331286046\n",
      " Regularized Logistic Regression GD (800/3999): loss=2545.2280575544983\n",
      " Regularized Logistic Regression GD (900/3999): loss=2540.154890585692\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2535.267820461221\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2530.544848021982\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2525.968712423583\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2521.525695413915\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2517.2047555910226\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2512.9968935683683\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2508.894681079431\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2504.891908083816\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2500.983315894889\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2497.1643937465747\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2493.431222638758\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2489.780354749305\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2486.2087198273366\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2482.713552206278\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2479.292333677355\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2475.942748631335\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2472.6626487350377\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2469.4500250481447\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2466.3029859644676\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2463.219739724707\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2460.198580523821\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2457.2378774485874\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2454.336065644987\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2451.4916392426235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Regularized Logistic Regression GD (3400/3999): loss=2448.7031456632562\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2445.9691810189015\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2443.2883863666384\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2440.659444636187\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2438.0810780849847\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2435.552046166417\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2588.636490016994\n",
      " Regularized Logistic Regression GD (200/3999): loss=2579.293213264644\n",
      " Regularized Logistic Regression GD (300/3999): loss=2570.970898394822\n",
      " Regularized Logistic Regression GD (400/3999): loss=2563.4508127510494\n",
      " Regularized Logistic Regression GD (500/3999): loss=2556.568394178519\n",
      " Regularized Logistic Regression GD (600/3999): loss=2550.199542297404\n",
      " Regularized Logistic Regression GD (700/3999): loss=2544.250150825974\n",
      " Regularized Logistic Regression GD (800/3999): loss=2538.6482756805235\n",
      " Regularized Logistic Regression GD (900/3999): loss=2533.338401283543\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2528.2772723352423\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2523.430859323531\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2518.7721365361717\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2514.27944205181\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2509.935256549504\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2505.725285746245\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2501.6377648617913\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2497.662926925804\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2493.7925930993083\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2490.019854672806\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2486.3388245359456\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2482.744441717923\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2479.232316778457\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2475.7986088651996\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2472.439927480917\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2469.1532536484465\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2465.935876389487\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2462.7853413546463\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2459.6994091406764\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2456.6760213633593\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2453.713272963483\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2450.8093895395723\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2447.962708747121\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2445.1716649964237\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2442.434776832828\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2439.7506365027407\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2437.117901304362\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2434.5352863976523\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2432.001558809484\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2429.5155324186862\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2587.9842393385943\n",
      " Regularized Logistic Regression GD (200/3999): loss=2578.347882236034\n",
      " Regularized Logistic Regression GD (300/3999): loss=2569.8954979094688\n",
      " Regularized Logistic Regression GD (400/3999): loss=2562.3337780288502\n",
      " Regularized Logistic Regression GD (500/3999): loss=2555.457575557929\n",
      " Regularized Logistic Regression GD (600/3999): loss=2549.1208312089116\n",
      " Regularized Logistic Regression GD (700/3999): loss=2543.2176235484712\n",
      " Regularized Logistic Regression GD (800/3999): loss=2537.6696459824166\n",
      " Regularized Logistic Regression GD (900/3999): loss=2532.4179544101835\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2527.4174545561286\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2522.633145459153\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2518.0375055885634\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2513.6086376937337\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2509.3289283250215\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2505.184063655332\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2501.1622966276113\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2497.2538944090284\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2493.4507171825403\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2489.745893918175\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2486.133570634248\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2482.608713440345\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2479.1669533896847\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2475.8044635248457\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2472.5178609110085\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2469.304128203959\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2466.1605505887646\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2463.084664883321\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2460.074218319471\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2457.127135059047\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2454.241488917475\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2451.415481087325\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2448.6474219023157\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2445.935715875161\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2443.278849394823\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2440.6753805883377\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2438.1239309475654\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2435.6231783970834\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2433.1718515402645\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2430.7687248693237\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2590.424677585739\n",
      " Regularized Logistic Regression GD (200/3999): loss=2582.527939919528\n",
      " Regularized Logistic Regression GD (300/3999): loss=2575.3685577882716\n",
      " Regularized Logistic Regression GD (400/3999): loss=2568.7809123268307\n",
      " Regularized Logistic Regression GD (500/3999): loss=2562.6448508571975\n",
      " Regularized Logistic Regression GD (600/3999): loss=2556.8724510841635\n",
      " Regularized Logistic Regression GD (700/3999): loss=2551.398697175917\n",
      " Regularized Logistic Regression GD (800/3999): loss=2546.174912437931\n",
      " Regularized Logistic Regression GD (900/3999): loss=2541.1641799797935\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2536.3381424302975\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2531.6747456760063\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2527.1566297520308\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2522.769967152954\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2518.5036140788457\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2514.3484833379744\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2510.297076247159\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2506.343129970258\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2502.481349620357\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2498.7072032517312\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2495.016763956245\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2491.406587543914\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2487.8736173120033\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2484.4151095776924\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2481.028575222537\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2477.711733651256\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2474.4624764197715\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2471.278838425303\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2468.1589750299668\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Regularized Logistic Regression GD (2900/3999): loss=2465.1011438536357\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2462.1036902491846\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2459.1650356870373\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2456.2836684412473\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2453.458136097678\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2450.6870395054543\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2447.9690278716707\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2445.302794761508\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2442.6870748150545\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2440.120641030978\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2437.602302498314\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2590.046930261741\n",
      " Regularized Logistic Regression GD (200/3999): loss=2582.0236503839255\n",
      " Regularized Logistic Regression GD (300/3999): loss=2574.771046387496\n",
      " Regularized Logistic Regression GD (400/3999): loss=2568.1039034765045\n",
      " Regularized Logistic Regression GD (500/3999): loss=2561.895641818458\n",
      " Regularized Logistic Regression GD (600/3999): loss=2556.055129145802\n",
      " Regularized Logistic Regression GD (700/3999): loss=2550.515745415467\n",
      " Regularized Logistic Regression GD (800/3999): loss=2545.228073012345\n",
      " Regularized Logistic Regression GD (900/3999): loss=2540.154909492296\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2535.2678430815395\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2530.5448746097472\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2525.9687432222754\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2521.5257306574103\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2517.204795504065\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2512.9969383668486\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2508.894730970622\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2504.891963266608\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2500.9833765597905\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2497.1644600759932\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2493.431294807041\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2489.780432922934\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2486.208804165088\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2482.7136428593344\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2479.2924307895228\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2475.9428523392094\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2472.662759168234\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2469.450142329465\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2466.3031102101477\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2463.219871044596\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2460.198719021606\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2457.2380232220644\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2454.336218786196\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2451.491799838125\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2448.7033137943577\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2445.969356761835\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2443.288569792792\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2440.6596358122706\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2438.081277073289\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2435.55225302496\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2588.63649043046\n",
      " Regularized Logistic Regression GD (200/3999): loss=2579.2932147876104\n",
      " Regularized Logistic Regression GD (300/3999): loss=2570.9709015722306\n",
      " Regularized Logistic Regression GD (400/3999): loss=2563.4508180264716\n",
      " Regularized Logistic Regression GD (500/3999): loss=2556.5684019268842\n",
      " Regularized Logistic Regression GD (600/3999): loss=2550.199552846406\n",
      " Regularized Logistic Regression GD (700/3999): loss=2544.250164469963\n",
      " Regularized Logistic Regression GD (800/3999): loss=2538.648292689615\n",
      " Regularized Logistic Regression GD (900/3999): loss=2533.3384219096783\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2528.2772968161908\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2523.4308878856023\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2518.7721693959943\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2514.2794794175693\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2509.935298621705\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2505.725332718244\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2501.6378169201716\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2497.6629842505718\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2493.7926558640556\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2490.01992304479\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2486.338898676198\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2482.7445217812947\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2479.2324029136366\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2475.7987012148187\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2472.440026181561\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2469.1533588307643\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2465.9359881782857\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2462.7854598689355\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2459.6995344937914\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2456.676153663062\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2453.7134123120995\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2450.8095360340976\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2447.962862479368\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2445.1718260531466\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2442.4349452958345\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2439.750812449107\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2437.1180848065324\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2434.5354775235705\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2432.0017576227833\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2429.5157389788137\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2587.9842397740867\n",
      " Regularized Logistic Regression GD (200/3999): loss=2578.347883815283\n",
      " Regularized Logistic Regression GD (300/3999): loss=2569.895501170341\n",
      " Regularized Logistic Regression GD (400/3999): loss=2562.333783402564\n",
      " Regularized Logistic Regression GD (500/3999): loss=2555.4575834068914\n",
      " Regularized Logistic Regression GD (600/3999): loss=2549.12084184946\n",
      " Regularized Logistic Regression GD (700/3999): loss=2543.217637264818\n",
      " Regularized Logistic Regression GD (800/3999): loss=2537.669663035514\n",
      " Regularized Logistic Regression GD (900/3999): loss=2532.417975043575\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2527.417478999692\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2522.6331739316684\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2518.0375382994644\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2513.6086748441376\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2509.3289701085\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2505.184110258433\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2501.1623482301097\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2497.25395118415\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2493.450779297102\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2489.7459615326156\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2486.13364390274\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2482.608792510782\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2479.167038403771\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Regularized Logistic Regression GD (2300/3999): loss=2475.804554618144\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2472.5179582129376\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2469.304231837948\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2466.160660672284\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2463.08478152796\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2460.0743416310775\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2457.1272651378213\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2454.2416258580797\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2451.415624979064\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2448.6475728292357\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2445.935873916178\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2443.279014623941\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2440.675553074752\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2438.124110755819\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2435.623365587224\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2433.172046168056\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2430.7689269863463\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2590.42467834869\n",
      " Regularized Logistic Regression GD (200/3999): loss=2582.527942758703\n",
      " Regularized Logistic Regression GD (300/3999): loss=2575.368563777206\n",
      " Regularized Logistic Regression GD (400/3999): loss=2568.7809223812546\n",
      " Regularized Logistic Regression GD (500/3999): loss=2562.6448657862766\n",
      " Regularized Logistic Regression GD (600/3999): loss=2556.8724716225142\n",
      " Regularized Logistic Regression GD (700/3999): loss=2551.3987240036586\n",
      " Regularized Logistic Regression GD (800/3999): loss=2546.17494619356\n",
      " Regularized Logistic Regression GD (900/3999): loss=2541.164221268424\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2536.3381918290997\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2531.6748037376938\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2527.156697007244\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2522.7700441117845\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2518.5037012318744\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2514.3485811569726\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2510.2971851856223\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2506.343250463806\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2502.481482087019\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2498.707348092256\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2495.016921554412\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2491.406758266813\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2487.873801510354\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2484.4153075861504\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2481.02878736012\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2477.7119602216535\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2474.462717711783\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2471.2790947132266\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2468.1592465740637\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2465.1014309005204\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2462.1039930323145\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2459.165354427127\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2456.2840033467505\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2453.4584873652043\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2450.6874073202425\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2447.9694124079933\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2445.3031961831525\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2442.687493275688\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2440.1210766746008\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2437.6027554596412\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2590.046931048583\n",
      " Regularized Logistic Regression GD (200/3999): loss=2582.023653280077\n",
      " Regularized Logistic Regression GD (300/3999): loss=2574.771052475113\n",
      " Regularized Logistic Regression GD (400/3999): loss=2568.1039136798768\n",
      " Regularized Logistic Regression GD (500/3999): loss=2561.895656955429\n",
      " Regularized Logistic Regression GD (600/3999): loss=2556.0551499600965\n",
      " Regularized Logistic Regression GD (700/3999): loss=2550.515772596644\n",
      " Regularized Logistic Regression GD (800/3999): loss=2545.228107208406\n",
      " Regularized Logistic Regression GD (900/3999): loss=2540.15495131774\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2535.267893122549\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2530.544933427608\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2525.9688113555835\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2521.5258086235563\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2517.204883800224\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2512.99703747064\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2508.8948413406183\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2504.89208534268\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2500.983510763483\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2497.164606810761\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2493.431454458629\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2489.7806058596357\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2486.208990738094\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2482.713843403101\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2479.292645622206\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2475.943081762988\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2472.663003469846\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2469.450401780576\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2466.3033850678494\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2463.22016155191\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2460.199025407952\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2457.238345703705\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2454.3365575667963\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2451.492155109191\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2448.703685735718\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2445.9697455421374\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2443.2889755699343\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2440.6600587338826\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2438.0817172771417\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2435.552710639404\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2588.6364913451625\n",
      " Regularized Logistic Regression GD (200/3999): loss=2579.2932181567244\n",
      " Regularized Logistic Regression GD (300/3999): loss=2570.9709086013404\n",
      " Regularized Logistic Regression GD (400/3999): loss=2563.450829696817\n",
      " Regularized Logistic Regression GD (500/3999): loss=2556.568419067953\n",
      " Regularized Logistic Regression GD (600/3999): loss=2550.199576183083\n",
      " Regularized Logistic Regression GD (700/3999): loss=2544.2501946533953\n",
      " Regularized Logistic Regression GD (800/3999): loss=2538.648330317359\n",
      " Regularized Logistic Regression GD (900/3999): loss=2533.338467539099\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2528.277350973283\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2523.4309510710127\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2518.772242088972\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2514.2795620786255\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2509.9353916944015\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2505.7254366303396\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2501.6379320844103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Regularized Logistic Regression GD (1700/3999): loss=2497.663111065178\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2493.792794713041\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2490.0200742982106\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2486.339062690253\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2482.7446988985107\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2479.232593462956\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2475.7989055117487\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2472.4402445283063\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2469.15359151636\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2465.936235478777\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2462.785722047613\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2459.6998118013685\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2456.676446337965\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2453.7137205806566\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2450.809860110902\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2447.963202567501\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2445.1721823445023\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2442.4353179714562\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2439.7512016794626\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2437.11849075186\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2434.5359003342037\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2432.0021974394904\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2429.5161959330967\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2587.9842407374704\n",
      " Regularized Logistic Regression GD (200/3999): loss=2578.3478873089266\n",
      " Regularized Logistic Regression GD (300/3999): loss=2569.8955083841233\n",
      " Regularized Logistic Regression GD (400/3999): loss=2562.3337952903667\n",
      " Regularized Logistic Regression GD (500/3999): loss=2555.4576007704995\n",
      " Regularized Logistic Regression GD (600/3999): loss=2549.1208653886606\n",
      " Regularized Logistic Regression GD (700/3999): loss=2543.217667608337\n",
      " Regularized Logistic Regression GD (800/3999): loss=2537.669700760659\n",
      " Regularized Logistic Regression GD (900/3999): loss=2532.418020689059\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2527.417533074085\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2522.6332369190263\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2518.0376106629815\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2513.6087570287677\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2509.329062542504\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2505.184213354434\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2501.162462385847\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2497.2540767828445\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2493.4509167077363\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2489.746111110162\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2486.1338059882114\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2482.608967431393\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2479.1672264730028\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2475.8047561358285\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2472.5181734654625\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2469.3044610983156\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2466.160904200335\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2463.085039570585\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2460.074614422408\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2457.1275528995266\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2454.241928799618\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2451.415943297939\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2448.647906711359\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2445.936223536209\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2443.2793801455364\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2440.6759346509625\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2438.124508529426\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2435.6237796911323\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2433.172476725567\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2430.769374111567\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2590.424680036516\n",
      " Regularized Logistic Regression GD (200/3999): loss=2582.527949039539\n",
      " Regularized Logistic Regression GD (300/3999): loss=2575.368577026016\n",
      " Regularized Logistic Regression GD (400/3999): loss=2568.7809446237866\n",
      " Regularized Logistic Regression GD (500/3999): loss=2562.644898812619\n",
      " Regularized Logistic Regression GD (600/3999): loss=2556.872517057717\n",
      " Regularized Logistic Regression GD (700/3999): loss=2551.398783352332\n",
      " Regularized Logistic Regression GD (800/3999): loss=2546.175020868184\n",
      " Regularized Logistic Regression GD (900/3999): loss=2541.1643126076533\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2536.3383011097203\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2531.674932182456\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2527.156845790023\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2522.7702143609667\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2518.503894032726\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2514.348797553173\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2510.297426180398\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2506.3435170207854\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2502.481775130971\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2498.707668509741\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2495.0172701944175\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2491.4071359413824\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2487.8742089953366\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2484.4157456219164\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2481.029256652345\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2477.712461442151\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2474.463251499433\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2471.2796616748424\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2468.159847285341\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2465.102065907066\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2462.1046628505264\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2459.166059545282\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2456.2847442259667\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2453.4592644404293\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2450.688221001217\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2447.9702630802863\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2445.3040842090145\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2442.6884189951024\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2440.1220404060846\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2437.603757501207\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2590.046932789244\n",
      " Regularized Logistic Regression GD (200/3999): loss=2582.0236596870013\n",
      " Regularized Logistic Regression GD (300/3999): loss=2574.7710659422487\n",
      " Regularized Logistic Regression GD (400/3999): loss=2568.1039362519086\n",
      " Regularized Logistic Regression GD (500/3999): loss=2561.895690441681\n",
      " Regularized Logistic Regression GD (600/3999): loss=2556.055196005783\n",
      " Regularized Logistic Regression GD (700/3999): loss=2550.515832727221\n",
      " Regularized Logistic Regression GD (800/3999): loss=2545.2281828573814\n",
      " Regularized Logistic Regression GD (900/3999): loss=2540.1550438445115\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2535.268003823878\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Regularized Logistic Regression GD (1100/3999): loss=2530.5450635451657\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2525.968962080861\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2521.5259811011365\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2517.2050791299353\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2512.997256709085\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2508.8950855022226\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2504.892355400543\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2500.983807650119\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2497.1649314187284\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2493.4318076412314\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2489.780988431592\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2486.2094034763113\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2482.7142870475086\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2479.2931208765376\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2475.9435892957854\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2472.66354391535\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2469.450975739788\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2466.3039931095236\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2463.2208042136285\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2460.199703197194\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2457.2390590989266\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2454.337307018521\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2451.492941041043\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2448.7045085455206\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2445.970605602948\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2443.2898732310528\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2440.660994321861\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2438.082691096762\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2435.553722974592\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2588.6364933686914\n",
      " Regularized Logistic Regression GD (200/3999): loss=2579.293225609971\n",
      " Regularized Logistic Regression GD (300/3999): loss=2570.970924151263\n",
      " Regularized Logistic Regression GD (400/3999): loss=2563.450855514161\n",
      " Regularized Logistic Regression GD (500/3999): loss=2556.5684569876335\n",
      " Regularized Logistic Regression GD (600/3999): loss=2550.199627808787\n",
      " Regularized Logistic Regression GD (700/3999): loss=2544.2502614255895\n",
      " Regularized Logistic Regression GD (800/3999): loss=2538.6484135579854\n",
      " Regularized Logistic Regression GD (900/3999): loss=2533.338568481086\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2528.277470780294\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2523.4310908505418\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2518.7724029012193\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2514.2797449423397\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2509.935597590805\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2505.725666505769\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2501.6381868519125\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2497.663391605679\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2493.793101876054\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2490.020408902342\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2486.3394255235085\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2482.745090718628\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2479.233014997636\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2475.7993574589386\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2472.440727556503\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2469.154106264961\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2465.9367825584677\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2462.786302040801\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2459.700425262706\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2456.6770937948345\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2453.714402533808\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2450.8105770349907\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2447.9639549117874\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2445.172970533503\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2442.4361424056374\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2439.752062735941\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2437.1193887850936\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2434.536835676767\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2432.0031704027388\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2429.5172068079337\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2587.9842428687025\n",
      " Regularized Logistic Regression GD (200/3999): loss=2578.347895037603\n",
      " Regularized Logistic Regression GD (300/3999): loss=2569.8955243425207\n",
      " Regularized Logistic Regression GD (400/3999): loss=2562.3338215887293\n",
      " Regularized Logistic Regression GD (500/3999): loss=2555.457639182514\n",
      " Regularized Logistic Regression GD (600/3999): loss=2549.120917462401\n",
      " Regularized Logistic Regression GD (700/3999): loss=2543.217734734652\n",
      " Regularized Logistic Regression GD (800/3999): loss=2537.6697842167177\n",
      " Regularized Logistic Regression GD (900/3999): loss=2532.418121666595\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2527.4176526981523\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2522.633376260372\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2518.0377707464186\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2513.6089388385335\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2509.329267025997\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2505.1844414245047\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2501.1627149223127\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2497.2543546334778\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2493.4512206888376\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2489.74644200694\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2486.134164555092\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2482.609354392207\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2479.1676425212167\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2475.8052019347647\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2472.518649648589\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2469.3049682696096\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2466.1614429346077\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2463.0856104139916\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2460.075217892909\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2457.12818948746\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2454.242598968302\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2451.416647484349\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2448.6486453267594\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2445.936996966893\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2443.280188753565\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2440.6767787749277\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2438.125388485203\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2435.6246957726567\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2433.173429205631\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2430.77036324259\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2590.424683770317\n",
      " Regularized Logistic Regression GD (200/3999): loss=2582.527962934104\n",
      " Regularized Logistic Regression GD (300/3999): loss=2575.368606335201\n",
      " Regularized Logistic Regression GD (400/3999): loss=2568.7809938290143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Regularized Logistic Regression GD (500/3999): loss=2562.6449718738613\n",
      " Regularized Logistic Regression GD (600/3999): loss=2556.8726175699853\n",
      " Regularized Logistic Regression GD (700/3999): loss=2551.398914644093\n",
      " Regularized Logistic Regression GD (800/3999): loss=2546.1751860640948\n",
      " Regularized Logistic Regression GD (900/3999): loss=2541.1645146690653\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2536.3385428611523\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2531.6752163288197\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2527.157174928116\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2522.7705909870565\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2518.504320547555\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2514.3492762655246\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2510.2979593095765\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2506.3441066985206\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2502.482423402979\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2498.7083773372105\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2495.0180414554175\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2491.4079714323075\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2487.8751104324456\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2484.4167146429886\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2481.030294818428\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2477.7135702394303\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2474.4644323411153\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2471.2809159032795\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2468.1611761740596\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2465.103470662913\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2462.106144615891\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2459.1676194002694\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2456.2863831905715\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2453.4609834767684\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2450.6900210156955\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2447.9721449257268\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2445.3060486868026\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2442.690466857205\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2440.1241723571184\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2437.6059742003613\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2590.046936639967\n",
      " Regularized Logistic Regression GD (200/3999): loss=2582.023673860485\n",
      " Regularized Logistic Regression GD (300/3999): loss=2574.771095734417\n",
      " Regularized Logistic Regression GD (400/3999): loss=2568.1039861860595\n",
      " Regularized Logistic Regression GD (500/3999): loss=2561.8957645203245\n",
      " Regularized Logistic Regression GD (600/3999): loss=2556.0552978685455\n",
      " Regularized Logistic Regression GD (700/3999): loss=2550.5159657486915\n",
      " Regularized Logistic Regression GD (800/3999): loss=2545.2283502087744\n",
      " Regularized Logistic Regression GD (900/3999): loss=2540.1552485330176\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2535.2682487181946\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2530.545351392096\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2525.969295516205\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2521.5263626568844\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2517.2055112391213\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2512.9977417090577\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2508.8956256370598\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2504.8929528229355\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2500.984464422893\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2497.1656495163925\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2493.432588951355\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2489.781834756495\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2486.210316534631\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2482.715268475996\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2479.2941722319856\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2475.9447120570617\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2472.6647394854836\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2469.452245448241\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2466.305338214449\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2463.222225904201\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2460.2012025959984\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2457.2406372643495\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2454.3389649471055\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2451.4946796699\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2448.706328754652\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2445.972508217559\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2443.2918590238073\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2440.6630640150915\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2438.0848453646163\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2435.5559624451294\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2588.6364978451593\n",
      " Regularized Logistic Regression GD (200/3999): loss=2579.2932420981138\n",
      " Regularized Logistic Regression GD (300/3999): loss=2570.970958551003\n",
      " Regularized Logistic Regression GD (400/3999): loss=2563.450912627567\n",
      " Regularized Logistic Regression GD (500/3999): loss=2556.5685408740055\n",
      " Regularized Logistic Regression GD (600/3999): loss=2550.1997420157495\n",
      " Regularized Logistic Regression GD (700/3999): loss=2544.25040913971\n",
      " Regularized Logistic Regression GD (800/3999): loss=2538.648597703643\n",
      " Regularized Logistic Regression GD (900/3999): loss=2533.338791785787\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2528.2777358182493\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2523.431400071758\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2518.7727586510046\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2514.2801494743617\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2509.9360530757267\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2505.726175036942\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2501.6387504492\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2497.66401221782\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2493.793781382329\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2490.0211491135706\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2486.3402281829226\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2482.745957502404\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2479.233947515558\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2475.8003572549483\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2472.441796109385\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2469.1552449891465\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2465.937992804852\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2462.787585097761\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2459.7017823571523\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2456.6785260934016\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2453.71591114415\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2450.8121630071237\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2447.9656192394987\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2445.1747141558394\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2442.4379662084457\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2439.7539675533835\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2437.1213754012815\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2434.538904827329\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2432.0053227764197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Regularized Logistic Regression GD (3900/3999): loss=2429.519443048206\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2587.984247583444\n",
      " Regularized Logistic Regression GD (200/3999): loss=2578.347912135113\n",
      " Regularized Logistic Regression GD (300/3999): loss=2569.8955596459286\n",
      " Regularized Logistic Regression GD (400/3999): loss=2562.33387976632\n",
      " Regularized Logistic Regression GD (500/3999): loss=2555.4577241580328\n",
      " Regularized Logistic Regression GD (600/3999): loss=2549.121032660513\n",
      " Regularized Logistic Regression GD (700/3999): loss=2543.217883232195\n",
      " Regularized Logistic Regression GD (800/3999): loss=2537.669968838969\n",
      " Regularized Logistic Regression GD (900/3999): loss=2532.4183450499386\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2527.4179173313883\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2522.633684512252\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2518.0381248839253\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2513.609341039035\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2509.329719385255\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2505.184945961888\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2501.1632735841404\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2497.2549692951275\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2493.4518931560697\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2489.7471740168044\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2486.1349577764477\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2482.6102104262627\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2479.1685629020017\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2475.8061881296553\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2472.5197030589143\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2469.3060902313628\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2466.162634719389\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2463.0868732299054\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2460.0765528857382\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2457.129597741893\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2454.2440815092536\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2451.4182052783954\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2448.6502792836436\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2445.938707941141\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2443.2819775460607\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2440.6786461346196\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2438.127335110814\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2435.6267223143805\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2433.175536266856\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2430.7725513816763\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2590.424692030302\n",
      " Regularized Logistic Regression GD (200/3999): loss=2582.527993671823\n",
      " Regularized Logistic Regression GD (300/3999): loss=2575.3686711732867\n",
      " Regularized Logistic Regression GD (400/3999): loss=2568.781102681238\n",
      " Regularized Logistic Regression GD (500/3999): loss=2562.6451335004112\n",
      " Regularized Logistic Regression GD (600/3999): loss=2556.8728399236848\n",
      " Regularized Logistic Regression GD (700/3999): loss=2551.3992050880543\n",
      " Regularized Logistic Regression GD (800/3999): loss=2546.1755515105606\n",
      " Regularized Logistic Regression GD (900/3999): loss=2541.164961669078\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2536.339077662865\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2531.6758449159424\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2527.1579030450985\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2522.771424155898\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2518.505264079018\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2514.3503352668845\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2510.299138690124\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2506.3454111738\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2502.4838574983164\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2498.7099453908863\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2495.019747621801\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2491.4098196850964\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2487.8771045680837\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2484.4188582842776\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2481.0325914184273\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2477.716023085518\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2474.467044559358\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2471.283690462857\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2468.16411589196\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2465.106578208631\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2462.1094225162715\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2459.1710700443737\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2456.290008834535\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2453.4647862486404\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2450.6940029202883\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2447.9763078493543\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2445.3103944019317\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2442.6949970270166\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2440.1288885400145\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2437.6108778542516\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2590.046945158609\n",
      " Regularized Logistic Regression GD (200/3999): loss=2582.0237052152256\n",
      " Regularized Logistic Regression GD (300/3999): loss=2574.771161640956\n",
      " Regularized Logistic Regression GD (400/3999): loss=2568.104096650793\n",
      " Regularized Logistic Regression GD (500/3999): loss=2561.8959283975955\n",
      " Regularized Logistic Regression GD (600/3999): loss=2556.055523209819\n",
      " Regularized Logistic Regression GD (700/3999): loss=2550.516260019141\n",
      " Regularized Logistic Regression GD (800/3999): loss=2545.2287204236136\n",
      " Regularized Logistic Regression GD (900/3999): loss=2540.155701344657\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2535.2687904726185\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2530.5459881656\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2525.970033139514\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2521.5272067310952\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2517.206467146346\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2512.9988146197884\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2508.8968205154324\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2504.8942744308315\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2500.9859173234713\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2497.167238077328\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2493.4343173481693\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2489.7837069759375\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2486.212336378363\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2482.7174395645825\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2479.296498009056\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2475.947195793843\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2472.6673842854793\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2469.4550542521442\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2466.308313805208\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2463.2253709122065\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2460.2045195043033\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2457.2441284139704\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2454.342632542308\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Regularized Logistic Regression GD (3300/3999): loss=2451.498525783408\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2448.710355332875\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2445.9767170856744\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2443.296251890762\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2440.667642478515\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2438.0896109155165\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2435.560916472561\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2588.636507748059\n",
      " Regularized Logistic Regression GD (200/3999): loss=2579.293278573417\n",
      " Regularized Logistic Regression GD (300/3999): loss=2570.9710346504758\n",
      " Regularized Logistic Regression GD (400/3999): loss=2563.451038974368\n",
      " Regularized Logistic Regression GD (500/3999): loss=2556.568726447985\n",
      " Regularized Logistic Regression GD (600/3999): loss=2550.1999946649\n",
      " Regularized Logistic Regression GD (700/3999): loss=2544.2507359133087\n",
      " Regularized Logistic Regression GD (800/3999): loss=2538.6490050708258\n",
      " Regularized Logistic Regression GD (900/3999): loss=2533.339285780216\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2528.2783221344184\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2523.4320841294266\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2518.773545638302\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2514.2810443766143\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2509.937060694661\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2505.7273000033033\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2501.639997231059\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2497.6653851259844\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2493.795284573829\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2490.022786594116\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2486.342003808548\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2482.7478749809366\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2479.236010407829\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2475.8025689762158\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2472.4441599307434\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2469.157764039253\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2465.940670071889\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2462.790423431536\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2459.7047844715275\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2456.6816945689066\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2453.719248430813\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2450.8156714274514\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2447.9693009916277\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2445.1785713168206\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2442.4420007375793\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2439.75818129573\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2437.125770091097\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2434.5434820916284\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2432.010084138525\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2429.5243899312577\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2587.9842580134664\n",
      " Regularized Logistic Regression GD (200/3999): loss=2578.347949958431\n",
      " Regularized Logistic Regression GD (300/3999): loss=2569.8956377444792\n",
      " Regularized Logistic Regression GD (400/3999): loss=2562.33400846727\n",
      " Regularized Logistic Regression GD (500/3999): loss=2555.4579121414454\n",
      " Regularized Logistic Regression GD (600/3999): loss=2549.1212875022948\n",
      " Regularized Logistic Regression GD (700/3999): loss=2543.218211738843\n",
      " Regularized Logistic Regression GD (800/3999): loss=2537.670377260473\n",
      " Regularized Logistic Regression GD (900/3999): loss=2532.4188392183464\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2527.4185027522913\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2522.6343664256083\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2518.038908304545\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2513.610230783535\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2509.330720089677\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2505.186062093279\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2501.164509447922\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2497.2563290397607\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2493.453380776055\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2489.7487933545244\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2486.1367125235374\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2482.6121041246756\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2479.1705989450165\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2475.8083697630973\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2472.522033382523\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2469.3085722003752\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2466.1652711464994\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2463.089666787411\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2460.0795061080503\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2457.132713028223\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2454.247361126615\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2451.4216513647334\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2448.65389385122\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2445.94249287996\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2443.2859346275554\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2440.6827770153154\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2438.1316413360723\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2435.63120532226\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2433.180197391909\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2430.7773918587336\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2590.424710303136\n",
      " Regularized Logistic Regression GD (200/3999): loss=2582.528061670061\n",
      " Regularized Logistic Regression GD (300/3999): loss=2575.368814608316\n",
      " Regularized Logistic Regression GD (400/3999): loss=2568.781343483981\n",
      " Regularized Logistic Regression GD (500/3999): loss=2562.6454910497905\n",
      " Regularized Logistic Regression GD (600/3999): loss=2556.8733318123586\n",
      " Regularized Logistic Regression GD (700/3999): loss=2551.399847604104\n",
      " Regularized Logistic Regression GD (800/3999): loss=2546.176359944571\n",
      " Regularized Logistic Regression GD (900/3999): loss=2541.165950512388\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2536.3402607368753\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2531.677235456851\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2527.1595137597997\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2522.7732672590623\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2518.507351318022\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2514.352677938849\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2510.301747654163\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2506.34829686038\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2502.487029917023\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2498.713414137474\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2495.0235218851726\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2491.4139082544434\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2487.8815158404072\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2484.4236002724447\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2481.03767175973\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2477.7214490507736\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2474.472823062455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Regularized Logistic Regression GD (2700/3999): loss=2471.289828070642\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2468.1706188345365\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2465.1134523898463\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2462.1166735243887\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2459.178703162803\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2456.298029052628\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2453.4731982724593\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2450.7028111832033\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2447.9855165225767\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2445.320007404833\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2442.705018037254\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2440.1393210033793\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2437.621724994305\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2590.0469640036363\n",
      " Regularized Logistic Regression GD (200/3999): loss=2582.023774578427\n",
      " Regularized Logistic Regression GD (300/3999): loss=2574.7713074396006\n",
      " Regularized Logistic Regression GD (400/3999): loss=2568.1043410207453\n",
      " Regularized Logistic Regression GD (500/3999): loss=2561.89629092599\n",
      " Regularized Logistic Regression GD (600/3999): loss=2556.0560217075636\n",
      " Regularized Logistic Regression GD (700/3999): loss=2550.5169110000825\n",
      " Regularized Logistic Regression GD (800/3999): loss=2545.229539406098\n",
      " Regularized Logistic Regression GD (900/3999): loss=2540.156703044357\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2535.2699889272253\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2530.5473968161577\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2525.971664883751\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2521.529073958677\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2517.2085817624697\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2513.0011880613674\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2508.8994637629116\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2504.897198017153\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2500.9891313419153\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2497.170752188701\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2493.4381407880983\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2489.787848562457\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2486.216804520039\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2482.7222422692807\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2479.301642893267\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2475.9526900925675\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2472.6732348627015\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2469.4612676116753\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2466.3148961018856\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2463.2323279633115\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2460.211856801247\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2457.2518511338944\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2454.35074555979\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2451.5070336821113\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2448.71926241698\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2445.9860273911513\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2443.3059691965204\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2440.6777703171174\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2438.1001525837582\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2435.571875041618\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2588.636529655364\n",
      " Regularized Logistic Regression GD (200/3999): loss=2579.2933592643394\n",
      " Regularized Logistic Regression GD (300/3999): loss=2570.9712029979282\n",
      " Regularized Logistic Regression GD (400/3999): loss=2563.4513184785915\n",
      " Regularized Logistic Regression GD (500/3999): loss=2556.569136973761\n",
      " Regularized Logistic Regression GD (600/3999): loss=2550.2005535729477\n",
      " Regularized Logistic Regression GD (700/3999): loss=2544.251458797344\n",
      " Regularized Logistic Regression GD (800/3999): loss=2538.6499062411804\n",
      " Regularized Logistic Regression GD (900/3999): loss=2533.340378583763\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2528.279619167637\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2523.4335973809457\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2518.7752865843017\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2514.2830240446247\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2509.9392897059306\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2505.7297886008346\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2501.6427552978985\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2497.668422197202\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2493.7986098443675\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2490.026408924033\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2486.3459317268266\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2482.75211668858\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2479.2405737807076\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2475.8074615679975\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2472.449388976294\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2469.1633364583026\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2465.946592473294\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2462.7967021180707\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2459.711425445217\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2456.688703536817\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2453.7266308112576\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2450.823432356662\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2447.977445330684\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2445.187103658916\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2442.4509254154746\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2439.7675023893808\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2437.135491435393\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2434.553607284223\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2432.0206165476206\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2429.5353327035104\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2587.984281086871\n",
      " Regularized Logistic Regression GD (200/3999): loss=2578.348033631446\n",
      " Regularized Logistic Regression GD (300/3999): loss=2569.895810514354\n",
      " Regularized Logistic Regression GD (400/3999): loss=2562.334293179368\n",
      " Regularized Logistic Regression GD (500/3999): loss=2555.4583279973504\n",
      " Regularized Logistic Regression GD (600/3999): loss=2549.1218512608675\n",
      " Regularized Logistic Regression GD (700/3999): loss=2543.2189384567578\n",
      " Regularized Logistic Regression GD (800/3999): loss=2537.6712807632243\n",
      " Regularized Logistic Regression GD (900/3999): loss=2532.4199324067995\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2527.4197978050665\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2522.6358749336255\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2518.0406413606493\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2513.612199041946\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2509.332933805199\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2505.1885311467713\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2501.167243362648\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2497.2593369917795\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2493.456671600699\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2489.7523755506245\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2486.140594256358\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2482.616293228335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Regularized Logistic Regression GD (2200/3999): loss=2479.175102924784\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2475.8131957979526\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2472.5271883282107\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2469.314062593053\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2466.1711032068556\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2463.0958464256028\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2460.0860389291565\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2457.139604338269\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2454.254615939097\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2451.4292744076824\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2448.6618895745396\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2445.950865463219\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2443.294687987921\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2440.6919148157563\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2438.1411669937074\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2435.641122016764\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2433.190508073972\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2430.7880992585365\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2590.424750726416\n",
      " Regularized Logistic Regression GD (200/3999): loss=2582.528212095526\n",
      " Regularized Logistic Regression GD (300/3999): loss=2575.3691319134336\n",
      " Regularized Logistic Regression GD (400/3999): loss=2568.781876182423\n",
      " Regularized Logistic Regression GD (500/3999): loss=2562.6462820090624\n",
      " Regularized Logistic Regression GD (600/3999): loss=2556.874419948102\n",
      " Regularized Logistic Regression GD (700/3999): loss=2551.4012689454394\n",
      " Regularized Logistic Regression GD (800/3999): loss=2546.1781483135296\n",
      " Regularized Logistic Regression GD (900/3999): loss=2541.168137962651\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2536.3428778396446\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2531.680311488415\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2527.163076825239\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2522.777344374265\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2518.511968464803\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2514.3578601027266\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2510.3075188514495\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2506.354680158865\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2502.494047453201\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2498.7210871322436\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2495.0318706591734\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2491.4229522438727\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2487.8912736136986\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2484.4340895481505\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2481.048909425435\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2477.7334511832423\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2474.4856049486825\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2471.303404229919\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2468.185003041068\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2465.1286576960733\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2462.132712284525\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2459.195587056673\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2456.315769109501\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2453.4918048948316\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2450.7222941703476\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2448.005885093809\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2445.3412702223686\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2442.7271832285505\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2440.162396183009\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2437.645717285161\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2590.0470056927397\n",
      " Regularized Logistic Regression GD (200/3999): loss=2582.0239280234755\n",
      " Regularized Logistic Regression GD (300/3999): loss=2574.771629973504\n",
      " Regularized Logistic Regression GD (400/3999): loss=2568.1048816104876\n",
      " Regularized Logistic Regression GD (500/3999): loss=2561.897092899733\n",
      " Regularized Logistic Regression GD (600/3999): loss=2556.0571244636462\n",
      " Regularized Logistic Regression GD (700/3999): loss=2550.518351067026\n",
      " Regularized Logistic Regression GD (800/3999): loss=2545.2313511097896\n",
      " Regularized Logistic Regression GD (900/3999): loss=2540.1589189346178\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2535.272640053718\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2530.5505129082744\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2525.9752744686484\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2521.5332044392485\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2517.213259469581\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2513.006438289792\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2508.9053107971577\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2504.9036651517263\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2500.9962408983497\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2497.1785255316986\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2493.4465983415394\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2489.7970098262736\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2486.2266880882758\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2482.7328658493025\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2479.313023326534\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2475.9648433765906\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2472.686176174084\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2469.4750113301398\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2466.329455835125\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2463.247716572288\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2460.228086425842\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2457.268933218825\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2454.368690880446\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2451.5258523702105\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2448.7389639859907\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2446.0066207613054\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2443.327462719239\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2440.7001717989733\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2438.1234693096853\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2435.596113797534\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2588.6365781188647\n",
      " Regularized Logistic Regression GD (200/3999): loss=2579.2935377685267\n",
      " Regularized Logistic Regression GD (300/3999): loss=2570.9715754140448\n",
      " Regularized Logistic Regression GD (400/3999): loss=2563.4519367916573\n",
      " Regularized Logistic Regression GD (500/3999): loss=2556.5700451258867\n",
      " Regularized Logistic Regression GD (600/3999): loss=2550.2017899664284\n",
      " Regularized Logistic Regression GD (700/3999): loss=2544.253057925104\n",
      " Regularized Logistic Regression GD (800/3999): loss=2538.6518997566304\n",
      " Regularized Logistic Regression GD (900/3999): loss=2533.3427960086283\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2528.2824883633643\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2523.4369448639354\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2518.7791377371377\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2514.287403257007\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2509.9442204685915\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2505.735293567438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Regularized Logistic Regression GD (1600/3999): loss=2501.6488563256175\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2497.6751403732414\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2493.805965503211\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2490.034421659173\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2486.3546203992023\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2482.761499433342\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2479.250668013601\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2475.8182839919846\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2472.4609555887628\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2469.1756625594503\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2465.9596926755094\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2462.810590356585\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2459.726114989824\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2456.704207004712\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2453.7429601808512\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2450.840598982244\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2447.9954599578673\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2445.20597644065\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2442.4706659285503\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2439.7881196513194\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2437.156993921597\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2434.576002945247\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2432.0439128265466\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2429.5595365531976\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2587.984332130015\n",
      " Regularized Logistic Regression GD (200/3999): loss=2578.3482187326345\n",
      " Regularized Logistic Regression GD (300/3999): loss=2569.89619271367\n",
      " Regularized Logistic Regression GD (400/3999): loss=2562.3349230131794\n",
      " Regularized Logistic Regression GD (500/3999): loss=2555.4592479406842\n",
      " Regularized Logistic Regression GD (600/3999): loss=2549.1230983846235\n",
      " Regularized Logistic Regression GD (700/3999): loss=2543.220546065788\n",
      " Regularized Logistic Regression GD (800/3999): loss=2537.6732794384625\n",
      " Regularized Logistic Regression GD (900/3999): loss=2532.4223506834533\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2527.4226626202294\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2522.6392119240354\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2518.044475060907\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2513.616553016119\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2509.337830733462\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2505.1939928818347\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2501.173290965955\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2497.2659907564157\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2493.4639510658376\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2489.760299510849\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2486.1491807682078\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2482.6255596155256\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2479.1850657834298\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2475.8238710022874\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2472.538591037271\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2469.326207258653\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2466.1840035830155\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2463.109515579271\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2460.100489252251\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2457.154847561096\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2454.270663144852\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2451.4461360479613\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2448.6795754857076\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2445.969384883509\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2443.3140495751095\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2440.7121266653103\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2438.162236657212\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2435.6630565205437\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2433.2133139378366\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2430.8117825144823\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2590.4248401505693\n",
      " Regularized Logistic Regression GD (200/3999): loss=2582.5285448626832\n",
      " Regularized Logistic Regression GD (300/3999): loss=2575.3698338404915\n",
      " Regularized Logistic Regression GD (400/3999): loss=2568.7830545809184\n",
      " Regularized Logistic Regression GD (500/3999): loss=2562.6480316976604\n",
      " Regularized Logistic Regression GD (600/3999): loss=2556.876827000788\n",
      " Regularized Logistic Regression GD (700/3999): loss=2551.404413048723\n",
      " Regularized Logistic Regression GD (800/3999): loss=2546.1821042695456\n",
      " Regularized Logistic Regression GD (900/3999): loss=2541.1729766597114\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2536.3486668848723\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2531.6871156227244\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2527.1709581970376\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2522.7863627210236\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2518.5221812349987\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2514.369322539581\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2510.3202840529043\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2506.3687991234306\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2502.509569118123\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2498.7380584082716\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2495.0503364646306\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2491.442955539728\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2487.9128554406334\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2484.45728906595\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2481.0737639551007\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2477.7599962514814\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2474.5138743347293\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2471.333430014191\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2468.216815655517\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2465.1622859755375\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2462.168183519068\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2459.2329270442287\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2456.3550022087684\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2453.5329540778102\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2450.76538107456\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2448.050930073738\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2445.388292400071\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2442.7762005431487\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2440.213425439005\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2437.698774199553\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2590.047097917152\n",
      " Regularized Logistic Regression GD (200/3999): loss=2582.024267470506\n",
      " Regularized Logistic Regression GD (300/3999): loss=2574.7723434674535\n",
      " Regularized Logistic Regression GD (400/3999): loss=2568.1060774656194\n",
      " Regularized Logistic Regression GD (500/3999): loss=2561.898866953554\n",
      " Regularized Logistic Regression GD (600/3999): loss=2556.0595638578984\n",
      " Regularized Logistic Regression GD (700/3999): loss=2550.5215365927156\n",
      " Regularized Logistic Regression GD (800/3999): loss=2545.235358683375\n",
      " Regularized Logistic Regression GD (900/3999): loss=2540.163820541714\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2535.278504359565\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2530.5574056557834\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Regularized Logistic Regression GD (1200/3999): loss=2525.983258739568\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2521.5423408267957\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2517.223606194013\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2513.0180512779925\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2508.9182437400273\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2504.9179695487783\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2501.0119660958026\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2497.1957187580156\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2493.465304743084\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2489.8172725046707\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2486.248548140315\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2482.7563624098098\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2479.3381936141673\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2475.9917227424603\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2472.714798152927\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2469.505407693554\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2466.361656646406\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2463.2817502425733\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2460.2639797709908\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2457.3067115166523\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2454.4083779279435\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2451.5674705404344\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2448.7825342842652\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2446.052162880542\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2443.374995094126\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2440.7497116588934\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2438.175032730189\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2435.6497157503172\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2588.6366853295526\n",
      " Regularized Logistic Regression GD (200/3999): loss=2579.2939326507117\n",
      " Regularized Logistic Regression GD (300/3999): loss=2570.972399255085\n",
      " Regularized Logistic Regression GD (400/3999): loss=2563.4533045811527\n",
      " Regularized Logistic Regression GD (500/3999): loss=2556.572054058605\n",
      " Regularized Logistic Regression GD (600/3999): loss=2550.204524979693\n",
      " Regularized Logistic Regression GD (700/3999): loss=2544.256595306236\n",
      " Regularized Logistic Regression GD (800/3999): loss=2538.6563095097545\n",
      " Regularized Logistic Regression GD (900/3999): loss=2533.3481434205396\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2528.288835046485\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2523.4443494538923\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2518.787656359758\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2514.2970898405415\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2509.955126951243\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2505.747470034498\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2501.662351099306\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2497.690000071251\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2493.822235077169\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2490.0521444205106\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2486.37383803788\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2482.7822520329996\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2479.2729940656654\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2475.8422204102617\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2472.4865377257374\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2469.20292422464\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2465.9886661564387\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2462.8413064422853\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2459.7586029968897\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2456.738494805762\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2453.7790742351076\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2450.878564368069\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2448.035300406932\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2445.247714373399\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2442.514322490796\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2439.8337147516236\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2437.2045462692254\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2434.6255300884395\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2432.095431190935\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2429.613061480135\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2587.984445047378\n",
      " Regularized Logistic Regression GD (200/3999): loss=2578.3486282085396\n",
      " Regularized Logistic Regression GD (300/3999): loss=2569.8970381967374\n",
      " Regularized Logistic Regression GD (400/3999): loss=2562.3363162883124\n",
      " Regularized Logistic Regression GD (500/3999): loss=2555.461282957134\n",
      " Regularized Logistic Regression GD (600/3999): loss=2549.125857134667\n",
      " Regularized Logistic Regression GD (700/3999): loss=2543.2241022087937\n",
      " Regularized Logistic Regression GD (800/3999): loss=2537.6777006063444\n",
      " Regularized Logistic Regression GD (900/3999): loss=2532.4276999809695\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2527.428999615529\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2522.6465933071477\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2518.0529550824663\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2513.626183778459\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2509.3486623824224\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2505.2060737315646\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2501.1866675790575\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2497.2807079947274\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2493.4800521224615\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2489.7778259307925\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2486.1681724628947\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2482.646054874583\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2479.207101286139\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2475.847481830301\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2472.5638106895994\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2469.353067669348\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2466.212535141941\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2463.1397471559394\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2460.132448222687\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2457.1885598374265\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2454.306153207435\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2451.483426979761\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2448.718689008616\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2446.0103413961497\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2443.3568681920633\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2440.7568252571727\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2438.2088318914875\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2435.7115639028693\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2433.2637478534466\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2430.8641562696116\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2590.425037971841\n",
      " Regularized Logistic Regression GD (200/3999): loss=2582.5292809837615\n",
      " Regularized Logistic Regression GD (300/3999): loss=2575.371386554683\n",
      " Regularized Logistic Regression GD (400/3999): loss=2568.7856612298147\n",
      " Regularized Logistic Regression GD (500/3999): loss=2562.65190197411\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Regularized Logistic Regression GD (600/3999): loss=2556.8821512420977\n",
      " Regularized Logistic Regression GD (700/3999): loss=2551.411367452\n",
      " Regularized Logistic Regression GD (800/3999): loss=2546.190854217544\n",
      " Regularized Logistic Regression GD (900/3999): loss=2541.183678868424\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2536.3614708033538\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2531.702164351627\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2527.188389094393\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2522.8063078044243\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2518.544767451654\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2514.3946719519504\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2510.3485139717777\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2506.4000222097798\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2502.5438934679873\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2498.7755876300143\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2495.0911697547153\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2491.4871877606442\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2487.9605772032437\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2484.508586817131\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2481.1287200711363\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2477.818689136869\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2474.576378525982\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2471.3998162878975\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2468.287151139771\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2465.2366342641253\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2462.246604787235\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2459.315478165557\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2456.4417368721474\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2453.6239229041403\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2450.860631732254\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2448.1505073927806\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2445.492238484039\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2442.8845548789213\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2440.3262250038388\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2437.816053565205\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2590.0473019330616\n",
      " Regularized Logistic Regression GD (200/3999): loss=2582.0250183683265\n",
      " Regularized Logistic Regression GD (300/3999): loss=2574.7739217686217\n",
      " Regularized Logistic Regression GD (400/3999): loss=2568.1087227293096\n",
      " Regularized Logistic Regression GD (500/3999): loss=2561.9027911257735\n",
      " Regularized Logistic Regression GD (600/3999): loss=2556.064959636976\n",
      " Regularized Logistic Regression GD (700/3999): loss=2550.528582618052\n",
      " Regularized Logistic Regression GD (800/3999): loss=2545.244222801523\n",
      " Regularized Logistic Regression GD (900/3999): loss=2540.1746618946313\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2535.291474735483\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2530.572650370201\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2526.000917213483\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2521.562546968849\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2517.2464886567395\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2513.043733634167\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2508.9468446101105\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2504.949602700006\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2501.0467405268573\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2497.233738778036\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2493.5066700510583\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2489.8620782723965\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2486.2968851011183\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2482.8083169536626\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2479.393847889467\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2476.0511547639308\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2472.7780819150844\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2469.5726132887276\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2466.4328503864867\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2463.356994783372\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2460.343334238335\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2457.3902316332096\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2454.496116140037\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2451.659476144069\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2448.87885354962\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2446.152839174553\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2443.480069000482\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2440.8592210952224\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2438.289013062102\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2435.7681999019815\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2588.636922497633\n",
      " Regularized Logistic Regression GD (200/3999): loss=2579.2948061778084\n",
      " Regularized Logistic Regression GD (300/3999): loss=2570.9742216524737\n",
      " Regularized Logistic Regression GD (400/3999): loss=2563.456330169542\n",
      " Regularized Logistic Regression GD (500/3999): loss=2556.576497781333\n",
      " Regularized Logistic Regression GD (600/3999): loss=2550.2105746546326\n",
      " Regularized Logistic Regression GD (700/3999): loss=2544.264419606881\n",
      " Regularized Logistic Regression GD (800/3999): loss=2538.6660632019543\n",
      " Regularized Logistic Regression GD (900/3999): loss=2533.3599708270335\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2528.3028723543684\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2523.460726258049\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2518.8064966964043\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2514.3185128779787\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2509.979247449414\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2505.7743986485107\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2501.6921945784925\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2497.7228613890643\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2493.85821351648\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2490.091335631197\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2486.4163340785417\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2482.828141404856\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2479.3223617440317\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2475.895147877236\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2472.5431030064137\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2469.2632019282087\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2466.052727522768\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2462.909219394231\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2459.8304321982455\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2456.814301724408\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2453.858917211054\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2450.962498685567\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2448.1233783702723\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2445.339985385654\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2442.610833134988\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2439.9345088735586\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2437.30966506169\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2434.735012175981\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2432.2093127149897\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2429.731376183995\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2587.984694839619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Regularized Logistic Regression GD (200/3999): loss=2578.3495340188633\n",
      " Regularized Logistic Regression GD (300/3999): loss=2569.8989084683053\n",
      " Regularized Logistic Regression GD (400/3999): loss=2562.3393982524576\n",
      " Regularized Logistic Regression GD (500/3999): loss=2555.465784378136\n",
      " Regularized Logistic Regression GD (600/3999): loss=2549.1319593161857\n",
      " Regularized Logistic Regression GD (700/3999): loss=2543.2319680121927\n",
      " Regularized Logistic Regression GD (800/3999): loss=2537.687479551324\n",
      " Regularized Logistic Regression GD (900/3999): loss=2532.4395315652114\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2527.443015506028\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2522.662918797487\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2518.071710063346\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2513.647483382112\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2509.372617407041\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2505.2327909177775\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2501.216249787511\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2497.313254318623\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2493.515657958733\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2489.8165830262155\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2486.210168946397\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2482.691375283649\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2479.2558265925545\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2475.8996894685806\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2472.6195745903806\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2469.4124582989534\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2466.2756195485736\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2463.206589022743\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2460.203107927394\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2457.2630945181477\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2454.3846168340133\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2451.5658704300536\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2448.8051601489883\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2446.100885165481\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2443.451526688559\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2440.8556378273397\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2438.3118352206557\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2435.8187921065432\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2433.375232569237\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2430.9799267490093\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2590.4254755742777\n",
      " Regularized Logistic Regression GD (200/3999): loss=2582.530909286917\n",
      " Regularized Logistic Regression GD (300/3999): loss=2575.374821003565\n",
      " Regularized Logistic Regression GD (400/3999): loss=2568.791426607519\n",
      " Regularized Logistic Regression GD (500/3999): loss=2562.660461841201\n",
      " Regularized Logistic Regression GD (600/3999): loss=2556.8939262891377\n",
      " Regularized Logistic Regression GD (700/3999): loss=2551.426747041171\n",
      " Regularized Logistic Regression GD (800/3999): loss=2546.21020374302\n",
      " Regularized Logistic Regression GD (900/3999): loss=2541.2073445096685\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2536.3897826239554\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2531.7354383331963\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2527.226928478554\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2522.8504039843415\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2518.5947005714074\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2514.450711319212\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2510.4109183541345\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2506.4690400619947\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2502.6197631216887\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2498.858537466665\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2495.181418375082\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2491.5849441591286\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2488.066040951773\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2484.6219482608562\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2481.250160536651\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2477.9483811524547\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2474.7144860527387\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2471.5464949602065\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2468.4425485132815\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2465.4008900693766\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2462.419851187176\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2459.4978400150194\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2456.633331977855\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2453.824862283562\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2451.071019870204\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2448.3704424943962\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2445.7218127234505\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2443.1238546425993\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2440.5753311279723\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2438.075041566786\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2590.0477532388027\n",
      " Regularized Logistic Regression GD (200/3999): loss=2582.026679358114\n",
      " Regularized Logistic Regression GD (300/3999): loss=2574.777412814188\n",
      " Regularized Logistic Regression GD (400/3999): loss=2568.114573516383\n",
      " Regularized Logistic Regression GD (500/3999): loss=2561.911470195403\n",
      " Regularized Logistic Regression GD (600/3999): loss=2556.0768928981024\n",
      " Regularized Logistic Regression GD (700/3999): loss=2550.544164830062\n",
      " Regularized Logistic Regression GD (800/3999): loss=2545.263824802648\n",
      " Regularized Logistic Regression GD (900/3999): loss=2540.198635223844\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2535.320154623163\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2530.6063576891174\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2526.0399597596715\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2521.6072203093718\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2517.2970766983167\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2513.1005090154667\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2509.010068985628\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2505.019526961568\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2501.1236049928043\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2497.317773370251\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2493.598094472403\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2489.9611021924447\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2486.403708354161\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2482.923129751077\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2479.516831046521\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2476.1824799402507\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2472.9179118671263\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2469.721102132443\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2466.590143867989\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2463.5232305553486\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2460.5186421401186\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2457.5747339725463\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2454.689927974614\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2451.8627055611596\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2449.0916019424776\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2446.3752015142163\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2443.7121341020716\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Regularized Logistic Regression GD (3700/3999): loss=2441.101071877761\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2438.5407268013882\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2436.0298484758077\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2588.6374471395716\n",
      " Regularized Logistic Regression GD (200/3999): loss=2579.2967384242183\n",
      " Regularized Logistic Regression GD (300/3999): loss=2570.978252616221\n",
      " Regularized Logistic Regression GD (400/3999): loss=2563.4630221637844\n",
      " Regularized Logistic Regression GD (500/3999): loss=2556.5863259529365\n",
      " Regularized Logistic Regression GD (600/3999): loss=2550.2239540963574\n",
      " Regularized Logistic Regression GD (700/3999): loss=2544.2817230183305\n",
      " Regularized Logistic Regression GD (800/3999): loss=2538.687632477575\n",
      " Regularized Logistic Regression GD (900/3999): loss=2533.3861247223217\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2528.333911592426\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2523.4969369453147\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2518.8481526102805\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2514.3658769963545\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2510.032572968449\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2505.833929655578\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2501.758166493927\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2497.7955012550065\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2493.937740187022\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2490.177959901475\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2486.510258778524\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2482.9295614759035\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2479.4314643100925\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2476.0121123188064\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2472.668101042215\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2469.3963977075437\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2466.1942777300583\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2463.059273366233\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2459.989132053531\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2456.9817825045143\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2454.0353070320552\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2451.147918898994\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2448.317943731818\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2445.5438042304922\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2442.8240075582153\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2440.157134914534\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2437.541832890957\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2434.976806283855\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2432.460812100521\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2429.992654543575\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2587.9852474076506\n",
      " Regularized Logistic Regression GD (200/3999): loss=2578.3515376765745\n",
      " Regularized Logistic Regression GD (300/3999): loss=2569.9030453270384\n",
      " Regularized Logistic Regression GD (400/3999): loss=2562.346214942801\n",
      " Regularized Logistic Regression GD (500/3999): loss=2555.4757401682054\n",
      " Regularized Logistic Regression GD (600/3999): loss=2549.145454892882\n",
      " Regularized Logistic Regression GD (700/3999): loss=2543.249363224105\n",
      " Regularized Logistic Regression GD (800/3999): loss=2537.70910469571\n",
      " Regularized Logistic Regression GD (900/3999): loss=2532.465694733637\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2527.4740074337224\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2522.6990160883784\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2518.1131773384236\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2513.6945747060445\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2509.4255772285323\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2505.291854682397\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2501.281644331929\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2497.385198121151\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2493.594361298239\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2489.9022480856165\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2486.3029898858886\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2482.7915383006602\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2479.3635099802477\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2476.015063680479\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2472.742802315789\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2469.543694551945\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2466.4150117718086\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2463.3542772062747\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2460.35922474202\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2457.427765462289\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2454.557960392868\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2451.747998245531\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2448.9961771989183\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2446.300889950534\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2443.660611425295\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2441.073888645948\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2438.5393323659623\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2436.055610141139\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2433.6214405773403\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2431.2355885401716\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2590.4264435426876\n",
      " Regularized Logistic Regression GD (200/3999): loss=2582.534510682955\n",
      " Regularized Logistic Regression GD (300/3999): loss=2575.3824163464346\n",
      " Regularized Logistic Regression GD (400/3999): loss=2568.8041755208915\n",
      " Regularized Logistic Regression GD (500/3999): loss=2562.679388230056\n",
      " Regularized Logistic Regression GD (600/3999): loss=2556.919958970237\n",
      " Regularized Logistic Regression GD (700/3999): loss=2551.4607452955843\n",
      " Regularized Logistic Regression GD (800/3999): loss=2546.2529736133934\n",
      " Regularized Logistic Regression GD (900/3999): loss=2541.2596493270858\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2536.452349873459\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2531.8089641624356\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2527.3120806760458\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2522.947823960963\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2518.705004695762\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2514.574491840034\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2510.548744065423\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2506.621456815459\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2502.7872942560925\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2499.041684199877\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2495.3806601884257\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2491.8007391922333\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2488.29882641819\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2484.872140889665\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2481.5181570412024\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2478.2345587255054\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2475.019202885136\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2471.8700907799175\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2468.7853451405513\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2465.763191984048\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2462.801946104064\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2459.8999994633996\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2457.055811881523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Regularized Logistic Regression GD (3300/3999): loss=2454.267903538178\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2451.534848915089\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2448.855271876266\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2446.2278416499516\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2443.6512695239617\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2441.1243061053124\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2438.6457390259525\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2590.0487515190757\n",
      " Regularized Logistic Regression GD (200/3999): loss=2582.0303530506994\n",
      " Regularized Logistic Regression GD (300/3999): loss=2574.7851333256262\n",
      " Regularized Logistic Regression GD (400/3999): loss=2568.1275113005963\n",
      " Regularized Logistic Regression GD (500/3999): loss=2561.9306601561725\n",
      " Regularized Logistic Regression GD (600/3999): loss=2556.1032753730365\n",
      " Regularized Logistic Regression GD (700/3999): loss=2550.578611012524\n",
      " Regularized Logistic Regression GD (800/3999): loss=2545.307152747074\n",
      " Regularized Logistic Regression GD (900/3999): loss=2540.2516200823693\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2535.3835352718575\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2530.6808410538756\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2526.12622365979\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2521.705915342318\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2517.4088275079666\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2513.2259151697485\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2509.149705614975\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2505.1739452589723\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2501.2933326425787\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2497.5033149497917\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2493.799931854704\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2490.1796949664417\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2486.6394942722936\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2483.176525207995\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2479.788231589447\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2476.4722608090256\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2473.2264285605006\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2470.048690996369\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2466.937122701012\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2463.8898992264762\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2460.905283214225\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2457.9816133390545\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2455.1172954751914\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2452.310795612959\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2449.5606341536923\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2446.865381289204\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2444.2236532338925\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2441.6341091261947\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2439.095448455072\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2436.6064088974395\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2588.6386076378444\n",
      " Regularized Logistic Regression GD (200/3999): loss=2579.301012068557\n",
      " Regularized Logistic Regression GD (300/3999): loss=2570.9871671742562\n",
      " Regularized Logistic Regression GD (400/3999): loss=2563.477820130133\n",
      " Regularized Logistic Regression GD (500/3999): loss=2556.6080567227687\n",
      " Regularized Logistic Regression GD (600/3999): loss=2550.2535339842525\n",
      " Regularized Logistic Regression GD (700/3999): loss=2544.319974352813\n",
      " Regularized Logistic Regression GD (800/3999): loss=2538.735309251554\n",
      " Regularized Logistic Regression GD (900/3999): loss=2533.443929563445\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2528.402507067759\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2523.576953124598\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2518.9401921331937\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2514.470518462267\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2510.150373194636\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2505.9654251725974\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2501.903874529754\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2497.955920381341\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2494.113350747512\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2490.36922430475\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2486.717621713609\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2483.1534500892517\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2479.6722883709704\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2476.2702643905845\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2472.943956670617\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2469.690315632411\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2466.506600123906\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2463.390326100411\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2460.3392249915364\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2457.351209820519\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2454.4243475524945\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2451.5568364642154\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2448.7469875749534\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2445.993209370416\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2443.2939952035326\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2440.647912875964\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2438.0535959991557\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2435.509736810378\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2433.015080179604\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2430.5684185927985\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2587.986469678329\n",
      " Regularized Logistic Regression GD (200/3999): loss=2578.3559692679983\n",
      " Regularized Logistic Regression GD (300/3999): loss=2569.9121940839436\n",
      " Regularized Logistic Regression GD (400/3999): loss=2562.361288669655\n",
      " Regularized Logistic Regression GD (500/3999): loss=2555.497753147204\n",
      " Regularized Logistic Regression GD (600/3999): loss=2549.175291593824\n",
      " Regularized Logistic Regression GD (700/3999): loss=2543.287817579273\n",
      " Regularized Logistic Regression GD (800/3999): loss=2537.7569050840166\n",
      " Regularized Logistic Regression GD (900/3999): loss=2532.523520240881\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2527.5424985881114\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2522.778782001409\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2518.2048004635067\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2513.7986140004946\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2509.54257023991\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2505.422318910959\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2501.4260781094918\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2497.5440811725316\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2493.7681551003475\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2490.0913961041683\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2486.507917745029\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2483.012653923747\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2479.601203727165\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2476.2697084984266\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2473.014753913911\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2469.833291605876\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2466.7225761606737\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Regularized Logistic Regression GD (2700/3999): loss=2463.680114282717\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2460.703623634016\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2457.7909994046454\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2454.940287085583\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2452.1496602356247\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2449.4174022824486\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2446.7418915912326\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2444.121589186541\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2441.5550286326625\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2439.0408076735343\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2436.5775813084824\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2434.1640560414176\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2431.7989850898443\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2590.428584395011\n",
      " Regularized Logistic Regression GD (200/3999): loss=2582.5424740186877\n",
      " Regularized Logistic Regression GD (300/3999): loss=2575.399207144946\n",
      " Regularized Logistic Regression GD (400/3999): loss=2568.8323527175385\n",
      " Regularized Logistic Regression GD (500/3999): loss=2562.721209150627\n",
      " Regularized Logistic Regression GD (600/3999): loss=2556.977469364551\n",
      " Regularized Logistic Regression GD (700/3999): loss=2551.5358359643037\n",
      " Regularized Logistic Regression GD (800/3999): loss=2546.3474165328385\n",
      " Regularized Logistic Regression GD (900/3999): loss=2541.3751209943493\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2536.5904465444337\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2531.9712118732223\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2527.499941877299\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2523.162701947369\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2518.9482466067893\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2514.847390747497\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2510.8525404924667\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2506.9573399177916\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2503.156402819176\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2499.4451075533366\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2495.8194391023503\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2492.275866795695\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2488.8112491623224\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2485.42275956653\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2482.1078278611976\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2478.8640944508556\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2475.6893740139476\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2472.5816267727078\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2469.538935680817\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2466.5594882634455\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2463.6415621233605\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2460.7835133406415\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2457.9837671594696\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2455.2408104839783\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2452.5531858058375\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2449.919486265117\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2447.3383516081803\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2444.808464855147\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2442.3285495287337\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2439.8973673267506\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2590.050959414033\n",
      " Regularized Logistic Regression GD (200/3999): loss=2582.038476256736\n",
      " Regularized Logistic Regression GD (300/3999): loss=2574.8022008502617\n",
      " Regularized Logistic Regression GD (400/3999): loss=2568.156105962962\n",
      " Regularized Logistic Regression GD (500/3999): loss=2561.973063519847\n",
      " Regularized Logistic Regression GD (600/3999): loss=2556.161558560711\n",
      " Regularized Logistic Regression GD (700/3999): loss=2550.6546910432535\n",
      " Regularized Logistic Regression GD (800/3999): loss=2545.4028280157618\n",
      " Regularized Logistic Regression GD (900/3999): loss=2540.368593064011\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2535.523427232093\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2530.84520166774\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2526.3165373503493\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2521.9236054989187\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2517.65525934066\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2513.5023977008873\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2509.45749314174\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2505.514238509987\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2501.6672797769816\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2497.9120124953597\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2494.244425647155\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2490.6609811268063\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2487.1585202428096\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2483.7341908551953\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2480.385390375137\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2477.1097210248113\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2473.904954618216\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2470.7690047646684\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2467.699904877654\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2464.695790735702\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2461.7548866187276\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2458.875494256781\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2456.0559839921493\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2453.2947876840044\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2450.5903929844258\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2447.9413386931096\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2445.346210959898\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2442.803640152733\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2440.3122982475156\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2437.8708966268127\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2588.6411743084836\n",
      " Regularized Logistic Regression GD (200/3999): loss=2579.3104618785233\n",
      " Regularized Logistic Regression GD (300/3999): loss=2571.006874382532\n",
      " Regularized Logistic Regression GD (400/3999): loss=2563.5105262499164\n",
      " Regularized Logistic Regression GD (500/3999): loss=2556.6560747793756\n",
      " Regularized Logistic Regression GD (600/3999): loss=2550.318881491428\n",
      " Regularized Logistic Regression GD (700/3999): loss=2544.404459918799\n",
      " Regularized Logistic Regression GD (800/3999): loss=2538.8405894003645\n",
      " Regularized Logistic Regression GD (900/3999): loss=2533.571546329665\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2528.553913045465\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2523.753528233231\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2519.1432548593075\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2514.70133338989\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2510.4101560331246\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2506.2553460979007\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2502.225060396397\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2498.3094561880184\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2494.500280621233\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2490.7905521868943\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2487.174311874628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Regularized Logistic Regression GD (2100/3999): loss=2483.646427558731\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2480.202439341754\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2476.838436635534\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2473.55095999634\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2470.3369223842137\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2467.193545748764\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2464.1183097700537\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2461.1089102840187\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2458.1632254564774\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2455.279288180534\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2452.45526348947\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2449.689430023931\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2446.980164785443\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2444.3259305602546\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2441.725265517503\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2439.176774581072\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2436.679122250945\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2434.2310266106\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2431.8312543062075\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2587.989172972655\n",
      " Regularized Logistic Regression GD (200/3999): loss=2578.365768343579\n",
      " Regularized Logistic Regression GD (300/3999): loss=2569.932419078932\n",
      " Regularized Logistic Regression GD (400/3999): loss=2562.394604370298\n",
      " Regularized Logistic Regression GD (500/3999): loss=2555.5463949713976\n",
      " Regularized Logistic Regression GD (600/3999): loss=2549.241206725415\n",
      " Regularized Logistic Regression GD (700/3999): loss=2543.372751969242\n",
      " Regularized Logistic Regression GD (800/3999): loss=2537.8624587944964\n",
      " Regularized Logistic Regression GD (900/3999): loss=2532.6511834668063\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2527.693675444359\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2522.9548063519037\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2518.406946478193\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2514.0281031801633\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2509.8005760624637\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2505.709969877622\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2501.7444597232357\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2497.894237215709\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2494.151088465911\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2490.5080693572963\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2486.9592535371708\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2483.499535342883\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2480.1244746406555\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2476.8301739252215\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2473.6131804491274\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2470.470407910377\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2467.399073521699\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2464.396647246078\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2461.4608107054305\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2458.589423815097\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2455.7804976144134\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2453.032172084008\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2450.3426979897354\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2447.710421986362\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2445.133774367073\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2442.6112589642876\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2440.1414448032106\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2437.722959185119\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2435.3544819384156\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2433.034740624283\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2590.433317962059\n",
      " Regularized Logistic Regression GD (200/3999): loss=2582.560072395787\n",
      " Regularized Logistic Regression GD (300/3999): loss=2575.43629463199\n",
      " Regularized Logistic Regression GD (400/3999): loss=2568.894558988062\n",
      " Regularized Logistic Regression GD (500/3999): loss=2562.813490001081\n",
      " Regularized Logistic Regression GD (600/3999): loss=2557.104306634236\n",
      " Regularized Logistic Regression GD (700/3999): loss=2551.7013632995327\n",
      " Regularized Logistic Regression GD (800/3999): loss=2546.5554996621104\n",
      " Regularized Logistic Regression GD (900/3999): loss=2541.629409661394\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2536.894408471108\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2532.3281551658592\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2527.913029934798\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2523.6349634912226\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2519.4825825524176\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2515.446579057552\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2511.5192397738147\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2507.694092271474\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2503.965636281912\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2500.3291383525047\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2496.7804738669156\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2493.316004809679\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2489.9324847102134\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2486.626984393523\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2483.396833753925\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2480.2395759328174\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2477.152931142323\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2474.134768019276\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2471.1830808773802\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2468.2959715916613\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2465.4716351292536\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2462.708347955177\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2460.0044587080565\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2457.358380669623\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2454.768585652544\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2452.2335990100714\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2449.751995532998\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2447.3223960484897\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2444.9434645739602\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2442.613905909981\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2590.055841226497\n",
      " Regularized Logistic Regression GD (200/3999): loss=2582.056427982126\n",
      " Regularized Logistic Regression GD (300/3999): loss=2574.839899665908\n",
      " Regularized Logistic Regression GD (400/3999): loss=2568.2192340114984\n",
      " Regularized Logistic Regression GD (500/3999): loss=2562.066629759912\n",
      " Regularized Logistic Regression GD (600/3999): loss=2556.290100407765\n",
      " Regularized Logistic Regression GD (700/3999): loss=2550.8223994914456\n",
      " Regularized Logistic Regression GD (800/3999): loss=2545.6136264883303\n",
      " Regularized Logistic Regression GD (900/3999): loss=2540.626187934591\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2535.8313406288426\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2531.206792998659\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2526.7350175681663\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2522.4020466756047\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2518.1966012279454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Regularized Logistic Regression GD (1500/3999): loss=2514.109452384348\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2510.1329485265255\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2506.2606611300903\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2502.4871172540343\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2498.807595859768\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2495.2179716560504\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2491.714594659763\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2488.294196818672\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2484.95381928814\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2481.69075557084\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2478.5025069061167\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2475.386747163001\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2472.3412951343203\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2469.364092613009\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2466.453186996736\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2463.6067174454747\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2460.8229038300537\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2458.1000378750514\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2455.4364760270137\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2452.8306336794853\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2450.2809804643525\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2447.7860363808145\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2445.344368581765\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2442.954588676018\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2440.615350434813\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2588.6468493914567\n",
      " Regularized Logistic Regression GD (200/3999): loss=2579.33134531146\n",
      " Regularized Logistic Regression GD (300/3999): loss=2571.0504039240727\n",
      " Regularized Logistic Regression GD (400/3999): loss=2563.582731812713\n",
      " Regularized Logistic Regression GD (500/3999): loss=2556.7620320175643\n",
      " Regularized Logistic Regression GD (600/3999): loss=2550.4630069668083\n",
      " Regularized Logistic Regression GD (700/3999): loss=2544.5907032905297\n",
      " Regularized Logistic Regression GD (800/3999): loss=2539.072559508475\n",
      " Regularized Logistic Regression GD (900/3999): loss=2533.8525946287195\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2528.8871892223196\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2524.1420174708696\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2519.5898028677516\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2515.2086632246187\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2510.980879379995\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2506.8919708328203\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2502.929995652294\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2499.085015776585\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2495.348685394104\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2491.7139317408105\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2488.1747058793735\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2484.7257868982374\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2481.3626271969006\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2478.0812295926225\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2474.8780492338365\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2471.749914968079\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2468.6939660509743\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2465.7076010137503\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2462.7884362112677\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2459.93427210962\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2457.143065784958\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2454.4129084236993\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2451.7420068622387\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2449.1286683981007\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2446.5712882567855\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2444.0683392190795\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2441.61836300934\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2439.219963121276\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2436.8717988193494\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2434.572580102479\n",
      " Regularized Logistic Regression GD (0/3999): loss=2599.3019270998116\n",
      " Regularized Logistic Regression GD (100/3999): loss=2587.9951501490436\n",
      " Regularized Logistic Regression GD (200/3999): loss=2578.3874237064038\n",
      " Regularized Logistic Regression GD (300/3999): loss=2569.977092552423\n",
      " Regularized Logistic Regression GD (400/3999): loss=2562.4681562058145\n",
      " Regularized Logistic Regression GD (500/3999): loss=2555.653729480987\n",
      " Regularized Logistic Regression GD (600/3999): loss=2549.3865854607857\n",
      " Regularized Logistic Regression GD (700/3999): loss=2543.559986764536\n",
      " Regularized Logistic Regression GD (800/3999): loss=2538.0950345671217\n",
      " Regularized Logistic Regression GD (900/3999): loss=2532.9323381575728\n",
      " Regularized Logistic Regression GD (1000/3999): loss=2528.026452831505\n",
      " Regularized Logistic Regression GD (1100/3999): loss=2523.342091230861\n",
      " Regularized Logistic Regression GD (1200/3999): loss=2518.851488167541\n",
      " Regularized Logistic Regression GD (1300/3999): loss=2514.5325312128534\n",
      " Regularized Logistic Regression GD (1400/3999): loss=2510.367410620048\n",
      " Regularized Logistic Regression GD (1500/3999): loss=2506.3416286921424\n",
      " Regularized Logistic Regression GD (1600/3999): loss=2502.4432626245093\n",
      " Regularized Logistic Regression GD (1700/3999): loss=2498.6624091354925\n",
      " Regularized Logistic Regression GD (1800/3999): loss=2494.990761460807\n",
      " Regularized Logistic Regression GD (1900/3999): loss=2491.4212840395903\n",
      " Regularized Logistic Regression GD (2000/3999): loss=2487.947960182195\n",
      " Regularized Logistic Regression GD (2100/3999): loss=2484.5655948564295\n",
      " Regularized Logistic Regression GD (2200/3999): loss=2481.2696595095576\n",
      " Regularized Logistic Regression GD (2300/3999): loss=2478.0561692310393\n",
      " Regularized Logistic Regression GD (2400/3999): loss=2474.921584994144\n",
      " Regularized Logistic Regression GD (2500/3999): loss=2471.8627354832142\n",
      " Regularized Logistic Regression GD (2600/3999): loss=2468.8767543141\n",
      " Regularized Logistic Regression GD (2700/3999): loss=2465.961029421976\n",
      " Regularized Logistic Regression GD (2800/3999): loss=2463.1131621154154\n",
      " Regularized Logistic Regression GD (2900/3999): loss=2460.330933845024\n",
      " Regularized Logistic Regression GD (3000/3999): loss=2457.612279153383\n",
      " Regularized Logistic Regression GD (3100/3999): loss=2454.9552635955524\n",
      " Regularized Logistic Regression GD (3200/3999): loss=2452.3580656689746\n",
      " Regularized Logistic Regression GD (3300/3999): loss=2449.818961986306\n",
      " Regularized Logistic Regression GD (3400/3999): loss=2447.336315077234\n",
      " Regularized Logistic Regression GD (3500/3999): loss=2444.908563326032\n",
      " Regularized Logistic Regression GD (3600/3999): loss=2442.534212647081\n",
      " Regularized Logistic Regression GD (3700/3999): loss=2440.2118295765854\n",
      " Regularized Logistic Regression GD (3800/3999): loss=2437.940035519889\n",
      " Regularized Logistic Regression GD (3900/3999): loss=2435.717501942502\n",
      "Best lambda from accuracy: 1.00e-07\n",
      "Best lambda from error: 1.00e-07\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cross_validation_demo()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
