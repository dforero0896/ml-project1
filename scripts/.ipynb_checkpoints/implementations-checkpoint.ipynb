{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T12:23:10.589467Z",
     "start_time": "2019-10-17T12:23:10.584006Z"
    }
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "from implementations import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T12:23:34.557615Z",
     "start_time": "2019-10-17T12:23:12.077374Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read train_y=LABELS, train_x=FEATURES and train_id=EVENT_IDS from dataset.\n",
    "subsamp = False\n",
    "y, x, id_ = load_csv_data('../data/train.csv', sub_sample=subsamp)\n",
    "y_out_test, x_out_test, id_out_test = load_csv_data('../data/test.csv', sub_sample=subsamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T12:30:16.368300Z",
     "start_time": "2019-10-17T12:30:15.790290Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((250000, 61), (0, 61))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean=False\n",
    "degree = 2\n",
    "feature_expansion = True\n",
    "x_train, y_train, x_test, y_test = split_data(x, y, ratio=1, seed = 42)\n",
    "if clean:\n",
    "    # Clean data\n",
    "    y_train, tx_train = clean_data(y_train, tx_train)\n",
    "# Standardize data\n",
    "x_train_std = standardize_features(x_train)\n",
    "x_test_std = standardize_features(x_test)\n",
    "x_train = x_train_std[0]\n",
    "x_test = x_test_std[0]\n",
    "# Build data matrix\n",
    "tx_train = build_poly(x_train, degree)\n",
    "tx_test = build_poly(x_test, degree)\n",
    "tx_train.shape, tx_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T12:35:02.599156Z",
     "start_time": "2019-10-17T12:30:25.259636Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD (0/4999): loss=0.5\n",
      "GD (100/4999): loss=0.33862756417594736\n",
      "GD (200/4999): loss=0.3307009373822068\n",
      "GD (300/4999): loss=0.32847987671460427\n",
      "GD (400/4999): loss=0.3272745361263504\n",
      "GD (500/4999): loss=0.32640451895899686\n",
      "GD (600/4999): loss=0.32611844138862706\n",
      "GD (700/4999): loss=0.3252543445931449\n",
      "GD (800/4999): loss=0.32504589188847505\n",
      "GD (900/4999): loss=0.32520888640048345\n",
      "GD (1000/4999): loss=0.32502004966239345\n",
      "GD (1100/4999): loss=0.32474911076108604\n",
      "GD (1200/4999): loss=0.3248000216698159\n",
      "GD (1300/4999): loss=0.3248794886322458\n",
      "GD (1400/4999): loss=0.3250758366512016\n",
      "GD (1500/4999): loss=0.32492172025002697\n",
      "GD (1600/4999): loss=0.324773902056429\n",
      "GD (1700/4999): loss=0.3249212691650208\n",
      "GD (1800/4999): loss=0.3248185515624715\n",
      "GD (1900/4999): loss=0.3246765031920483\n",
      "GD (2000/4999): loss=0.32461928195026984\n",
      "GD (2100/4999): loss=0.3245721685170827\n",
      "GD (2200/4999): loss=0.3246012195678137\n",
      "GD (2300/4999): loss=0.3245377643418787\n",
      "GD (2400/4999): loss=0.32439201432540826\n",
      "GD (2500/4999): loss=0.3245105340941744\n",
      "GD (2600/4999): loss=0.32462926668317604\n",
      "GD (2700/4999): loss=0.3246430564285196\n",
      "GD (2800/4999): loss=0.3246012323994996\n",
      "GD (2900/4999): loss=0.32457514541530436\n",
      "GD (3000/4999): loss=0.3246009944876496\n",
      "GD (3100/4999): loss=0.3246153642300266\n",
      "GD (3200/4999): loss=0.32450935503110384\n",
      "GD (3300/4999): loss=0.3244532189030252\n",
      "GD (3400/4999): loss=0.3244839287375769\n",
      "GD (3500/4999): loss=0.3244516870032954\n",
      "GD (3600/4999): loss=0.3244243016362641\n",
      "GD (3700/4999): loss=0.3243850613964313\n",
      "GD (3800/4999): loss=0.3244247774457604\n",
      "GD (3900/4999): loss=0.3245237333345055\n",
      "GD (4000/4999): loss=0.3245144054498204\n",
      "GD (4100/4999): loss=0.32447542659415385\n",
      "GD (4200/4999): loss=0.32451493154033556\n",
      "GD (4300/4999): loss=0.3244977465116549\n",
      "GD (4400/4999): loss=0.324467266536758\n",
      "GD (4500/4999): loss=0.3244246011365547\n",
      "GD (4600/4999): loss=0.32439319544569256\n",
      "GD (4700/4999): loss=0.32442421178920905\n",
      "GD (4800/4999): loss=0.32440048041114006\n",
      "GD (4900/4999): loss=0.3243412714652571\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-033db2ab07f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m                                 accel=True)\n\u001b[1;32m     13\u001b[0m \u001b[0mgd_prediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_gd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0macc_gd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_ratio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgd_prediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Accuracy ratio = %.3f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0macc_gd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test loss = %.3f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_gd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gdrive/EPFL/2019-2020/MachineLearning/Project/ml-project1/scripts/implementations.py\u001b[0m in \u001b[0;36maccuracy_ratio\u001b[0;34m(prediction, labels)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mcount\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcorrect\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbatch_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_batches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \"\"\"\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "w_init = np.array([0] * tx_train.shape[1])\n",
    "max_iter = 5000\n",
    "gamma = 0.0009\n",
    "w_gd, loss_gd = least_squares_GD(y_train,\n",
    "                                 tx_train,\n",
    "                                 w_init,\n",
    "                                 max_iter,\n",
    "                                 gamma,\n",
    "                                 pr=True,\n",
    "                                 adapt_gamma=False,\n",
    "                                 kind='mse',\n",
    "                                accel=True)\n",
    "gd_prediction = predict_labels(w_gd, tx_test)\n",
    "acc_gd = accuracy_ratio(gd_prediction, y_test)\n",
    "print('Accuracy ratio = %.3f' % acc_gd)\n",
    "print('Test loss = %.3f' % compute_loss(y_test, tx_test, w_gd))\n",
    "print('Train loss = %.3f' % loss_gd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T12:37:03.486835Z",
     "start_time": "2019-10-17T12:37:03.480662Z"
    }
   },
   "source": [
    "np.savetxt('../data/w_gd_acc.dat', w_gd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T12:44:38.463674Z",
     "start_time": "2019-10-17T12:43:24.467477Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD (0/4999): loss=0.5\n",
      "SGD (100/4999): loss=19.058232645238103\n",
      "SGD (200/4999): loss=18.58767606371344\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-c96146cef937>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m                                     \u001b[0mpr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                                     \u001b[0madapt_gamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m                                     choose_best=True)\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0msgd_prediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_sgd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0macc_sgd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_ratio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msgd_prediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gdrive/EPFL/2019-2020/MachineLearning/Project/ml-project1/scripts/implementations.py\u001b[0m in \u001b[0;36mleast_squares_SGD\u001b[0;34m(y, tx, initial_w, batch_size, max_iters, gamma, kind, adapt_gamma, pr, choose_best)\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mn_iter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mnew_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_tx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_batches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m             \u001b[0;31m# compute gradient and loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_tx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gdrive/EPFL/2019-2020/MachineLearning/Project/ml-project1/scripts/implementations.py\u001b[0m in \u001b[0;36mbatch_iter\u001b[0;34m(y, tx, batch_size, num_batches, shuffle)\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mshuffle_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mshuffled_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mshuffle_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0mshuffled_tx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mshuffle_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mshuffled_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "w_init = np.array([0] * tx_train.shape[1])\n",
    "max_iter = 5000\n",
    "gamma = 1e-2\n",
    "batch_size = 1\n",
    "\n",
    "w_sgd, loss_sgd = least_squares_SGD(y_train,\n",
    "                                    tx_train,\n",
    "                                    w_init,\n",
    "                                    batch_size,\n",
    "                                    max_iter,\n",
    "                                    gamma,\n",
    "                                    pr=True,\n",
    "                                    adapt_gamma=False,\n",
    "                                    choose_best=True)\n",
    "sgd_prediction = predict_labels(w_sgd, tx_test)\n",
    "acc_sgd = accuracy_ratio(sgd_prediction, y_test)\n",
    "print('Accuracy ratio = %.2f' % acc_sgd)\n",
    "print('Test loss = %.2e' % compute_loss(y_test, tx_test, w_sgd))\n",
    "print('Train loss = %.2e' % loss_sgd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T12:44:53.858762Z",
     "start_time": "2019-10-17T12:44:53.793733Z"
    }
   },
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-dd580b01d9c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mw_lsq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_lsq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mleast_squares\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlsq_prediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_lsq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0macc_lsq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_ratio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlsq_prediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Accuracy ratio = %.2f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0macc_lsq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Train loss = %.2f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mloss_lsq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gdrive/EPFL/2019-2020/MachineLearning/Project/ml-project1/scripts/implementations.py\u001b[0m in \u001b[0;36maccuracy_ratio\u001b[0;34m(prediction, labels)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mcount\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcorrect\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbatch_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_batches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \"\"\"\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "w_lsq, loss_lsq = least_squares(y_train, tx_train)\n",
    "lsq_prediction = predict_labels(w_lsq, tx_test)\n",
    "acc_lsq = accuracy_ratio(lsq_prediction, y_test)\n",
    "print('Accuracy ratio = %.2f' % acc_lsq)\n",
    "print('Train loss = %.2f' % loss_lsq)\n",
    "print('Test loss = %.2e' % compute_loss(y_test, tx_test, w_lsq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T12:45:30.271461Z",
     "start_time": "2019-10-17T12:45:30.267909Z"
    }
   },
   "source": [
    "np.savetxt('../data/w_lsq.dat', w_lsq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T12:53:27.398694Z",
     "start_time": "2019-10-17T12:53:27.333828Z"
    }
   },
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-7d6e8a8aea3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mw_rr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_rr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mridge_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mrr_prediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_rr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0macc_rr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_ratio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrr_prediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Accuracy ratio = %.3f'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0macc_rr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test loss = %.3f'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_rr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gdrive/EPFL/2019-2020/MachineLearning/Project/ml-project1/scripts/implementations.py\u001b[0m in \u001b[0;36maccuracy_ratio\u001b[0;34m(prediction, labels)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mcount\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcorrect\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbatch_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_batches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \"\"\"\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "lambda_ = 2.4e-6\n",
    "w_rr, loss_rr = ridge_regression(y_train, tx_train, lambda_)\n",
    "rr_prediction = predict_labels(w_rr, tx_test)\n",
    "acc_rr = accuracy_ratio(rr_prediction, y_test)\n",
    "print('Accuracy ratio = %.3f'%acc_rr)\n",
    "print('Test loss = %.3f'%compute_loss(y_test, tx_test, w_rr))\n",
    "print('Train loss = %.3f'%loss_rr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T12:53:38.923376Z",
     "start_time": "2019-10-17T12:53:38.916041Z"
    }
   },
   "outputs": [],
   "source": [
    "np.savetxt('../data/w_rr.dat', w_rr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T12:46:25.177923Z",
     "start_time": "2019-10-17T12:46:25.143746Z"
    }
   },
   "outputs": [],
   "source": [
    "def ridge_regression_demo(x, y, degree, ratio, seed):\n",
    "    \"\"\"ridge regression demo.\"\"\"\n",
    "    # define parameter\n",
    "    lambdas = np.logspace(-5, 3, 50)\n",
    "    # split the data, and return train and test data\n",
    "    x_train, y_train, x_test, y_test = split_data(x, y, ratio, seed)\n",
    "    # form train and test data with offset column\n",
    "    x_train_std = standardize_features(x_train)[0]\n",
    "    x_test_std = standardize_features(x_test)[0]\n",
    "    tx_train=build_poly(x_train_std, degree)\n",
    "    tx_test=build_poly(x_test_std, degree)\n",
    "    rmse_tr = []\n",
    "    rmse_te = []\n",
    "    accuracies = []\n",
    "    for ind, lambda_ in enumerate(lambdas):\n",
    "        # calcualte weight through least square.\n",
    "        w_train, loss_train = ridge_regression(y_train, tx_train, lambda_)\n",
    "        rmse_tr.append(np.sqrt(2*loss_train))\n",
    "        rmse_te.append(np.sqrt(2*compute_loss(y_test, tx_test, w_train, kind = 'mse')))\n",
    "        accuracies.append(accuracy_ratio(predict_labels(w_train, tx_test), y_test))\n",
    "        print(\"proportion={p}, degree={d}, lambda={l:.3e}, Training RMSE={tr:.3f}, Testing RMSE={te:.3f}, Accuracy={ac:.3f}\".format(\n",
    "               p=ratio, d=degree, l=lambda_, tr=rmse_tr[ind], te=rmse_te[ind], ac=accuracies[ind]))\n",
    "        \n",
    "    # Plot the obtained results\n",
    "    plot_train_test(rmse_tr, rmse_te, lambdas, degree)\n",
    "    plt.figure()\n",
    "    plt.semilogx(lambdas,accuracies, marker='o')\n",
    "def plot_train_test(train_errors, test_errors, lambdas, degree):\n",
    "    \"\"\"\n",
    "    train_errors, test_errors and lambas should be list (of the same size) the respective train error and test error for a given lambda,\n",
    "    * lambda[0] = 1\n",
    "    * train_errors[0] = RMSE of a ridge regression on the train set\n",
    "    * test_errors[0] = RMSE of the parameter found by ridge regression applied on the test set\n",
    "    \n",
    "    degree is just used for the title of the plot.\n",
    "    \"\"\"\n",
    "    plt.semilogx(lambdas, train_errors, color='b', marker='*', label=\"Train error\")\n",
    "    plt.semilogx(lambdas, test_errors, color='r', marker='*', label=\"Test error\")\n",
    "    plt.xlabel(\"lambda\")\n",
    "    plt.ylabel(\"RMSE\")\n",
    "    plt.title(\"Ridge regression for polynomial degree \" + str(degree))\n",
    "    leg = plt.legend(loc=1, shadow=True)\n",
    "    leg.draw_frame(False)\n",
    "    plt.savefig(\"../results/ridge_regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T12:46:31.162180Z",
     "start_time": "2019-10-17T12:46:26.925013Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proportion=0.9, degree=2, lambda=1.000e-05, Training RMSE=0.805, Testing RMSE=0.804, Accuracy=0.761\n",
      "proportion=0.9, degree=2, lambda=1.456e-05, Training RMSE=0.805, Testing RMSE=0.804, Accuracy=0.760\n",
      "proportion=0.9, degree=2, lambda=2.121e-05, Training RMSE=0.805, Testing RMSE=0.805, Accuracy=0.761\n",
      "proportion=0.9, degree=2, lambda=3.089e-05, Training RMSE=0.805, Testing RMSE=0.805, Accuracy=0.761\n",
      "proportion=0.9, degree=2, lambda=4.498e-05, Training RMSE=0.805, Testing RMSE=0.805, Accuracy=0.761\n",
      "proportion=0.9, degree=2, lambda=6.551e-05, Training RMSE=0.805, Testing RMSE=0.805, Accuracy=0.761\n",
      "proportion=0.9, degree=2, lambda=9.541e-05, Training RMSE=0.805, Testing RMSE=0.805, Accuracy=0.761\n",
      "proportion=0.9, degree=2, lambda=1.389e-04, Training RMSE=0.805, Testing RMSE=0.805, Accuracy=0.760\n",
      "proportion=0.9, degree=2, lambda=2.024e-04, Training RMSE=0.805, Testing RMSE=0.805, Accuracy=0.761\n",
      "proportion=0.9, degree=2, lambda=2.947e-04, Training RMSE=0.806, Testing RMSE=0.805, Accuracy=0.761\n",
      "proportion=0.9, degree=2, lambda=4.292e-04, Training RMSE=0.806, Testing RMSE=0.805, Accuracy=0.761\n",
      "proportion=0.9, degree=2, lambda=6.251e-04, Training RMSE=0.806, Testing RMSE=0.805, Accuracy=0.761\n",
      "proportion=0.9, degree=2, lambda=9.103e-04, Training RMSE=0.806, Testing RMSE=0.805, Accuracy=0.761\n",
      "proportion=0.9, degree=2, lambda=1.326e-03, Training RMSE=0.806, Testing RMSE=0.805, Accuracy=0.761\n",
      "proportion=0.9, degree=2, lambda=1.931e-03, Training RMSE=0.806, Testing RMSE=0.805, Accuracy=0.761\n",
      "proportion=0.9, degree=2, lambda=2.812e-03, Training RMSE=0.806, Testing RMSE=0.805, Accuracy=0.761\n",
      "proportion=0.9, degree=2, lambda=4.095e-03, Training RMSE=0.806, Testing RMSE=0.806, Accuracy=0.760\n",
      "proportion=0.9, degree=2, lambda=5.964e-03, Training RMSE=0.807, Testing RMSE=0.806, Accuracy=0.760\n",
      "proportion=0.9, degree=2, lambda=8.685e-03, Training RMSE=0.807, Testing RMSE=0.807, Accuracy=0.760\n",
      "proportion=0.9, degree=2, lambda=1.265e-02, Training RMSE=0.808, Testing RMSE=0.807, Accuracy=0.759\n",
      "proportion=0.9, degree=2, lambda=1.842e-02, Training RMSE=0.809, Testing RMSE=0.808, Accuracy=0.758\n",
      "proportion=0.9, degree=2, lambda=2.683e-02, Training RMSE=0.810, Testing RMSE=0.809, Accuracy=0.756\n",
      "proportion=0.9, degree=2, lambda=3.907e-02, Training RMSE=0.811, Testing RMSE=0.810, Accuracy=0.756\n",
      "proportion=0.9, degree=2, lambda=5.690e-02, Training RMSE=0.813, Testing RMSE=0.812, Accuracy=0.755\n",
      "proportion=0.9, degree=2, lambda=8.286e-02, Training RMSE=0.815, Testing RMSE=0.814, Accuracy=0.754\n",
      "proportion=0.9, degree=2, lambda=1.207e-01, Training RMSE=0.817, Testing RMSE=0.817, Accuracy=0.752\n",
      "proportion=0.9, degree=2, lambda=1.758e-01, Training RMSE=0.820, Testing RMSE=0.820, Accuracy=0.749\n",
      "proportion=0.9, degree=2, lambda=2.560e-01, Training RMSE=0.824, Testing RMSE=0.824, Accuracy=0.746\n",
      "proportion=0.9, degree=2, lambda=3.728e-01, Training RMSE=0.830, Testing RMSE=0.830, Accuracy=0.742\n",
      "proportion=0.9, degree=2, lambda=5.429e-01, Training RMSE=0.837, Testing RMSE=0.837, Accuracy=0.736\n",
      "proportion=0.9, degree=2, lambda=7.906e-01, Training RMSE=0.845, Testing RMSE=0.845, Accuracy=0.731\n",
      "proportion=0.9, degree=2, lambda=1.151e+00, Training RMSE=0.854, Testing RMSE=0.855, Accuracy=0.725\n",
      "proportion=0.9, degree=2, lambda=1.677e+00, Training RMSE=0.865, Testing RMSE=0.865, Accuracy=0.720\n",
      "proportion=0.9, degree=2, lambda=2.442e+00, Training RMSE=0.876, Testing RMSE=0.876, Accuracy=0.713\n",
      "proportion=0.9, degree=2, lambda=3.556e+00, Training RMSE=0.887, Testing RMSE=0.888, Accuracy=0.707\n",
      "proportion=0.9, degree=2, lambda=5.179e+00, Training RMSE=0.899, Testing RMSE=0.900, Accuracy=0.700\n",
      "proportion=0.9, degree=2, lambda=7.543e+00, Training RMSE=0.912, Testing RMSE=0.913, Accuracy=0.691\n",
      "proportion=0.9, degree=2, lambda=1.099e+01, Training RMSE=0.925, Testing RMSE=0.925, Accuracy=0.683\n",
      "proportion=0.9, degree=2, lambda=1.600e+01, Training RMSE=0.938, Testing RMSE=0.938, Accuracy=0.674\n",
      "proportion=0.9, degree=2, lambda=2.330e+01, Training RMSE=0.950, Testing RMSE=0.950, Accuracy=0.666\n",
      "proportion=0.9, degree=2, lambda=3.393e+01, Training RMSE=0.961, Testing RMSE=0.961, Accuracy=0.662\n",
      "proportion=0.9, degree=2, lambda=4.942e+01, Training RMSE=0.970, Testing RMSE=0.970, Accuracy=0.659\n",
      "proportion=0.9, degree=2, lambda=7.197e+01, Training RMSE=0.977, Testing RMSE=0.977, Accuracy=0.658\n",
      "proportion=0.9, degree=2, lambda=1.048e+02, Training RMSE=0.983, Testing RMSE=0.983, Accuracy=0.658\n",
      "proportion=0.9, degree=2, lambda=1.526e+02, Training RMSE=0.988, Testing RMSE=0.988, Accuracy=0.658\n",
      "proportion=0.9, degree=2, lambda=2.223e+02, Training RMSE=0.991, Testing RMSE=0.991, Accuracy=0.658\n",
      "proportion=0.9, degree=2, lambda=3.237e+02, Training RMSE=0.994, Testing RMSE=0.994, Accuracy=0.658\n",
      "proportion=0.9, degree=2, lambda=4.715e+02, Training RMSE=0.996, Testing RMSE=0.996, Accuracy=0.658\n",
      "proportion=0.9, degree=2, lambda=6.866e+02, Training RMSE=0.997, Testing RMSE=0.997, Accuracy=0.658\n",
      "proportion=0.9, degree=2, lambda=1.000e+03, Training RMSE=0.998, Testing RMSE=0.998, Accuracy=0.658\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "seed = 42\n",
    "degree = 2\n",
    "split_ratio = 0.9\n",
    "ridge_regression_demo(x, y, degree, split_ratio, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T12:46:48.133126Z",
     "start_time": "2019-10-17T12:46:48.116515Z"
    }
   },
   "outputs": [],
   "source": [
    "def cross_validation(y, x, k_indices, k, lambda_, degree):\n",
    "    \"\"\"return the loss of ridge regression.\"\"\"\n",
    "    # get k'th subgroup in test, others in train\n",
    "    id_test = k_indices[k]\n",
    "    id_train = k_indices[~(np.arange(k_indices.shape[0]) == k)].reshape(-1)\n",
    "    x_test = x[id_test]\n",
    "    x_train = x[id_train]\n",
    "    y_test = y[id_test]\n",
    "    y_train = y[id_train]\n",
    "    # Standardize\n",
    "    x_train_std = standardize(x_train)[0]\n",
    "    x_test_std = standardize(x_test)[0]\n",
    "    # Define feature matrix\n",
    "    tx_train = build_poly(x_train_std, degree)\n",
    "    tx_test = build_poly(x_test_std, degree)\n",
    "    # ridge regression\n",
    "    weight, loss_tr = ridge_regression(y_train, tx_train, lambda_)\n",
    "    # calculate the loss for train and test data\n",
    "    loss_te = compute_loss(y_test, tx_test, weight)\n",
    "    accuracy = accuracy_ratio(predict_labels(weight, tx_test), y_test)\n",
    "\n",
    "    return loss_tr, loss_te, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T12:49:13.920373Z",
     "start_time": "2019-10-17T12:46:49.296378Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best lambda from accuracy: 4.89e-07\n",
      "Best lambda from error: 2.40e-06\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def cross_validation_visualization(lambds, mse_tr, mse_te):\n",
    "    \"\"\"visualization the curves of mse_tr and mse_te.\"\"\"\n",
    "    best_l_err = lambds[np.argmin(mse_te)]\n",
    "    print('Best lambda from error: %.2e'%best_l_err)\n",
    "    plt.semilogx(lambds, mse_tr, marker=\".\", color='b', label='train error')\n",
    "    plt.semilogx(lambds, mse_te, marker=\".\", color='r', label='test error')\n",
    "    plt.axvline(best_l_err, c = 'g', label = '$\\lambda^*_{rmse}=%.1e$'%best_l_err, ls = ':')\n",
    "    plt.xlabel(\"lambda\")\n",
    "    plt.ylabel(\"rmse\")\n",
    "    plt.title(\"cross validation\")\n",
    "    plt.legend(loc=0)\n",
    "    plt.grid(True)\n",
    "    plt.savefig(\"../results/cross_validation\")\n",
    "def cross_validation_visualization_accuracy(lambdas, accuracies):\n",
    "    \"\"\"visualization the curves of mse_tr and mse_te.\"\"\"\n",
    "    plt.semilogx(lambdas, accuracies, lw =2, marker = '*', label = 'Accuracy ratio')\n",
    "    best_l_acc = lambdas[np.argmax(accuracies)]\n",
    "    plt.axvline(best_l_acc, c= 'k', label = '$\\lambda^*_{acc}=%.1e$'%best_l_acc, ls = ':')\n",
    "    print('Best lambda from accuracy: %.2e'%best_l_acc)\n",
    "    plt.xlabel(\"lambda\")\n",
    "    plt.ylabel(\"accuracy\")\n",
    "    plt.title(\"cross validation\")\n",
    "    plt.legend(loc=0)\n",
    "    plt.grid(True)\n",
    "    plt.savefig(\"../results/cross_validation_accuracies\")\n",
    "def cross_validation_demo():\n",
    "    seed = 42\n",
    "    degree = 3\n",
    "    k_fold = 4\n",
    "    lambdas = np.logspace(-7, 3, 30)\n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    # define lists to store the loss of training data and test data\n",
    "    rmse_tr = []\n",
    "    rmse_te = []\n",
    "    std_tr = []\n",
    "    std_te = []\n",
    "    accuracies = []\n",
    "    # cross validation\n",
    "    for ind, lambda_ in enumerate(lambdas):\n",
    "        x_validation = np.array([cross_validation(y, x, k_indices, k, lambda_, degree) for k in range(k_fold)])\n",
    "        rmse_tr.append(np.mean(np.sqrt(2 * x_validation[:, 0])))\n",
    "        rmse_te.append(np.mean(np.sqrt(2 * x_validation[:, 1])))\n",
    "        std_tr.append(np.std(np.sqrt(2 * x_validation[:, 0])))\n",
    "        std_te.append(np.std(np.sqrt(2 * x_validation[:, 1])))\n",
    "        accuracies.append(np.mean(x_validation[:,2]))\n",
    "    cross_validation_visualization_accuracy(lambdas, accuracies)\n",
    "    plt.figure()\n",
    "    cross_validation_visualization(lambdas, rmse_tr, rmse_te)\n",
    "\n",
    "cross_validation_demo()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias-Variance test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T12:53:17.453735Z",
     "start_time": "2019-10-17T12:50:27.446578Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-19e5c47428f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m \u001b[0mbias_variance_demo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-31-19e5c47428f6>\u001b[0m in \u001b[0;36mbias_variance_demo\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mindex_degrees\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdegree\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdegrees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mtx_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_poly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_std\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0mtx_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_poly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test_std\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_tr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mridge_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.89e-05\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mloss_te\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mse'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gdrive/EPFL/2019-2020/MachineLearning/Project/ml-project1/scripts/implementations.py\u001b[0m in \u001b[0;36mbuild_poly\u001b[0;34m(x, degree)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbuild_poly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;34m\"\"\"polynomial basis functions for input data x, for j=0 up to j=degree.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m     \u001b[0mphi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdegree\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m     \u001b[0mphi_off\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mphi_off\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gdrive/EPFL/2019-2020/MachineLearning/Project/ml-project1/scripts/implementations.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbuild_poly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;34m\"\"\"polynomial basis functions for input data x, for j=0 up to j=degree.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m     \u001b[0mphi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdegree\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m     \u001b[0mphi_off\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mphi_off\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def bias_variance_decomposition_visualization(degrees, rmse_tr, rmse_te):\n",
    "    \"\"\"visualize the bias variance decomposition.\"\"\"\n",
    "    rmse_tr_mean = np.expand_dims(np.mean(rmse_tr, axis=0), axis=0)\n",
    "    rmse_te_mean = np.expand_dims(np.mean(rmse_te, axis=0), axis=0)\n",
    "    print(rmse_te_mean, rmse_tr_mean)\n",
    "    plt.plot(degrees,\n",
    "             rmse_tr.T,\n",
    "             'b',\n",
    "             linestyle=\"-\",\n",
    "             color=([0.7, 0.7, 1]),\n",
    "             label='train',\n",
    "             linewidth=0.3)\n",
    "    plt.plot(degrees,\n",
    "             rmse_te.T,\n",
    "             'r',\n",
    "             linestyle=\"-\",\n",
    "             color=[1, 0.7, 0.7],\n",
    "             label='test',\n",
    "             linewidth=0.3)\n",
    "    plt.plot(degrees,\n",
    "             rmse_tr_mean.T,\n",
    "             'b',\n",
    "             linestyle=\"-\",\n",
    "             label='train',\n",
    "             linewidth=3)\n",
    "    plt.plot(degrees,\n",
    "             rmse_te_mean.T,\n",
    "             'r',\n",
    "             linestyle=\"-\",\n",
    "             label='test',\n",
    "             linewidth=3)\n",
    "    plt.ylim(0.7, 1)\n",
    "    plt.xlabel(\"degree\")\n",
    "    plt.ylabel(\"error\")\n",
    "    plt.title(\"Bias-Variance Decomposition\")\n",
    "    plt.savefig(\"bias_variance\")\n",
    "\n",
    "\n",
    "def bias_variance_demo():\n",
    "    \"\"\"The entry.\"\"\"\n",
    "    # define parameters\n",
    "    seeds = range(100)\n",
    "    ratio_train = 0.5\n",
    "    degrees = range(1, 8)\n",
    "    # define list to store the variable\n",
    "    rmse_tr = np.empty((len(seeds), len(degrees)))\n",
    "    rmse_te = np.empty((len(seeds), len(degrees)))\n",
    "    for index_seed, seed in enumerate(seeds):\n",
    "        np.random.seed(seed)\n",
    "        # split data with a specific seed\n",
    "        x_train, y_train, x_test, y_test = split_data(x, y, ratio_train, seed)\n",
    "        x_train_std = standardize(x_train)[0]\n",
    "        x_test_std = standardize(x_test)[0]\n",
    "        for index_degrees, degree in enumerate(degrees):\n",
    "            tx_train = build_poly(x_train_std, degree)\n",
    "            tx_test = build_poly(x_test_std, degree)\n",
    "            weight, loss_tr = ridge_regression(y_train, tx_train, 1.89e-05 )\n",
    "            loss_te = compute_loss(y_test, tx_test, weight, kind='mse')\n",
    "            rmse_tr[index_seed, index_degrees] = np.sqrt(2 * loss_tr)\n",
    "            rmse_te[index_seed, index_degrees] = np.sqrt(2 * loss_te)\n",
    "    bias_variance_decomposition_visualization(degrees, rmse_tr, rmse_te)\n",
    "\n",
    "\n",
    "bias_variance_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T12:54:53.029395Z",
     "start_time": "2019-10-17T12:54:47.011563Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(568238,)\n"
     ]
    }
   ],
   "source": [
    "print(id_out_test.shape)\n",
    "x_out_test_std = standardize_features(x_out_test)\n",
    "x_out = x_out_test_std[0]\n",
    "tx_out = build_poly(x_out, 2)\n",
    "\n",
    "create_csv_submission(id_out_test, predict_labels(w_rr, tx_out) , '../results/rr_pred.csv')\n",
    "create_csv_submission(id_out_test, predict_labels(w_gd, tx_out) , '../results/gd_pred_accel.csv')\n",
    "create_csv_submission(id_out_test, predict_labels(w_lsq, tx_out) , '../results/lsq_pred.csv')\n",
    "#create_csv_submission(id_out_test, predict_labels(w_sgd, tx_out) , '../results/sgd_pred_noadapt.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T19:56:40.502785Z",
     "start_time": "2019-10-15T19:56:32.944405Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression GD (0/3999): loss=3119.1623125199\n",
      "Logistic Regression GD (100/3999): loss=-5052.666973716754\n",
      "Logistic Regression GD (200/3999): loss=-11433.464618887743\n",
      "Logistic Regression GD (300/3999): loss=-17413.60415105426\n",
      "Logistic Regression GD (400/3999): loss=-23223.98806948015\n",
      "Logistic Regression GD (500/3999): loss=-28945.610910390023\n",
      "Logistic Regression GD (600/3999): loss=-34614.76643112936\n",
      "Logistic Regression GD (700/3999): loss=-40250.402858839385\n",
      "Logistic Regression GD (800/3999): loss=-45863.44758469955\n",
      "Logistic Regression GD (900/3999): loss=-51460.63998304881\n",
      "Logistic Regression GD (1000/3999): loss=-57046.342020135344\n",
      "Logistic Regression GD (1100/3999): loss=-62623.481785886215\n",
      "Logistic Regression GD (1200/3999): loss=-68194.07948376483\n",
      "Logistic Regression GD (1300/3999): loss=-73759.56306488346\n",
      "Logistic Regression GD (1400/3999): loss=-79320.97905633446\n",
      "Logistic Regression GD (1500/3999): loss=-84879.11791756989\n",
      "Logistic Regression GD (1600/3999): loss=-90434.58244345462\n",
      "Logistic Regression GD (1700/3999): loss=-95987.83777402012\n",
      "Logistic Regression GD (1800/3999): loss=-101539.24770951094\n",
      "Logistic Regression GD (1900/3999): loss=-107089.10041091037\n",
      "Logistic Regression GD (2000/3999): loss=-112637.62686403707\n",
      "Logistic Regression GD (2100/3999): loss=-118185.01430831033\n",
      "Logistic Regression GD (2200/3999): loss=-123731.416111573\n",
      "Logistic Regression GD (2300/3999): loss=-129276.9591130256\n",
      "Logistic Regression GD (2400/3999): loss=-134821.74914539861\n",
      "Logistic Regression GD (2500/3999): loss=-140365.87523463654\n",
      "Logistic Regression GD (2600/3999): loss=-145909.4128289802\n",
      "Logistic Regression GD (2700/3999): loss=-151452.42630812994\n",
      "Logistic Regression GD (2800/3999): loss=-156994.9709527565\n",
      "Logistic Regression GD (2900/3999): loss=-162537.09450526276\n",
      "Logistic Regression GD (3000/3999): loss=-168078.83841780457\n",
      "Logistic Regression GD (3100/3999): loss=-173620.238858693\n",
      "Logistic Regression GD (3200/3999): loss=-179161.327530384\n",
      "Logistic Regression GD (3300/3999): loss=-184702.1323392457\n",
      "Logistic Regression GD (3400/3999): loss=-190242.6779477429\n",
      "Logistic Regression GD (3500/3999): loss=-195782.98623260937\n",
      "Logistic Regression GD (3600/3999): loss=-201323.0766672925\n",
      "Logistic Regression GD (3700/3999): loss=-206862.96664297345\n",
      "Logistic Regression GD (3800/3999): loss=-212402.6717394368\n",
      "Logistic Regression GD (3900/3999): loss=-217942.20595474137\n",
      "Accuracy ratio = 0.650\n",
      "Test loss = -25823.906\n",
      "Train loss = -223426.189\n"
     ]
    }
   ],
   "source": [
    "w_init = np.array([0]*tx_train.shape[1])\n",
    "max_iter = 4000\n",
    "gamma = 1e-7\n",
    "w_lrgd, loss_lrgd = logistic_regression(y_train, tx_train, w_init, max_iter, gamma, pr=True, adapt_gamma = False)\n",
    "lrgd_prediction = predict_labels(w_lrgd, tx_test)\n",
    "acc_lrgd = accuracy_ratio(lrgd_prediction, y_test)\n",
    "print('Accuracy ratio = %.3f'%acc_lrgd)\n",
    "print('Test loss = %.3f'%compute_loss_logistic(y_test, tx_test, w_lrgd))\n",
    "print('Train loss = %.3f'%loss_lrgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T20:26:25.163485Z",
     "start_time": "2019-10-15T20:24:37.811107Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Regularized Logistic Regression GD (0/99999): loss=3119.1623125199\n",
      " Regularized Logistic Regression GD (100/99999): loss=2688.691845895465\n",
      " Regularized Logistic Regression GD (200/99999): loss=2304.1297938103958\n",
      " Regularized Logistic Regression GD (300/99999): loss=1956.6286948875052\n",
      " Regularized Logistic Regression GD (400/99999): loss=1639.7748428355728\n",
      " Regularized Logistic Regression GD (500/99999): loss=1348.914140418746\n",
      " Regularized Logistic Regression GD (600/99999): loss=1080.5033867836107\n",
      " Regularized Logistic Regression GD (700/99999): loss=831.7359821070947\n",
      " Regularized Logistic Regression GD (800/99999): loss=600.3250439258538\n",
      " Regularized Logistic Regression GD (900/99999): loss=384.36877008910847\n",
      " Regularized Logistic Regression GD (1000/99999): loss=182.26125630421242\n",
      " Regularized Logistic Regression GD (1100/99999): loss=-7.369722867539792\n",
      " Regularized Logistic Regression GD (1200/99999): loss=-185.70792245373315\n",
      " Regularized Logistic Regression GD (1300/99999): loss=-353.7825494377411\n",
      " Regularized Logistic Regression GD (1400/99999): loss=-512.4943819615405\n",
      " Regularized Logistic Regression GD (1500/99999): loss=-662.636306060723\n",
      " Regularized Logistic Regression GD (1600/99999): loss=-804.9097428973747\n",
      " Regularized Logistic Regression GD (1700/99999): loss=-939.9379771369386\n",
      " Regularized Logistic Regression GD (1800/99999): loss=-1068.2771005418208\n",
      " Regularized Logistic Regression GD (1900/99999): loss=-1190.4250883643144\n",
      " Regularized Logistic Regression GD (2000/99999): loss=-1306.8293921529373\n",
      " Regularized Logistic Regression GD (2100/99999): loss=-1417.8933388738578\n",
      " Regularized Logistic Regression GD (2200/99999): loss=-1523.9815591733793\n",
      " Regularized Logistic Regression GD (2300/99999): loss=-1625.4246185997067\n",
      " Regularized Logistic Regression GD (2400/99999): loss=-1722.5229891316862\n",
      " Regularized Logistic Regression GD (2500/99999): loss=-1815.550470773639\n",
      " Regularized Logistic Regression GD (2600/99999): loss=-1904.7571517994015\n",
      " Regularized Logistic Regression GD (2700/99999): loss=-1990.371979762943\n",
      " Regularized Logistic Regression GD (2800/99999): loss=-2072.6050024421265\n",
      " Regularized Logistic Regression GD (2900/99999): loss=-2151.6493275907023\n",
      " Regularized Logistic Regression GD (3000/99999): loss=-2227.6828421213418\n",
      " Regularized Logistic Regression GD (3100/99999): loss=-2300.869724670907\n",
      " Regularized Logistic Regression GD (3200/99999): loss=-2371.361780066146\n",
      " Regularized Logistic Regression GD (3300/99999): loss=-2439.2996197547063\n",
      " Regularized Logistic Regression GD (3400/99999): loss=-2504.813708594098\n",
      " Regularized Logistic Regression GD (3500/99999): loss=-2568.02529534695\n",
      " Regularized Logistic Regression GD (3600/99999): loss=-2629.047241694522\n",
      " Regularized Logistic Regression GD (3700/99999): loss=-2687.9847624578933\n",
      " Regularized Logistic Regression GD (3800/99999): loss=-2744.936087932221\n",
      " Regularized Logistic Regression GD (3900/99999): loss=-2799.9930577345895\n",
      " Regularized Logistic Regression GD (4000/99999): loss=-2853.241654291567\n",
      " Regularized Logistic Regression GD (4100/99999): loss=-2904.762483009991\n",
      " Regularized Logistic Regression GD (4200/99999): loss=-2954.6312052517815\n",
      " Regularized Logistic Regression GD (4300/99999): loss=-3002.9189294446824\n",
      " Regularized Logistic Regression GD (4400/99999): loss=-3049.6925649847335\n",
      " Regularized Logistic Regression GD (4500/99999): loss=-3095.015143004818\n",
      " Regularized Logistic Regression GD (4600/99999): loss=-3138.9461075826684\n",
      " Regularized Logistic Regression GD (4700/99999): loss=-3181.541580528955\n",
      " Regularized Logistic Regression GD (4800/99999): loss=-3222.854602521056\n",
      " Regularized Logistic Regression GD (4900/99999): loss=-3262.9353530228673\n",
      " Regularized Logistic Regression GD (5000/99999): loss=-3301.831351147932\n",
      " Regularized Logistic Regression GD (5100/99999): loss=-3339.587639376388\n",
      " Regularized Logistic Regression GD (5200/99999): loss=-3376.2469518208154\n",
      " Regularized Logistic Regression GD (5300/99999): loss=-3411.8498685473833\n",
      " Regularized Logistic Regression GD (5400/99999): loss=-3446.4349572932547\n",
      " Regularized Logistic Regression GD (5500/99999): loss=-3480.0389037760388\n",
      " Regularized Logistic Regression GD (5600/99999): loss=-3512.6966316630246\n",
      " Regularized Logistic Regression GD (5700/99999): loss=-3544.441413155365\n",
      " Regularized Logistic Regression GD (5800/99999): loss=-3575.304971042715\n",
      " Regularized Logistic Regression GD (5900/99999): loss=-3605.31757299578\n",
      " Regularized Logistic Regression GD (6000/99999): loss=-3634.508118786248\n",
      " Regularized Logistic Regression GD (6100/99999): loss=-3662.904221054364\n",
      " Regularized Logistic Regression GD (6200/99999): loss=-3690.5322801827506\n",
      " Regularized Logistic Regression GD (6300/99999): loss=-3717.4175537805377\n",
      " Regularized Logistic Regression GD (6400/99999): loss=-3743.584221232859\n",
      " Regularized Logistic Regression GD (6500/99999): loss=-3769.0554437273345\n",
      " Regularized Logistic Regression GD (6600/99999): loss=-3793.853420130212\n",
      " Regularized Logistic Regression GD (6700/99999): loss=-3817.999439049916\n",
      " Regularized Logistic Regression GD (6800/99999): loss=-3841.5139273946843\n",
      " Regularized Logistic Regression GD (6900/99999): loss=-3864.416495702779\n",
      " Regularized Logistic Regression GD (7000/99999): loss=-3886.725980498738\n",
      " Regularized Logistic Regression GD (7100/99999): loss=-3908.4604839063973\n",
      " Regularized Logistic Regression GD (7200/99999): loss=-3929.6374107290385\n",
      " Regularized Logistic Regression GD (7300/99999): loss=-3950.273503188717\n",
      " Regularized Logistic Regression GD (7400/99999): loss=-3970.384873500065\n",
      " Regularized Logistic Regression GD (7500/99999): loss=-3989.987034439031\n",
      " Regularized Logistic Regression GD (7600/99999): loss=-4009.0949280533596\n",
      " Regularized Logistic Regression GD (7700/99999): loss=-4027.7229526493184\n",
      " Regularized Logistic Regression GD (7800/99999): loss=-4045.884988178075\n",
      " Regularized Logistic Regression GD (7900/99999): loss=-4063.5944201350453\n",
      " Regularized Logistic Regression GD (8000/99999): loss=-4080.864162076202\n",
      " Regularized Logistic Regression GD (8100/99999): loss=-4097.706676847185\n",
      " Regularized Logistic Regression GD (8200/99999): loss=-4114.13399661326\n",
      " Regularized Logistic Regression GD (8300/99999): loss=-4130.157741771377\n",
      " Regularized Logistic Regression GD (8400/99999): loss=-4145.789138819149\n",
      " Regularized Logistic Regression GD (8500/99999): loss=-4161.039037249926\n",
      " Regularized Logistic Regression GD (8600/99999): loss=-4175.917925537717\n",
      " Regularized Logistic Regression GD (8700/99999): loss=-4190.435946271042\n",
      " Regularized Logistic Regression GD (8800/99999): loss=-4204.602910490243\n",
      " Regularized Logistic Regression GD (8900/99999): loss=-4218.428311278881\n",
      " Regularized Logistic Regression GD (9000/99999): loss=-4231.921336655989\n",
      " Regularized Logistic Regression GD (9100/99999): loss=-4245.090881812658\n",
      " Regularized Logistic Regression GD (9200/99999): loss=-4257.945560733281\n",
      " Regularized Logistic Regression GD (9300/99999): loss=-4270.493717238891\n",
      " Regularized Logistic Regression GD (9400/99999): loss=-4282.743435487339\n",
      " Regularized Logistic Regression GD (9500/99999): loss=-4294.702549962816\n",
      " Regularized Logistic Regression GD (9600/99999): loss=-4306.378654984674\n",
      " Regularized Logistic Regression GD (9700/99999): loss=-4317.779113763732\n",
      " Regularized Logistic Regression GD (9800/99999): loss=-4328.911067032197\n",
      " Regularized Logistic Regression GD (9900/99999): loss=-4339.781441271521\n",
      " Regularized Logistic Regression GD (10000/99999): loss=-4350.396956561051\n",
      " Regularized Logistic Regression GD (10100/99999): loss=-4360.764134068704\n",
      " Regularized Logistic Regression GD (10200/99999): loss=-4370.88930320349\n",
      " Regularized Logistic Regression GD (10300/99999): loss=-4380.778608448516\n",
      " Regularized Logistic Regression GD (10400/99999): loss=-4390.4380158918475\n",
      " Regularized Logistic Regression GD (10500/99999): loss=-4399.873319471475\n",
      " Regularized Logistic Regression GD (10600/99999): loss=-4409.090146949671\n",
      " Regularized Logistic Regression GD (10700/99999): loss=-4418.093965631004\n",
      " Regularized Logistic Regression GD (10800/99999): loss=-4426.890087837432\n",
      " Regularized Logistic Regression GD (10900/99999): loss=-4435.483676153063\n",
      " Regularized Logistic Regression GD (11000/99999): loss=-4443.879748450388\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Regularized Logistic Regression GD (11100/99999): loss=-4452.0831827090915\n",
      " Regularized Logistic Regression GD (11200/99999): loss=-4460.098721637905\n",
      " Regularized Logistic Regression GD (11300/99999): loss=-4467.930977109281\n",
      " Regularized Logistic Regression GD (11400/99999): loss=-4475.584434416149\n",
      " Regularized Logistic Regression GD (11500/99999): loss=-4483.06345635949\n",
      " Regularized Logistic Regression GD (11600/99999): loss=-4490.372287174869\n",
      " Regularized Logistic Regression GD (11700/99999): loss=-4497.515056305698\n",
      " Regularized Logistic Regression GD (11800/99999): loss=-4504.495782030524\n",
      " Regularized Logistic Regression GD (11900/99999): loss=-4511.318374951188\n",
      " Regularized Logistic Regression GD (12000/99999): loss=-4517.986641348391\n",
      " Regularized Logistic Regression GD (12100/99999): loss=-4524.504286410783\n",
      " Regularized Logistic Regression GD (12200/99999): loss=-4530.87491734338\n",
      " Regularized Logistic Regression GD (12300/99999): loss=-4537.1020463608065\n",
      " Regularized Logistic Regression GD (12400/99999): loss=-4543.189093570509\n",
      " Regularized Logistic Regression GD (12500/99999): loss=-4549.139389750928\n",
      " Regularized Logistic Regression GD (12600/99999): loss=-4554.956179029197\n",
      " Regularized Logistic Regression GD (12700/99999): loss=-4560.642621462836\n",
      " Regularized Logistic Regression GD (12800/99999): loss=-4566.201795529589\n",
      " Regularized Logistic Regression GD (12900/99999): loss=-4571.63670052935\n",
      " Regularized Logistic Regression GD (13000/99999): loss=-4576.95025890199\n",
      " Regularized Logistic Regression GD (13100/99999): loss=-4582.145318464587\n",
      " Regularized Logistic Regression GD (13200/99999): loss=-4587.224654571468\n",
      " Regularized Logistic Regression GD (13300/99999): loss=-4592.190972200321\n",
      " Regularized Logistic Regression GD (13400/99999): loss=-4597.046907967346\n",
      " Regularized Logistic Regression GD (13500/99999): loss=-4601.795032074449\n",
      " Regularized Logistic Regression GD (13600/99999): loss=-4606.4378501911615\n",
      " Regularized Logistic Regression GD (13700/99999): loss=-4610.977805273974\n",
      " Regularized Logistic Regression GD (13800/99999): loss=-4615.417279325538\n",
      " Regularized Logistic Regression GD (13900/99999): loss=-4619.7585950961775\n",
      " Regularized Logistic Regression GD (14000/99999): loss=-4624.004017729924\n",
      " Regularized Logistic Regression GD (14100/99999): loss=-4628.155756357282\n",
      " Regularized Logistic Regression GD (14200/99999): loss=-4632.215965636772\n",
      " Regularized Logistic Regression GD (14300/99999): loss=-4636.186747247205\n",
      " Regularized Logistic Regression GD (14400/99999): loss=-4640.07015133262\n",
      " Regularized Logistic Regression GD (14500/99999): loss=-4643.868177901599\n",
      " Regularized Logistic Regression GD (14600/99999): loss=-4647.582778182741\n",
      " Regularized Logistic Regression GD (14700/99999): loss=-4651.215855937906\n",
      " Regularized Logistic Regression GD (14800/99999): loss=-4654.769268734768\n",
      " Regularized Logistic Regression GD (14900/99999): loss=-4658.24482918023\n",
      " Regularized Logistic Regression GD (15000/99999): loss=-4661.644306116044\n",
      " Regularized Logistic Regression GD (15100/99999): loss=-4664.969425778105\n",
      " Regularized Logistic Regression GD (15200/99999): loss=-4668.221872920611\n",
      " Regularized Logistic Regression GD (15300/99999): loss=-4671.403291906491\n",
      " Regularized Logistic Regression GD (15400/99999): loss=-4674.515287765117\n",
      " Regularized Logistic Regression GD (15500/99999): loss=-4677.559427218612\n",
      " Regularized Logistic Regression GD (15600/99999): loss=-4680.537239677757\n",
      " Regularized Logistic Regression GD (15700/99999): loss=-4683.450218208578\n",
      " Regularized Logistic Regression GD (15800/99999): loss=-4686.299820470649\n",
      " Regularized Logistic Regression GD (15900/99999): loss=-4689.087469628037\n",
      " Regularized Logistic Regression GD (16000/99999): loss=-4691.814555233839\n",
      " Regularized Logistic Regression GD (16100/99999): loss=-4694.482434089236\n",
      " Regularized Logistic Regression GD (16200/99999): loss=-4697.092431077852\n",
      " Regularized Logistic Regression GD (16300/99999): loss=-4699.645839976309\n",
      " Regularized Logistic Regression GD (16400/99999): loss=-4702.143924241716\n",
      " Regularized Logistic Regression GD (16500/99999): loss=-4704.587917776911\n",
      " Regularized Logistic Regression GD (16600/99999): loss=-4706.979025674107\n",
      " Regularized Logistic Regression GD (16700/99999): loss=-4709.318424937706\n",
      " Regularized Logistic Regression GD (16800/99999): loss=-4711.607265186969\n",
      " Regularized Logistic Regression GD (16900/99999): loss=-4713.846669339093\n",
      " Regularized Logistic Regression GD (17000/99999): loss=-4716.037734273445\n",
      " Regularized Logistic Regression GD (17100/99999): loss=-4718.181531477466\n",
      " Regularized Logistic Regression GD (17200/99999): loss=-4720.279107674868\n",
      " Regularized Logistic Regression GD (17300/99999): loss=-4722.331485436638\n",
      " Regularized Logistic Regression GD (17400/99999): loss=-4724.33966377546\n",
      " Regularized Logistic Regression GD (17500/99999): loss=-4726.3046187239715\n",
      " Regularized Logistic Regression GD (17600/99999): loss=-4728.227303897464\n",
      " Regularized Logistic Regression GD (17700/99999): loss=-4730.108651041397\n",
      " Regularized Logistic Regression GD (17800/99999): loss=-4731.94957056428\n",
      " Regularized Logistic Regression GD (17900/99999): loss=-4733.750952056313\n",
      " Regularized Logistic Regression GD (18000/99999): loss=-4735.513664794244\n",
      " Regularized Logistic Regression GD (18100/99999): loss=-4737.238558232833\n",
      " Regularized Logistic Regression GD (18200/99999): loss=-4738.926462483366\n",
      " Regularized Logistic Regression GD (18300/99999): loss=-4740.578188779547\n",
      " Regularized Logistic Regression GD (18400/99999): loss=-4742.194529931207\n",
      " Regularized Logistic Regression GD (18500/99999): loss=-4743.776260766131\n",
      " Regularized Logistic Regression GD (18600/99999): loss=-4745.324138560395\n",
      " Regularized Logistic Regression GD (18700/99999): loss=-4746.83890345751\n",
      " Regularized Logistic Regression GD (18800/99999): loss=-4748.321278876766\n",
      " Regularized Logistic Regression GD (18900/99999): loss=-4749.771971910988\n",
      " Regularized Logistic Regression GD (19000/99999): loss=-4751.191673714112\n",
      " Regularized Logistic Regression GD (19100/99999): loss=-4752.581059878789\n",
      " Regularized Logistic Regression GD (19200/99999): loss=-4753.940790804381\n",
      " Regularized Logistic Regression GD (19300/99999): loss=-4755.271512055535\n",
      " Regularized Logistic Regression GD (19400/99999): loss=-4756.573854711691\n",
      " Regularized Logistic Regression GD (19500/99999): loss=-4757.848435707705\n",
      " Regularized Logistic Regression GD (19600/99999): loss=-4759.095858165895\n",
      " Regularized Logistic Regression GD (19700/99999): loss=-4760.3167117197\n",
      " Regularized Logistic Regression GD (19800/99999): loss=-4761.511572829215\n",
      " Regularized Logistic Regression GD (19900/99999): loss=-4762.68100508885\n",
      " Regularized Logistic Regression GD (20000/99999): loss=-4763.825559527264\n",
      " Regularized Logistic Regression GD (20100/99999): loss=-4764.945774899833\n",
      " Regularized Logistic Regression GD (20200/99999): loss=-4766.042177973893\n",
      " Regularized Logistic Regression GD (20300/99999): loss=-4767.115283806822\n",
      " Regularized Logistic Regression GD (20400/99999): loss=-4768.16559601737\n",
      " Regularized Logistic Regression GD (20500/99999): loss=-4769.193607050164\n",
      " Regularized Logistic Regression GD (20600/99999): loss=-4770.199798433802\n",
      " Regularized Logistic Regression GD (20700/99999): loss=-4771.184641032576\n",
      " Regularized Logistic Regression GD (20800/99999): loss=-4772.148595292019\n",
      " Regularized Logistic Regression GD (20900/99999): loss=-4773.092111478464\n",
      " Regularized Logistic Regression GD (21000/99999): loss=-4774.015629912775\n",
      " Regularized Logistic Regression GD (21100/99999): loss=-4774.919581198374\n",
      " Regularized Logistic Regression GD (21200/99999): loss=-4775.804386443737\n",
      " Regularized Logistic Regression GD (21300/99999): loss=-4776.67045747951\n",
      " Regularized Logistic Regression GD (21400/99999): loss=-4777.518197070371\n",
      " Regularized Logistic Regression GD (21500/99999): loss=-4778.347999121776\n",
      " Regularized Logistic Regression GD (21600/99999): loss=-4779.160248881766\n",
      " Regularized Logistic Regression GD (21700/99999): loss=-4779.955323137854\n",
      " Regularized Logistic Regression GD (21800/99999): loss=-4780.733590409287\n",
      " Regularized Logistic Regression GD (21900/99999): loss=-4781.495411134625\n",
      " Regularized Logistic Regression GD (22000/99999): loss=-4782.2411378548895\n",
      " Regularized Logistic Regression GD (22100/99999): loss=-4782.971115392335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Regularized Logistic Regression GD (22200/99999): loss=-4783.685681024971\n",
      " Regularized Logistic Regression GD (22300/99999): loss=-4784.385164656912\n",
      " Regularized Logistic Regression GD (22400/99999): loss=-4785.069888984767\n",
      " Regularized Logistic Regression GD (22500/99999): loss=-4785.7401696599845\n",
      " Regularized Logistic Regression GD (22600/99999): loss=-4786.396315447468\n",
      " Regularized Logistic Regression GD (22700/99999): loss=-4787.038628380373\n",
      " Regularized Logistic Regression GD (22800/99999): loss=-4787.667403911321\n",
      " Regularized Logistic Regression GD (22900/99999): loss=-4788.282931060018\n",
      " Regularized Logistic Regression GD (23000/99999): loss=-4788.885492557426\n",
      " Regularized Logistic Regression GD (23100/99999): loss=-4789.475364986581\n",
      " Regularized Logistic Regression GD (23200/99999): loss=-4790.05281892007\n",
      " Regularized Logistic Regression GD (23300/99999): loss=-4790.61811905435\n",
      " Regularized Logistic Regression GD (23400/99999): loss=-4791.171524340907\n",
      " Regularized Logistic Regression GD (23500/99999): loss=-4791.713288114365\n",
      " Regularized Logistic Regression GD (23600/99999): loss=-4792.243658217639\n",
      " Regularized Logistic Regression GD (23700/99999): loss=-4792.7628771241525\n",
      " Regularized Logistic Regression GD (23800/99999): loss=-4793.271182057261\n",
      " Regularized Logistic Regression GD (23900/99999): loss=-4793.768805106878\n",
      " Regularized Logistic Regression GD (24000/99999): loss=-4794.255973343439\n",
      " Regularized Logistic Regression GD (24100/99999): loss=-4794.732908929204\n",
      " Regularized Logistic Regression GD (24200/99999): loss=-4795.199829227028\n",
      " Regularized Logistic Regression GD (24300/99999): loss=-4795.656946906611\n",
      " Regularized Logistic Regression GD (24400/99999): loss=-4796.104470048313\n",
      " Regularized Logistic Regression GD (24500/99999): loss=-4796.542602244578\n",
      " Regularized Logistic Regression GD (24600/99999): loss=-4796.971542699067\n",
      " Regularized Logistic Regression GD (24700/99999): loss=-4797.391486323472\n",
      " Regularized Logistic Regression GD (24800/99999): loss=-4797.802623832157\n",
      " Regularized Logistic Regression GD (24900/99999): loss=-4798.205141834634\n",
      " Regularized Logistic Regression GD (25000/99999): loss=-4798.599222925896\n",
      " Regularized Logistic Regression GD (25100/99999): loss=-4798.985045774768\n",
      " Regularized Logistic Regression GD (25200/99999): loss=-4799.36278521016\n",
      " Regularized Logistic Regression GD (25300/99999): loss=-4799.732612305421\n",
      " Regularized Logistic Regression GD (25400/99999): loss=-4800.094694460758\n",
      " Regularized Logistic Regression GD (25500/99999): loss=-4800.449195483795\n",
      " Regularized Logistic Regression GD (25600/99999): loss=-4800.79627566831\n",
      " Regularized Logistic Regression GD (25700/99999): loss=-4801.136091871164\n",
      " Regularized Logistic Regression GD (25800/99999): loss=-4801.468797587535\n",
      " Regularized Logistic Regression GD (25900/99999): loss=-4801.794543024437\n",
      " Regularized Logistic Regression GD (26000/99999): loss=-4802.113475172566\n",
      " Regularized Logistic Regression GD (26100/99999): loss=-4802.425737876553\n",
      " Regularized Logistic Regression GD (26200/99999): loss=-4802.7314719036185\n",
      " Regularized Logistic Regression GD (26300/99999): loss=-4803.030815010702\n",
      " Regularized Logistic Regression GD (26400/99999): loss=-4803.323902010063\n",
      " Regularized Logistic Regression GD (26500/99999): loss=-4803.610864833443\n",
      " Regularized Logistic Regression GD (26600/99999): loss=-4803.8918325947525\n",
      " Regularized Logistic Regression GD (26700/99999): loss=-4804.1669316513935\n",
      " Regularized Logistic Regression GD (26800/99999): loss=-4804.436285664161\n",
      " Regularized Logistic Regression GD (26900/99999): loss=-4804.700015655873\n",
      " Regularized Logistic Regression GD (27000/99999): loss=-4804.958240068629\n",
      " Regularized Logistic Regression GD (27100/99999): loss=-4805.211074819837\n",
      " Regularized Logistic Regression GD (27200/99999): loss=-4805.458633356973\n",
      " Regularized Logistic Regression GD (27300/99999): loss=-4805.7010267111045\n",
      " Regularized Logistic Regression GD (27400/99999): loss=-4805.938363549275\n",
      " Regularized Logistic Regression GD (27500/99999): loss=-4806.170750225663\n",
      " Regularized Logistic Regression GD (27600/99999): loss=-4806.398290831665\n",
      " Regularized Logistic Regression GD (27700/99999): loss=-4806.621087244809\n",
      " Regularized Logistic Regression GD (27800/99999): loss=-4806.839239176634\n",
      " Regularized Logistic Regression GD (27900/99999): loss=-4807.052844219491\n",
      " Regularized Logistic Regression GD (28000/99999): loss=-4807.2619978922885\n",
      " Regularized Logistic Regression GD (28100/99999): loss=-4807.466793685259\n",
      " Regularized Logistic Regression GD (28200/99999): loss=-4807.667323103729\n",
      " Regularized Logistic Regression GD (28300/99999): loss=-4807.863675710896\n",
      " Regularized Logistic Regression GD (28400/99999): loss=-4808.055939169707\n",
      " Regularized Logistic Regression GD (28500/99999): loss=-4808.244199283774\n",
      " Regularized Logistic Regression GD (28600/99999): loss=-4808.4285400374\n",
      " Regularized Logistic Regression GD (28700/99999): loss=-4808.6090436347595\n",
      " Regularized Logistic Regression GD (28800/99999): loss=-4808.785790538155\n",
      " Regularized Logistic Regression GD (28900/99999): loss=-4808.95885950548\n",
      " Regularized Logistic Regression GD (29000/99999): loss=-4809.128327626833\n",
      " Regularized Logistic Regression GD (29100/99999): loss=-4809.294270360363\n",
      " Regularized Logistic Regression GD (29200/99999): loss=-4809.456761567288\n",
      " Regularized Logistic Regression GD (29300/99999): loss=-4809.615873546163\n",
      " Regularized Logistic Regression GD (29400/99999): loss=-4809.771677066411\n",
      " Regularized Logistic Regression GD (29500/99999): loss=-4809.924241401094\n",
      " Regularized Logistic Regression GD (29600/99999): loss=-4810.073634358995\n",
      " Regularized Logistic Regression GD (29700/99999): loss=-4810.219922315977\n",
      " Regularized Logistic Regression GD (29800/99999): loss=-4810.3631702456705\n",
      " Regularized Logistic Regression GD (29900/99999): loss=-4810.503441749487\n",
      " Regularized Logistic Regression GD (30000/99999): loss=-4810.640799085971\n",
      " Regularized Logistic Regression GD (30100/99999): loss=-4810.775303199531\n",
      " Regularized Logistic Regression GD (30200/99999): loss=-4810.90701374851\n",
      " Regularized Logistic Regression GD (30300/99999): loss=-4811.035989132698\n",
      " Regularized Logistic Regression GD (30400/99999): loss=-4811.162286520192\n",
      " Regularized Logistic Regression GD (30500/99999): loss=-4811.285961873701\n",
      " Regularized Logistic Regression GD (30600/99999): loss=-4811.407069976289\n",
      " Regularized Logistic Regression GD (30700/99999): loss=-4811.525664456507\n",
      " Regularized Logistic Regression GD (30800/99999): loss=-4811.641797813054\n",
      " Regularized Logistic Regression GD (30900/99999): loss=-4811.755521438851\n",
      " Regularized Logistic Regression GD (31000/99999): loss=-4811.8668856446\n",
      " Regularized Logistic Regression GD (31100/99999): loss=-4811.975939681838\n",
      " Regularized Logistic Regression GD (31200/99999): loss=-4812.082731765517\n",
      " Regularized Logistic Regression GD (31300/99999): loss=-4812.187309096041\n",
      " Regularized Logistic Regression GD (31400/99999): loss=-4812.289717880879\n",
      " Regularized Logistic Regression GD (31500/99999): loss=-4812.390003355672\n",
      " Regularized Logistic Regression GD (31600/99999): loss=-4812.488209804905\n",
      " Regularized Logistic Regression GD (31700/99999): loss=-4812.584380582143\n",
      " Regularized Logistic Regression GD (31800/99999): loss=-4812.678558129781\n",
      " Regularized Logistic Regression GD (31900/99999): loss=-4812.770783998441\n",
      " Regularized Logistic Regression GD (32000/99999): loss=-4812.861098865875\n",
      " Regularized Logistic Regression GD (32100/99999): loss=-4812.949542555525\n",
      " Regularized Logistic Regression GD (32200/99999): loss=-4813.036154054626\n",
      " Regularized Logistic Regression GD (32300/99999): loss=-4813.120971531985\n",
      " Regularized Logistic Regression GD (32400/99999): loss=-4813.204032355293\n",
      " Regularized Logistic Regression GD (32500/99999): loss=-4813.28537310816\n",
      " Regularized Logistic Regression GD (32600/99999): loss=-4813.365029606693\n",
      " Regularized Logistic Regression GD (32700/99999): loss=-4813.443036915785\n",
      " Regularized Logistic Regression GD (32800/99999): loss=-4813.519429365011\n",
      " Regularized Logistic Regression GD (32900/99999): loss=-4813.594240564233\n",
      " Regularized Logistic Regression GD (33000/99999): loss=-4813.667503418784\n",
      " Regularized Logistic Regression GD (33100/99999): loss=-4813.739250144426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Regularized Logistic Regression GD (33200/99999): loss=-4813.809512281921\n",
      " Regularized Logistic Regression GD (33300/99999): loss=-4813.878320711301\n",
      " Regularized Logistic Regression GD (33400/99999): loss=-4813.94570566586\n",
      " Regularized Logistic Regression GD (33500/99999): loss=-4814.011696745802\n",
      " Regularized Logistic Regression GD (33600/99999): loss=-4814.076322931651\n",
      " Regularized Logistic Regression GD (33700/99999): loss=-4814.139612597307\n",
      " Regularized Logistic Regression GD (33800/99999): loss=-4814.201593522888\n",
      " Regularized Logistic Regression GD (33900/99999): loss=-4814.262292907268\n",
      " Regularized Logistic Regression GD (34000/99999): loss=-4814.321737380318\n",
      " Regularized Logistic Regression GD (34100/99999): loss=-4814.379953014953\n",
      " Regularized Logistic Regression GD (34200/99999): loss=-4814.436965338865\n",
      " Regularized Logistic Regression GD (34300/99999): loss=-4814.492799346009\n",
      " Regularized Logistic Regression GD (34400/99999): loss=-4814.547479507894\n",
      " Regularized Logistic Regression GD (34500/99999): loss=-4814.6010297845705\n",
      " Regularized Logistic Regression GD (34600/99999): loss=-4814.65347363542\n",
      " Regularized Logistic Regression GD (34700/99999): loss=-4814.704834029695\n",
      " Regularized Logistic Regression GD (34800/99999): loss=-4814.755133456874\n",
      " Regularized Logistic Regression GD (34900/99999): loss=-4814.8043939367235\n",
      " Regularized Logistic Regression GD (35000/99999): loss=-4814.852637029233\n",
      " Regularized Logistic Regression GD (35100/99999): loss=-4814.899883844251\n",
      " Regularized Logistic Regression GD (35200/99999): loss=-4814.946155050998\n",
      " Regularized Logistic Regression GD (35300/99999): loss=-4814.991470887313\n",
      " Regularized Logistic Regression GD (35400/99999): loss=-4815.0358511687355\n",
      " Regularized Logistic Regression GD (35500/99999): loss=-4815.0793152973865\n",
      " Regularized Logistic Regression GD (35600/99999): loss=-4815.121882270664\n",
      " Regularized Logistic Regression GD (35700/99999): loss=-4815.163570689749\n",
      " Regularized Logistic Regression GD (35800/99999): loss=-4815.204398767928\n",
      " Regularized Logistic Regression GD (35900/99999): loss=-4815.244384338754\n",
      " Regularized Logistic Regression GD (36000/99999): loss=-4815.28354486402\n",
      " Regularized Logistic Regression GD (36100/99999): loss=-4815.321897441559\n",
      " Regularized Logistic Regression GD (36200/99999): loss=-4815.359458812897\n",
      " Regularized Logistic Regression GD (36300/99999): loss=-4815.396245370722\n",
      " Regularized Logistic Regression GD (36400/99999): loss=-4815.432273166229\n",
      " Regularized Logistic Regression GD (36500/99999): loss=-4815.467557916252\n",
      " Regularized Logistic Regression GD (36600/99999): loss=-4815.502115010303\n",
      " Regularized Logistic Regression GD (36700/99999): loss=-4815.535959517442\n",
      " Regularized Logistic Regression GD (36800/99999): loss=-4815.569106192974\n",
      " Regularized Logistic Regression GD (36900/99999): loss=-4815.601569485047\n",
      " Regularized Logistic Regression GD (37000/99999): loss=-4815.633363541084\n",
      " Regularized Logistic Regression GD (37100/99999): loss=-4815.6645022140865\n",
      " Regularized Logistic Regression GD (37200/99999): loss=-4815.6949990688045\n",
      " Regularized Logistic Regression GD (37300/99999): loss=-4815.7248673877775\n",
      " Regularized Logistic Regression GD (37400/99999): loss=-4815.754120177236\n",
      " Regularized Logistic Regression GD (37500/99999): loss=-4815.782770172904\n",
      " Regularized Logistic Regression GD (37600/99999): loss=-4815.810829845649\n",
      " Regularized Logistic Regression GD (37700/99999): loss=-4815.838311407031\n",
      " Regularized Logistic Regression GD (37800/99999): loss=-4815.865226814737\n",
      " Regularized Logistic Regression GD (37900/99999): loss=-4815.8915877778845\n",
      " Regularized Logistic Regression GD (38000/99999): loss=-4815.917405762219\n",
      " Regularized Logistic Regression GD (38100/99999): loss=-4815.94269199521\n",
      " Regularized Logistic Regression GD (38200/99999): loss=-4815.967457471047\n",
      " Regularized Logistic Regression GD (38300/99999): loss=-4815.991712955508\n",
      " Regularized Logistic Regression GD (38400/99999): loss=-4816.015468990703\n",
      " Regularized Logistic Regression GD (38500/99999): loss=-4816.0387358998105\n",
      " Regularized Logistic Regression GD (38600/99999): loss=-4816.061523791611\n",
      " Regularized Logistic Regression GD (38700/99999): loss=-4816.08384256497\n",
      " Regularized Logistic Regression GD (38800/99999): loss=-4816.105701913243\n",
      " Regularized Logistic Regression GD (38900/99999): loss=-4816.1271113285475\n",
      " Regularized Logistic Regression GD (39000/99999): loss=-4816.1480801059815\n",
      " Regularized Logistic Regression GD (39100/99999): loss=-4816.168617347721\n",
      " Regularized Logistic Regression GD (39200/99999): loss=-4816.188731967061\n",
      " Regularized Logistic Regression GD (39300/99999): loss=-4816.208432692364\n",
      " Regularized Logistic Regression GD (39400/99999): loss=-4816.227728070893\n",
      " Regularized Logistic Regression GD (39500/99999): loss=-4816.246626472625\n",
      " Regularized Logistic Regression GD (39600/99999): loss=-4816.2651360939235\n",
      " Regularized Logistic Regression GD (39700/99999): loss=-4816.283264961167\n",
      " Regularized Logistic Regression GD (39800/99999): loss=-4816.301020934309\n",
      " Regularized Logistic Regression GD (39900/99999): loss=-4816.318411710316\n",
      " Regularized Logistic Regression GD (40000/99999): loss=-4816.335444826608\n",
      " Regularized Logistic Regression GD (40100/99999): loss=-4816.352127664343\n",
      " Regularized Logistic Regression GD (40200/99999): loss=-4816.368467451706\n",
      " Regularized Logistic Regression GD (40300/99999): loss=-4816.384471267088\n",
      " Regularized Logistic Regression GD (40400/99999): loss=-4816.400146042193\n",
      " Regularized Logistic Regression GD (40500/99999): loss=-4816.415498565109\n",
      " Regularized Logistic Regression GD (40600/99999): loss=-4816.4305354832995\n",
      " Regularized Logistic Regression GD (40700/99999): loss=-4816.445263306529\n",
      " Regularized Logistic Regression GD (40800/99999): loss=-4816.459688409724\n",
      " Regularized Logistic Regression GD (40900/99999): loss=-4816.473817035792\n",
      " Regularized Logistic Regression GD (41000/99999): loss=-4816.4876552983715\n",
      " Regularized Logistic Regression GD (41100/99999): loss=-4816.501209184501\n",
      " Regularized Logistic Regression GD (41200/99999): loss=-4816.514484557279\n",
      " Regularized Logistic Regression GD (41300/99999): loss=-4816.527487158433\n",
      " Regularized Logistic Regression GD (41400/99999): loss=-4816.540222610846\n",
      " Regularized Logistic Regression GD (41500/99999): loss=-4816.552696421026\n",
      " Regularized Logistic Regression GD (41600/99999): loss=-4816.564913981525\n",
      " Regularized Logistic Regression GD (41700/99999): loss=-4816.576880573317\n",
      " Regularized Logistic Regression GD (41800/99999): loss=-4816.588601368118\n",
      " Regularized Logistic Regression GD (41900/99999): loss=-4816.600081430648\n",
      " Regularized Logistic Regression GD (42000/99999): loss=-4816.611325720865\n",
      " Regularized Logistic Regression GD (42100/99999): loss=-4816.622339096134\n",
      " Regularized Logistic Regression GD (42200/99999): loss=-4816.63312631337\n",
      " Regularized Logistic Regression GD (42300/99999): loss=-4816.6436920311135\n",
      " Regularized Logistic Regression GD (42400/99999): loss=-4816.654040811587\n",
      " Regularized Logistic Regression GD (42500/99999): loss=-4816.664177122686\n",
      " Regularized Logistic Regression GD (42600/99999): loss=-4816.674105339944\n",
      " Regularized Logistic Regression GD (42700/99999): loss=-4816.683829748448\n",
      " Regularized Logistic Regression GD (42800/99999): loss=-4816.693354544718\n",
      " Regularized Logistic Regression GD (42900/99999): loss=-4816.702683838545\n",
      " Regularized Logistic Regression GD (43000/99999): loss=-4816.711821654793\n",
      " Regularized Logistic Regression GD (43100/99999): loss=-4816.72077193517\n",
      " Regularized Logistic Regression GD (43200/99999): loss=-4816.7295385399275\n",
      " Regularized Logistic Regression GD (43300/99999): loss=-4816.738125249585\n",
      " Regularized Logistic Regression GD (43400/99999): loss=-4816.746535766572\n",
      " Regularized Logistic Regression GD (43500/99999): loss=-4816.754773716825\n",
      " Regularized Logistic Regression GD (43600/99999): loss=-4816.762842651412\n",
      " Regularized Logistic Regression GD (43700/99999): loss=-4816.770746048069\n",
      " Regularized Logistic Regression GD (43800/99999): loss=-4816.778487312709\n",
      " Regularized Logistic Regression GD (43900/99999): loss=-4816.786069780929\n",
      " Regularized Logistic Regression GD (44000/99999): loss=-4816.793496719469\n",
      " Regularized Logistic Regression GD (44100/99999): loss=-4816.800771327619\n",
      " Regularized Logistic Regression GD (44200/99999): loss=-4816.807896738644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Regularized Logistic Regression GD (44300/99999): loss=-4816.814876021127\n",
      " Regularized Logistic Regression GD (44400/99999): loss=-4816.821712180339\n",
      " Regularized Logistic Regression GD (44500/99999): loss=-4816.828408159519\n",
      " Regularized Logistic Regression GD (44600/99999): loss=-4816.834966841185\n",
      " Regularized Logistic Regression GD (44700/99999): loss=-4816.841391048381\n",
      " Regularized Logistic Regression GD (44800/99999): loss=-4816.847683545903\n",
      " Regularized Logistic Regression GD (44900/99999): loss=-4816.853847041524\n",
      " Regularized Logistic Regression GD (45000/99999): loss=-4816.85988418716\n",
      " Regularized Logistic Regression GD (45100/99999): loss=-4816.8657975800215\n",
      " Regularized Logistic Regression GD (45200/99999): loss=-4816.871589763774\n",
      " Regularized Logistic Regression GD (45300/99999): loss=-4816.877263229618\n",
      " Regularized Logistic Regression GD (45400/99999): loss=-4816.882820417382\n",
      " Regularized Logistic Regression GD (45500/99999): loss=-4816.888263716602\n",
      " Regularized Logistic Regression GD (45600/99999): loss=-4816.893595467542\n",
      " Regularized Logistic Regression GD (45700/99999): loss=-4816.898817962223\n",
      " Regularized Logistic Regression GD (45800/99999): loss=-4816.9039334454255\n",
      " Regularized Logistic Regression GD (45900/99999): loss=-4816.90894411566\n",
      " Regularized Logistic Regression GD (46000/99999): loss=-4816.913852126122\n",
      " Regularized Logistic Regression GD (46100/99999): loss=-4816.918659585643\n",
      " Regularized Logistic Regression GD (46200/99999): loss=-4816.923368559593\n",
      " Regularized Logistic Regression GD (46300/99999): loss=-4816.927981070786\n",
      " Regularized Logistic Regression GD (46400/99999): loss=-4816.932499100364\n",
      " Regularized Logistic Regression GD (46500/99999): loss=-4816.936924588654\n",
      " Regularized Logistic Regression GD (46600/99999): loss=-4816.941259436013\n",
      " Regularized Logistic Regression GD (46700/99999): loss=-4816.94550550365\n",
      " Regularized Logistic Regression GD (46800/99999): loss=-4816.949664614445\n",
      " Regularized Logistic Regression GD (46900/99999): loss=-4816.953738553738\n",
      " Regularized Logistic Regression GD (47000/99999): loss=-4816.9577290700945\n",
      " Regularized Logistic Regression GD (47100/99999): loss=-4816.96163787609\n",
      " Regularized Logistic Regression GD (47200/99999): loss=-4816.965466649016\n",
      " Regularized Logistic Regression GD (47300/99999): loss=-4816.969217031652\n",
      " Regularized Logistic Regression GD (47400/99999): loss=-4816.972890632942\n",
      " Regularized Logistic Regression GD (47500/99999): loss=-4816.976489028718\n",
      " Regularized Logistic Regression GD (47600/99999): loss=-4816.980013762378\n",
      " Regularized Logistic Regression GD (47700/99999): loss=-4816.983466345544\n",
      " Regularized Logistic Regression GD (47800/99999): loss=-4816.986848258733\n",
      " Regularized Logistic Regression GD (47900/99999): loss=-4816.990160951996\n",
      " Regularized Logistic Regression GD (48000/99999): loss=-4816.993405845542\n",
      " Regularized Logistic Regression GD (48100/99999): loss=-4816.996584330361\n",
      " Regularized Logistic Regression GD (48200/99999): loss=-4816.999697768816\n",
      " Regularized Logistic Regression GD (48300/99999): loss=-4817.002747495252\n",
      " Regularized Logistic Regression GD (48400/99999): loss=-4817.0057348165565\n",
      " Regularized Logistic Regression GD (48500/99999): loss=-4817.008661012737\n",
      " Regularized Logistic Regression GD (48600/99999): loss=-4817.0115273374795\n",
      " Regularized Logistic Regression GD (48700/99999): loss=-4817.014335018666\n",
      " Regularized Logistic Regression GD (48800/99999): loss=-4817.017085258949\n",
      " Regularized Logistic Regression GD (48900/99999): loss=-4817.019779236229\n",
      " Regularized Logistic Regression GD (49000/99999): loss=-4817.022418104206\n",
      " Regularized Logistic Regression GD (49100/99999): loss=-4817.025002992836\n",
      " Regularized Logistic Regression GD (49200/99999): loss=-4817.027535008859\n",
      " Regularized Logistic Regression GD (49300/99999): loss=-4817.030015236258\n",
      " Regularized Logistic Regression GD (49400/99999): loss=-4817.0324447367275\n",
      " Regularized Logistic Regression GD (49500/99999): loss=-4817.03482455014\n",
      " Regularized Logistic Regression GD (49600/99999): loss=-4817.0371556949995\n",
      " Regularized Logistic Regression GD (49700/99999): loss=-4817.039439168866\n",
      " Regularized Logistic Regression GD (49800/99999): loss=-4817.041675948808\n",
      " Regularized Logistic Regression GD (49900/99999): loss=-4817.043866991809\n",
      " Regularized Logistic Regression GD (50000/99999): loss=-4817.046013235195\n",
      " Regularized Logistic Regression GD (50100/99999): loss=-4817.048115597013\n",
      " Regularized Logistic Regression GD (50200/99999): loss=-4817.050174976473\n",
      " Regularized Logistic Regression GD (50300/99999): loss=-4817.052192254295\n",
      " Regularized Logistic Regression GD (50400/99999): loss=-4817.054168293107\n",
      " Regularized Logistic Regression GD (50500/99999): loss=-4817.056103937818\n",
      " Regularized Logistic Regression GD (50600/99999): loss=-4817.05800001599\n",
      " Regularized Logistic Regression GD (50700/99999): loss=-4817.059857338169\n",
      " Regularized Logistic Regression GD (50800/99999): loss=-4817.061676698272\n",
      " Regularized Logistic Regression GD (50900/99999): loss=-4817.063458873902\n",
      " Regularized Logistic Regression GD (51000/99999): loss=-4817.065204626692\n",
      " Regularized Logistic Regression GD (51100/99999): loss=-4817.0669147026465\n",
      " Regularized Logistic Regression GD (51200/99999): loss=-4817.068589832439\n",
      " Regularized Logistic Regression GD (51300/99999): loss=-4817.07023073174\n",
      " Regularized Logistic Regression GD (51400/99999): loss=-4817.071838101538\n",
      " Regularized Logistic Regression GD (51500/99999): loss=-4817.0734126284315\n",
      " Regularized Logistic Regression GD (51600/99999): loss=-4817.074954984909\n",
      " Regularized Logistic Regression GD (51700/99999): loss=-4817.076465829669\n",
      " Regularized Logistic Regression GD (51800/99999): loss=-4817.077945807889\n",
      " Regularized Logistic Regression GD (51900/99999): loss=-4817.079395551493\n",
      " Regularized Logistic Regression GD (52000/99999): loss=-4817.080815679451\n",
      " Regularized Logistic Regression GD (52100/99999): loss=-4817.082206798014\n",
      " Regularized Logistic Regression GD (52200/99999): loss=-4817.083569501015\n",
      " Regularized Logistic Regression GD (52300/99999): loss=-4817.084904370084\n",
      " Regularized Logistic Regression GD (52400/99999): loss=-4817.086211974915\n",
      " Regularized Logistic Regression GD (52500/99999): loss=-4817.087492873529\n",
      " Regularized Logistic Regression GD (52600/99999): loss=-4817.0887476124935\n",
      " Regularized Logistic Regression GD (52700/99999): loss=-4817.089976727143\n",
      " Regularized Logistic Regression GD (52800/99999): loss=-4817.09118074187\n",
      " Regularized Logistic Regression GD (52900/99999): loss=-4817.0923601702725\n",
      " Regularized Logistic Regression GD (53000/99999): loss=-4817.0935155154375\n",
      " Regularized Logistic Regression GD (53100/99999): loss=-4817.094647270124\n",
      " Regularized Logistic Regression GD (53200/99999): loss=-4817.095755916991\n",
      " Regularized Logistic Regression GD (53300/99999): loss=-4817.096841928795\n",
      " Regularized Logistic Regression GD (53400/99999): loss=-4817.097905768592\n",
      " Regularized Logistic Regression GD (53500/99999): loss=-4817.098947889956\n",
      " Regularized Logistic Regression GD (53600/99999): loss=-4817.099968737161\n",
      " Regularized Logistic Regression GD (53700/99999): loss=-4817.100968745363\n",
      " Regularized Logistic Regression GD (53800/99999): loss=-4817.101948340798\n",
      " Regularized Logistic Regression GD (53900/99999): loss=-4817.102907940959\n",
      " Regularized Logistic Regression GD (54000/99999): loss=-4817.1038479548015\n",
      " Regularized Logistic Regression GD (54100/99999): loss=-4817.1047687828805\n",
      " Regularized Logistic Regression GD (54200/99999): loss=-4817.105670817537\n",
      " Regularized Logistic Regression GD (54300/99999): loss=-4817.106554443088\n",
      " Regularized Logistic Regression GD (54400/99999): loss=-4817.107420035959\n",
      " Regularized Logistic Regression GD (54500/99999): loss=-4817.108267964868\n",
      " Regularized Logistic Regression GD (54600/99999): loss=-4817.109098590973\n",
      " Regularized Logistic Regression GD (54700/99999): loss=-4817.109912268038\n",
      " Regularized Logistic Regression GD (54800/99999): loss=-4817.110709342575\n",
      " Regularized Logistic Regression GD (54900/99999): loss=-4817.111490153994\n",
      " Regularized Logistic Regression GD (55000/99999): loss=-4817.112255034745\n",
      " Regularized Logistic Regression GD (55100/99999): loss=-4817.113004310486\n",
      " Regularized Logistic Regression GD (55200/99999): loss=-4817.113738300184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Regularized Logistic Regression GD (55300/99999): loss=-4817.114457316294\n",
      " Regularized Logistic Regression GD (55400/99999): loss=-4817.115161664837\n",
      " Regularized Logistic Regression GD (55500/99999): loss=-4817.115851645603\n",
      " Regularized Logistic Regression GD (55600/99999): loss=-4817.116527552217\n",
      " Regularized Logistic Regression GD (55700/99999): loss=-4817.117189672299\n",
      " Regularized Logistic Regression GD (55800/99999): loss=-4817.117838287572\n",
      " Regularized Logistic Regression GD (55900/99999): loss=-4817.118473674004\n",
      " Regularized Logistic Regression GD (56000/99999): loss=-4817.119096101899\n",
      " Regularized Logistic Regression GD (56100/99999): loss=-4817.1197058360385\n",
      " Regularized Logistic Regression GD (56200/99999): loss=-4817.120303135764\n",
      " Regularized Logistic Regression GD (56300/99999): loss=-4817.120888255119\n",
      " Regularized Logistic Regression GD (56400/99999): loss=-4817.121461442951\n",
      " Regularized Logistic Regression GD (56500/99999): loss=-4817.1220229430055\n",
      " Regularized Logistic Regression GD (56600/99999): loss=-4817.122572994033\n",
      " Regularized Logistic Regression GD (56700/99999): loss=-4817.123111829909\n",
      " Regularized Logistic Regression GD (56800/99999): loss=-4817.123639679715\n",
      " Regularized Logistic Regression GD (56900/99999): loss=-4817.124156767844\n",
      " Regularized Logistic Regression GD (57000/99999): loss=-4817.124663314096\n",
      " Regularized Logistic Regression GD (57100/99999): loss=-4817.1251595337735\n",
      " Regularized Logistic Regression GD (57200/99999): loss=-4817.125645637774\n",
      " Regularized Logistic Regression GD (57300/99999): loss=-4817.126121832681\n",
      " Regularized Logistic Regression GD (57400/99999): loss=-4817.12658832084\n",
      " Regularized Logistic Regression GD (57500/99999): loss=-4817.127045300467\n",
      " Regularized Logistic Regression GD (57600/99999): loss=-4817.127492965718\n",
      " Regularized Logistic Regression GD (57700/99999): loss=-4817.127931506786\n",
      " Regularized Logistic Regression GD (57800/99999): loss=-4817.128361109953\n",
      " Regularized Logistic Regression GD (57900/99999): loss=-4817.128781957706\n",
      " Regularized Logistic Regression GD (58000/99999): loss=-4817.129194228798\n",
      " Regularized Logistic Regression GD (58100/99999): loss=-4817.129598098314\n",
      " Regularized Logistic Regression GD (58200/99999): loss=-4817.12999373777\n",
      " Regularized Logistic Regression GD (58300/99999): loss=-4817.130381315164\n",
      " Regularized Logistic Regression GD (58400/99999): loss=-4817.130760995072\n",
      " Regularized Logistic Regression GD (58500/99999): loss=-4817.131132938682\n",
      " Regularized Logistic Regression GD (58600/99999): loss=-4817.1314973039125\n",
      " Regularized Logistic Regression GD (58700/99999): loss=-4817.131854245427\n",
      " Regularized Logistic Regression GD (58800/99999): loss=-4817.132203914744\n",
      " Regularized Logistic Regression GD (58900/99999): loss=-4817.132546460278\n",
      " Regularized Logistic Regression GD (59000/99999): loss=-4817.132882027397\n",
      " Regularized Logistic Regression GD (59100/99999): loss=-4817.133210758521\n",
      " Regularized Logistic Regression GD (59200/99999): loss=-4817.1335327931365\n",
      " Regularized Logistic Regression GD (59300/99999): loss=-4817.133848267891\n",
      " Regularized Logistic Regression GD (59400/99999): loss=-4817.134157316632\n",
      " Regularized Logistic Regression GD (59500/99999): loss=-4817.134460070477\n",
      " Regularized Logistic Regression GD (59600/99999): loss=-4817.13475665785\n",
      " Regularized Logistic Regression GD (59700/99999): loss=-4817.135047204566\n",
      " Regularized Logistic Regression GD (59800/99999): loss=-4817.135331833856\n",
      " Regularized Logistic Regression GD (59900/99999): loss=-4817.135610666438\n",
      " Regularized Logistic Regression GD (60000/99999): loss=-4817.135883820565\n",
      " Regularized Logistic Regression GD (60100/99999): loss=-4817.136151412074\n",
      " Regularized Logistic Regression GD (60200/99999): loss=-4817.136413554418\n",
      " Regularized Logistic Regression GD (60300/99999): loss=-4817.13667035876\n",
      " Regularized Logistic Regression GD (60400/99999): loss=-4817.136921933963\n",
      " Regularized Logistic Regression GD (60500/99999): loss=-4817.137168386684\n",
      " Regularized Logistic Regression GD (60600/99999): loss=-4817.137409821397\n",
      " Regularized Logistic Regression GD (60700/99999): loss=-4817.137646340431\n",
      " Regularized Logistic Regression GD (60800/99999): loss=-4817.137878044042\n",
      " Regularized Logistic Regression GD (60900/99999): loss=-4817.1381050304235\n",
      " Regularized Logistic Regression GD (61000/99999): loss=-4817.13832739577\n",
      " Regularized Logistic Regression GD (61100/99999): loss=-4817.138545234308\n",
      " Regularized Logistic Regression GD (61200/99999): loss=-4817.138758638338\n",
      " Regularized Logistic Regression GD (61300/99999): loss=-4817.138967698277\n",
      " Regularized Logistic Regression GD (61400/99999): loss=-4817.139172502698\n",
      " Regularized Logistic Regression GD (61500/99999): loss=-4817.139373138364\n",
      " Regularized Logistic Regression GD (61600/99999): loss=-4817.139569690261\n",
      " Regularized Logistic Regression GD (61700/99999): loss=-4817.139762241643\n",
      " Regularized Logistic Regression GD (61800/99999): loss=-4817.13995087406\n",
      " Regularized Logistic Regression GD (61900/99999): loss=-4817.140135667398\n",
      " Regularized Logistic Regression GD (62000/99999): loss=-4817.140316699921\n",
      " Regularized Logistic Regression GD (62100/99999): loss=-4817.14049404828\n",
      " Regularized Logistic Regression GD (62200/99999): loss=-4817.140667787575\n",
      " Regularized Logistic Regression GD (62300/99999): loss=-4817.140837991362\n",
      " Regularized Logistic Regression GD (62400/99999): loss=-4817.141004731695\n",
      " Regularized Logistic Regression GD (62500/99999): loss=-4817.141168079166\n",
      " Regularized Logistic Regression GD (62600/99999): loss=-4817.141328102913\n",
      " Regularized Logistic Regression GD (62700/99999): loss=-4817.141484870685\n",
      " Regularized Logistic Regression GD (62800/99999): loss=-4817.141638448819\n",
      " Regularized Logistic Regression GD (62900/99999): loss=-4817.141788902316\n",
      " Regularized Logistic Regression GD (63000/99999): loss=-4817.141936294846\n",
      " Regularized Logistic Regression GD (63100/99999): loss=-4817.14208068878\n",
      " Regularized Logistic Regression GD (63200/99999): loss=-4817.142222145215\n",
      " Regularized Logistic Regression GD (63300/99999): loss=-4817.142360724004\n",
      " Regularized Logistic Regression GD (63400/99999): loss=-4817.1424964837715\n",
      " Regularized Logistic Regression GD (63500/99999): loss=-4817.142629481946\n",
      " Regularized Logistic Regression GD (63600/99999): loss=-4817.142759774805\n",
      " Regularized Logistic Regression GD (63700/99999): loss=-4817.142887417442\n",
      " Regularized Logistic Regression GD (63800/99999): loss=-4817.143012463853\n",
      " Regularized Logistic Regression GD (63900/99999): loss=-4817.143134966927\n",
      " Regularized Logistic Regression GD (64000/99999): loss=-4817.143254978456\n",
      " Regularized Logistic Regression GD (64100/99999): loss=-4817.143372549209\n",
      " Regularized Logistic Regression GD (64200/99999): loss=-4817.1434877288875\n",
      " Regularized Logistic Regression GD (64300/99999): loss=-4817.143600566199\n",
      " Regularized Logistic Regression GD (64400/99999): loss=-4817.143711108845\n",
      " Regularized Logistic Regression GD (64500/99999): loss=-4817.14381940356\n",
      " Regularized Logistic Regression GD (64600/99999): loss=-4817.143925496129\n",
      " Regularized Logistic Regression GD (64700/99999): loss=-4817.14402943139\n",
      " Regularized Logistic Regression GD (64800/99999): loss=-4817.144131253282\n",
      " Regularized Logistic Regression GD (64900/99999): loss=-4817.144231004832\n",
      " Regularized Logistic Regression GD (65000/99999): loss=-4817.144328728203\n",
      " Regularized Logistic Regression GD (65100/99999): loss=-4817.144424464693\n",
      " Regularized Logistic Regression GD (65200/99999): loss=-4817.144518254759\n",
      " Regularized Logistic Regression GD (65300/99999): loss=-4817.14461013802\n",
      " Regularized Logistic Regression GD (65400/99999): loss=-4817.144700153316\n",
      " Regularized Logistic Regression GD (65500/99999): loss=-4817.144788338664\n",
      " Regularized Logistic Regression GD (65600/99999): loss=-4817.144874731325\n",
      " Regularized Logistic Regression GD (65700/99999): loss=-4817.144959367789\n",
      " Regularized Logistic Regression GD (65800/99999): loss=-4817.145042283817\n",
      " Regularized Logistic Regression GD (65900/99999): loss=-4817.145123514422\n",
      " Regularized Logistic Regression GD (66000/99999): loss=-4817.145203093911\n",
      " Regularized Logistic Regression GD (66100/99999): loss=-4817.145281055902\n",
      " Regularized Logistic Regression GD (66200/99999): loss=-4817.145357433308\n",
      " Regularized Logistic Regression GD (66300/99999): loss=-4817.14543225839\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Regularized Logistic Regression GD (66400/99999): loss=-4817.145505562739\n",
      " Regularized Logistic Regression GD (66500/99999): loss=-4817.145577377308\n",
      " Regularized Logistic Regression GD (66600/99999): loss=-4817.145647732413\n",
      " Regularized Logistic Regression GD (66700/99999): loss=-4817.1457166577575\n",
      " Regularized Logistic Regression GD (66800/99999): loss=-4817.145784182436\n",
      " Regularized Logistic Regression GD (66900/99999): loss=-4817.145850334957\n",
      " Regularized Logistic Regression GD (67000/99999): loss=-4817.145915143238\n",
      " Regularized Logistic Regression GD (67100/99999): loss=-4817.145978634638\n",
      " Regularized Logistic Regression GD (67200/99999): loss=-4817.146040835946\n",
      " Regularized Logistic Regression GD (67300/99999): loss=-4817.146101773415\n",
      " Regularized Logistic Regression GD (67400/99999): loss=-4817.146161472758\n",
      " Regularized Logistic Regression GD (67500/99999): loss=-4817.14621995917\n",
      " Regularized Logistic Regression GD (67600/99999): loss=-4817.14627725732\n",
      " Regularized Logistic Regression GD (67700/99999): loss=-4817.146333391398\n",
      " Regularized Logistic Regression GD (67800/99999): loss=-4817.146388385072\n",
      " Regularized Logistic Regression GD (67900/99999): loss=-4817.146442261549\n",
      " Regularized Logistic Regression GD (68000/99999): loss=-4817.146495043548\n",
      " Regularized Logistic Regression GD (68100/99999): loss=-4817.146546753346\n",
      " Regularized Logistic Regression GD (68200/99999): loss=-4817.14659741274\n",
      " Regularized Logistic Regression GD (68300/99999): loss=-4817.146647043106\n",
      " Regularized Logistic Regression GD (68400/99999): loss=-4817.146695665366\n",
      " Regularized Logistic Regression GD (68500/99999): loss=-4817.146743300031\n",
      " Regularized Logistic Regression GD (68600/99999): loss=-4817.146789967187\n",
      " Regularized Logistic Regression GD (68700/99999): loss=-4817.146835686508\n",
      " Regularized Logistic Regression GD (68800/99999): loss=-4817.146880477272\n",
      " Regularized Logistic Regression GD (68900/99999): loss=-4817.146924358367\n",
      " Regularized Logistic Regression GD (69000/99999): loss=-4817.14696734829\n",
      " Regularized Logistic Regression GD (69100/99999): loss=-4817.1470094651595\n",
      " Regularized Logistic Regression GD (69200/99999): loss=-4817.147050726737\n",
      " Regularized Logistic Regression GD (69300/99999): loss=-4817.147091150407\n",
      " Regularized Logistic Regression GD (69400/99999): loss=-4817.147130753211\n",
      " Regularized Logistic Regression GD (69500/99999): loss=-4817.147169551841\n",
      " Regularized Logistic Regression GD (69600/99999): loss=-4817.147207562649\n",
      " Regularized Logistic Regression GD (69700/99999): loss=-4817.1472448016475\n",
      " Regularized Logistic Regression GD (69800/99999): loss=-4817.147281284538\n",
      " Regularized Logistic Regression GD (69900/99999): loss=-4817.147317026684\n",
      " Regularized Logistic Regression GD (70000/99999): loss=-4817.147352043147\n",
      " Regularized Logistic Regression GD (70100/99999): loss=-4817.147386348684\n",
      " Regularized Logistic Regression GD (70200/99999): loss=-4817.147419957743\n",
      " Regularized Logistic Regression GD (70300/99999): loss=-4817.147452884485\n",
      " Regularized Logistic Regression GD (70400/99999): loss=-4817.147485142776\n",
      " Regularized Logistic Regression GD (70500/99999): loss=-4817.147516746213\n",
      " Regularized Logistic Regression GD (70600/99999): loss=-4817.147547708097\n",
      " Regularized Logistic Regression GD (70700/99999): loss=-4817.147578041473\n",
      " Regularized Logistic Regression GD (70800/99999): loss=-4817.147607759114\n",
      " Regularized Logistic Regression GD (70900/99999): loss=-4817.147636873536\n",
      " Regularized Logistic Regression GD (71000/99999): loss=-4817.147665397002\n",
      " Regularized Logistic Regression GD (71100/99999): loss=-4817.147693341513\n",
      " Regularized Logistic Regression GD (71200/99999): loss=-4817.14772071885\n",
      " Regularized Logistic Regression GD (71300/99999): loss=-4817.147747540525\n",
      " Regularized Logistic Regression GD (71400/99999): loss=-4817.147773817837\n",
      " Regularized Logistic Regression GD (71500/99999): loss=-4817.147799561852\n",
      " Regularized Logistic Regression GD (71600/99999): loss=-4817.147824783394\n",
      " Regularized Logistic Regression GD (71700/99999): loss=-4817.14784949309\n",
      " Regularized Logistic Regression GD (71800/99999): loss=-4817.147873701339\n",
      " Regularized Logistic Regression GD (71900/99999): loss=-4817.147897418332\n",
      " Regularized Logistic Regression GD (72000/99999): loss=-4817.147920654049\n",
      " Regularized Logistic Regression GD (72100/99999): loss=-4817.147943418263\n",
      " Regularized Logistic Regression GD (72200/99999): loss=-4817.147965720557\n",
      " Regularized Logistic Regression GD (72300/99999): loss=-4817.147987570323\n",
      " Regularized Logistic Regression GD (72400/99999): loss=-4817.148008976748\n",
      " Regularized Logistic Regression GD (72500/99999): loss=-4817.148029948836\n",
      " Regularized Logistic Regression GD (72600/99999): loss=-4817.148050495412\n",
      " Regularized Logistic Regression GD (72700/99999): loss=-4817.148070625124\n",
      " Regularized Logistic Regression GD (72800/99999): loss=-4817.148090346441\n",
      " Regularized Logistic Regression GD (72900/99999): loss=-4817.148109667652\n",
      " Regularized Logistic Regression GD (73000/99999): loss=-4817.148128596889\n",
      " Regularized Logistic Regression GD (73100/99999): loss=-4817.148147142111\n",
      " Regularized Logistic Regression GD (73200/99999): loss=-4817.148165311125\n",
      " Regularized Logistic Regression GD (73300/99999): loss=-4817.148183111561\n",
      " Regularized Logistic Regression GD (73400/99999): loss=-4817.1482005509115\n",
      " Regularized Logistic Regression GD (73500/99999): loss=-4817.148217636512\n",
      " Regularized Logistic Regression GD (73600/99999): loss=-4817.148234375548\n",
      " Regularized Logistic Regression GD (73700/99999): loss=-4817.148250775048\n",
      " Regularized Logistic Regression GD (73800/99999): loss=-4817.148266841914\n",
      " Regularized Logistic Regression GD (73900/99999): loss=-4817.148282582902\n",
      " Regularized Logistic Regression GD (74000/99999): loss=-4817.148298004625\n",
      " Regularized Logistic Regression GD (74100/99999): loss=-4817.148313113576\n",
      " Regularized Logistic Regression GD (74200/99999): loss=-4817.1483279160975\n",
      " Regularized Logistic Regression GD (74300/99999): loss=-4817.1483424184125\n",
      " Regularized Logistic Regression GD (74400/99999): loss=-4817.148356626616\n",
      " Regularized Logistic Regression GD (74500/99999): loss=-4817.148370546684\n",
      " Regularized Logistic Regression GD (74600/99999): loss=-4817.148384184465\n",
      " Regularized Logistic Regression GD (74700/99999): loss=-4817.1483975456895\n",
      " Regularized Logistic Regression GD (74800/99999): loss=-4817.148410635974\n",
      " Regularized Logistic Regression GD (74900/99999): loss=-4817.148423460813\n",
      " Regularized Logistic Regression GD (75000/99999): loss=-4817.148436025604\n",
      " Regularized Logistic Regression GD (75100/99999): loss=-4817.148448335621\n",
      " Regularized Logistic Regression GD (75200/99999): loss=-4817.148460396042\n",
      " Regularized Logistic Regression GD (75300/99999): loss=-4817.148472211924\n",
      " Regularized Logistic Regression GD (75400/99999): loss=-4817.14848378824\n",
      " Regularized Logistic Regression GD (75500/99999): loss=-4817.148495129846\n",
      " Regularized Logistic Regression GD (75600/99999): loss=-4817.1485062415095\n",
      " Regularized Logistic Regression GD (75700/99999): loss=-4817.1485171279\n",
      " Regularized Logistic Regression GD (75800/99999): loss=-4817.148527793589\n",
      " Regularized Logistic Regression GD (75900/99999): loss=-4817.148538243052\n",
      " Regularized Logistic Regression GD (76000/99999): loss=-4817.148548480681\n",
      " Regularized Logistic Regression GD (76100/99999): loss=-4817.148558510774\n",
      " Regularized Logistic Regression GD (76200/99999): loss=-4817.148568337549\n",
      " Regularized Logistic Regression GD (76300/99999): loss=-4817.148577965122\n",
      " Regularized Logistic Regression GD (76400/99999): loss=-4817.148587397539\n",
      " Regularized Logistic Regression GD (76500/99999): loss=-4817.148596638765\n",
      " Regularized Logistic Regression GD (76600/99999): loss=-4817.148605692677\n",
      " Regularized Logistic Regression GD (76700/99999): loss=-4817.148614563073\n",
      " Regularized Logistic Regression GD (76800/99999): loss=-4817.148623253684\n",
      " Regularized Logistic Regression GD (76900/99999): loss=-4817.148631768152\n",
      " Regularized Logistic Regression GD (77000/99999): loss=-4817.148640110052\n",
      " Regularized Logistic Regression GD (77100/99999): loss=-4817.148648282888\n",
      " Regularized Logistic Regression GD (77200/99999): loss=-4817.148656290089\n",
      " Regularized Logistic Regression GD (77300/99999): loss=-4817.148664135016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Regularized Logistic Regression GD (77400/99999): loss=-4817.148671820955\n",
      " Regularized Logistic Regression GD (77500/99999): loss=-4817.148679351146\n",
      " Regularized Logistic Regression GD (77600/99999): loss=-4817.148686728736\n",
      " Regularized Logistic Regression GD (77700/99999): loss=-4817.148693956828\n",
      " Regularized Logistic Regression GD (77800/99999): loss=-4817.148701038446\n",
      " Regularized Logistic Regression GD (77900/99999): loss=-4817.148707976573\n",
      " Regularized Logistic Regression GD (78000/99999): loss=-4817.148714774111\n",
      " Regularized Logistic Regression GD (78100/99999): loss=-4817.148721433914\n",
      " Regularized Logistic Regression GD (78200/99999): loss=-4817.148727958781\n",
      " Regularized Logistic Regression GD (78300/99999): loss=-4817.148734351444\n",
      " Regularized Logistic Regression GD (78400/99999): loss=-4817.148740614584\n",
      " Regularized Logistic Regression GD (78500/99999): loss=-4817.148746750831\n",
      " Regularized Logistic Regression GD (78600/99999): loss=-4817.148752762758\n",
      " Regularized Logistic Regression GD (78700/99999): loss=-4817.1487586528865\n",
      " Regularized Logistic Regression GD (78800/99999): loss=-4817.148764423681\n",
      " Regularized Logistic Regression GD (78900/99999): loss=-4817.148770077573\n",
      " Regularized Logistic Regression GD (79000/99999): loss=-4817.14877561693\n",
      " Regularized Logistic Regression GD (79100/99999): loss=-4817.148781044068\n",
      " Regularized Logistic Regression GD (79200/99999): loss=-4817.148786361265\n",
      " Regularized Logistic Regression GD (79300/99999): loss=-4817.148791570755\n",
      " Regularized Logistic Regression GD (79400/99999): loss=-4817.14879667472\n",
      " Regularized Logistic Regression GD (79500/99999): loss=-4817.148801675302\n",
      " Regularized Logistic Regression GD (79600/99999): loss=-4817.148806574589\n",
      " Regularized Logistic Regression GD (79700/99999): loss=-4817.1488113746445\n",
      " Regularized Logistic Regression GD (79800/99999): loss=-4817.148816077477\n",
      " Regularized Logistic Regression GD (79900/99999): loss=-4817.148820685062\n",
      " Regularized Logistic Regression GD (80000/99999): loss=-4817.14882519932\n",
      " Regularized Logistic Regression GD (80100/99999): loss=-4817.148829622154\n",
      " Regularized Logistic Regression GD (80200/99999): loss=-4817.148833955412\n",
      " Regularized Logistic Regression GD (80300/99999): loss=-4817.148838200911\n",
      " Regularized Logistic Regression GD (80400/99999): loss=-4817.14884236043\n",
      " Regularized Logistic Regression GD (80500/99999): loss=-4817.148846435714\n",
      " Regularized Logistic Regression GD (80600/99999): loss=-4817.148850428469\n",
      " Regularized Logistic Regression GD (80700/99999): loss=-4817.148854340371\n",
      " Regularized Logistic Regression GD (80800/99999): loss=-4817.148858173054\n",
      " Regularized Logistic Regression GD (80900/99999): loss=-4817.148861928127\n",
      " Regularized Logistic Regression GD (81000/99999): loss=-4817.148865607162\n",
      " Regularized Logistic Regression GD (81100/99999): loss=-4817.148869211705\n",
      " Regularized Logistic Regression GD (81200/99999): loss=-4817.14887274326\n",
      " Regularized Logistic Regression GD (81300/99999): loss=-4817.148876203307\n",
      " Regularized Logistic Regression GD (81400/99999): loss=-4817.148879593297\n",
      " Regularized Logistic Regression GD (81500/99999): loss=-4817.148882914651\n",
      " Regularized Logistic Regression GD (81600/99999): loss=-4817.148886168759\n",
      " Regularized Logistic Regression GD (81700/99999): loss=-4817.148889356983\n",
      " Regularized Logistic Regression GD (81800/99999): loss=-4817.148892480659\n",
      " Regularized Logistic Regression GD (81900/99999): loss=-4817.148895541097\n",
      " Regularized Logistic Regression GD (82000/99999): loss=-4817.148898539575\n",
      " Regularized Logistic Regression GD (82100/99999): loss=-4817.148901477352\n",
      " Regularized Logistic Regression GD (82200/99999): loss=-4817.148904355658\n",
      " Regularized Logistic Regression GD (82300/99999): loss=-4817.148907175695\n",
      " Regularized Logistic Regression GD (82400/99999): loss=-4817.148909938645\n",
      " Regularized Logistic Regression GD (82500/99999): loss=-4817.148912645665\n",
      " Regularized Logistic Regression GD (82600/99999): loss=-4817.148915297894\n",
      " Regularized Logistic Regression GD (82700/99999): loss=-4817.148917896431\n",
      " Regularized Logistic Regression GD (82800/99999): loss=-4817.148920442373\n",
      " Regularized Logistic Regression GD (82900/99999): loss=-4817.1489229367835\n",
      " Regularized Logistic Regression GD (83000/99999): loss=-4817.148925380704\n",
      " Regularized Logistic Regression GD (83100/99999): loss=-4817.148927775159\n",
      " Regularized Logistic Regression GD (83200/99999): loss=-4817.148930121151\n",
      " Regularized Logistic Regression GD (83300/99999): loss=-4817.148932419662\n",
      " Regularized Logistic Regression GD (83400/99999): loss=-4817.1489346716535\n",
      " Regularized Logistic Regression GD (83500/99999): loss=-4817.14893687807\n",
      " Regularized Logistic Regression GD (83600/99999): loss=-4817.148939039835\n",
      " Regularized Logistic Regression GD (83700/99999): loss=-4817.148941157848\n",
      " Regularized Logistic Regression GD (83800/99999): loss=-4817.1489432329945\n",
      " Regularized Logistic Regression GD (83900/99999): loss=-4817.148945266153\n",
      " Regularized Logistic Regression GD (84000/99999): loss=-4817.148947258165\n",
      " Regularized Logistic Regression GD (84100/99999): loss=-4817.148949209864\n",
      " Regularized Logistic Regression GD (84200/99999): loss=-4817.1489511220725\n",
      " Regularized Logistic Regression GD (84300/99999): loss=-4817.148952995588\n",
      " Regularized Logistic Regression GD (84400/99999): loss=-4817.148954831192\n",
      " Regularized Logistic Regression GD (84500/99999): loss=-4817.148956629656\n",
      " Regularized Logistic Regression GD (84600/99999): loss=-4817.148958391727\n",
      " Regularized Logistic Regression GD (84700/99999): loss=-4817.148960118145\n",
      " Regularized Logistic Regression GD (84800/99999): loss=-4817.148961809636\n",
      " Regularized Logistic Regression GD (84900/99999): loss=-4817.148963466903\n",
      " Regularized Logistic Regression GD (85000/99999): loss=-4817.148965090639\n",
      " Regularized Logistic Regression GD (85100/99999): loss=-4817.1489666815205\n",
      " Regularized Logistic Regression GD (85200/99999): loss=-4817.148968240219\n",
      " Regularized Logistic Regression GD (85300/99999): loss=-4817.148969767387\n",
      " Regularized Logistic Regression GD (85400/99999): loss=-4817.148971263657\n",
      " Regularized Logistic Regression GD (85500/99999): loss=-4817.148972729656\n",
      " Regularized Logistic Regression GD (85600/99999): loss=-4817.148974166\n",
      " Regularized Logistic Regression GD (85700/99999): loss=-4817.148975573282\n",
      " Regularized Logistic Regression GD (85800/99999): loss=-4817.1489769521\n",
      " Regularized Logistic Regression GD (85900/99999): loss=-4817.148978303028\n",
      " Regularized Logistic Regression GD (86000/99999): loss=-4817.1489796266305\n",
      " Regularized Logistic Regression GD (86100/99999): loss=-4817.148980923455\n",
      " Regularized Logistic Regression GD (86200/99999): loss=-4817.148982194053\n",
      " Regularized Logistic Regression GD (86300/99999): loss=-4817.148983438946\n",
      " Regularized Logistic Regression GD (86400/99999): loss=-4817.14898465866\n",
      " Regularized Logistic Regression GD (86500/99999): loss=-4817.148985853705\n",
      " Regularized Logistic Regression GD (86600/99999): loss=-4817.1489870245805\n",
      " Regularized Logistic Regression GD (86700/99999): loss=-4817.14898817177\n",
      " Regularized Logistic Regression GD (86800/99999): loss=-4817.148989295762\n",
      " Regularized Logistic Regression GD (86900/99999): loss=-4817.148990397021\n",
      " Regularized Logistic Regression GD (87000/99999): loss=-4817.148991476007\n",
      " Regularized Logistic Regression GD (87100/99999): loss=-4817.148992533174\n",
      " Regularized Logistic Regression GD (87200/99999): loss=-4817.14899356896\n",
      " Regularized Logistic Regression GD (87300/99999): loss=-4817.148994583798\n",
      "Accuracy ratio = 0.654\n",
      "Test loss = -1090.215\n",
      "Train loss = -4817.149\n"
     ]
    }
   ],
   "source": [
    "lambda_ = 10000\n",
    "gamma = 1e-8\n",
    "w_rlrgd, loss_rlrgd = reg_logistic_regression(y_train, tx_train, lambda_, w_init, max_iter, gamma, pr=True, adapt_gamma = False)\n",
    "rlrgd_prediction = predict_labels(w_rlrgd, tx_test)\n",
    "acc_rlrgd = accuracy_ratio(rlrgd_prediction, y_test)\n",
    "print('Accuracy ratio = %.3f'%acc_rlrgd)\n",
    "print('Test loss = %.3f'%compute_loss_logistic(y_test, tx_test, w_rlrgd))\n",
    "print('Train loss = %.3f'%loss_rlrgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T09:25:28.491664Z",
     "start_time": "2019-10-16T09:25:27.995714Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reset css and font defaults in:\r\n",
      "/home/daniel/.jupyter/custom &\r\n",
      "/home/daniel/.local/share/jupyter/nbextensions\r\n"
     ]
    }
   ],
   "source": [
    "!jt -r -cellw=90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
