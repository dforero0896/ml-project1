{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T18:08:14.387486Z",
     "start_time": "2019-10-18T18:08:14.153089Z"
    }
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "from implementations import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T18:08:33.728339Z",
     "start_time": "2019-10-18T18:08:14.389345Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read train_y=LABELS, train_x=FEATURES and train_id=EVENT_IDS from dataset.\n",
    "subsamp = True\n",
    "y, x, id_ = load_csv_data('../data/train.csv', sub_sample=subsamp)\n",
    "y_out_test, x_out_test, id_out_test = load_csv_data('../data/test.csv', sub_sample=subsamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T18:28:52.668100Z",
     "start_time": "2019-10-18T18:28:51.985091Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3500, 61), (1500, 61))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean=True\n",
    "degree = 1\n",
    "feature_expansion = False\n",
    "x_train, y_train, x_test, y_test = split_data(x, y, ratio=0.7, seed = 42)\n",
    "if clean:\n",
    "    # Clean data\n",
    "    y_train, x_train = clean_data(y_train, x_train)\n",
    "    y_test, x_test = clean_data(y_test, x_test)\n",
    "# Standardize data\n",
    "x_train_std = standardize_features(x_train)\n",
    "x_test_std = standardize_features(x_test)\n",
    "x_train = x_train_std[0]\n",
    "x_test = x_test_std[0]\n",
    "# Build data matrix\n",
    "tx_train = build_poly(x_train, degree)\n",
    "tx_test = build_poly(x_test, degree)\n",
    "tx_train.shape, tx_test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T18:29:24.387727Z",
     "start_time": "2019-10-18T18:29:03.744434Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "w_init = np.array([0] * tx_train.shape[1])\n",
    "max_iter = 400\n",
    "gamma = 0.0009\n",
    "w_gd, loss_gd = least_squares_GD(y_train,\n",
    "                                 tx_train,\n",
    "                                 w_init,\n",
    "                                 max_iter,\n",
    "                                 gamma,\n",
    "                                 pr=True,\n",
    "                                 adapt_gamma=False,\n",
    "                                 kind='mse',\n",
    "                                accel=True)\n",
    "gd_prediction = predict_labels(w_gd, tx_test)\n",
    "acc_gd = accuracy_ratio(gd_prediction, y_test)\n",
    "print('Accuracy ratio = %.3f' % acc_gd)\n",
    "print('Test loss = %.3f' % compute_loss(y_test, tx_test, w_gd))\n",
    "print('Train loss = %.3f' % loss_gd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T12:37:03.486835Z",
     "start_time": "2019-10-17T12:37:03.480662Z"
    }
   },
   "source": [
    "np.savetxt('../data/w_gd_acc.dat', w_gd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T17:25:23.973086Z",
     "start_time": "2019-10-18T17:25:21.157754Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "w_init = np.array([0] * tx_train.shape[1])\n",
    "max_iter = 5000\n",
    "gamma = 1e-4\n",
    "batch_size = 1\n",
    "\n",
    "w_sgd, loss_sgd = least_squares_SGD(y_train,\n",
    "                                    tx_train,\n",
    "                                    w_init,\n",
    "                                    batch_size,\n",
    "                                    max_iter,\n",
    "                                    gamma,\n",
    "                                    pr=True,\n",
    "                                    adapt_gamma=False,\n",
    "                                    choose_best=True)\n",
    "sgd_prediction = predict_labels(w_sgd, tx_test)\n",
    "acc_sgd = accuracy_ratio(sgd_prediction, y_test)\n",
    "print('Accuracy ratio = %.2f' % acc_sgd)\n",
    "print('Test loss = %.2e' % compute_loss(y_test, tx_test, w_sgd))\n",
    "print('Train loss = %.2e' % loss_sgd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T18:29:58.126363Z",
     "start_time": "2019-10-18T18:29:58.071738Z"
    }
   },
   "outputs": [],
   "source": [
    "w_lsq, loss_lsq = least_squares(y_train, tx_train)\n",
    "lsq_prediction = predict_labels(w_lsq, tx_test)\n",
    "acc_lsq = accuracy_ratio(lsq_prediction, y_test)\n",
    "print('Accuracy ratio = %.2f' % acc_lsq)\n",
    "print('Train loss = %.2f' % loss_lsq)\n",
    "print('Test loss = %.2e' % compute_loss(y_test, tx_test, w_lsq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T12:45:30.271461Z",
     "start_time": "2019-10-17T12:45:30.267909Z"
    }
   },
   "source": [
    "np.savetxt('../data/w_lsq.dat', w_lsq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T18:31:55.175315Z",
     "start_time": "2019-10-18T18:31:55.126195Z"
    }
   },
   "outputs": [],
   "source": [
    "lambda_ = 3.3e-2\n",
    "w_rr, loss_rr = ridge_regression(y_train, tx_train, lambda_)\n",
    "rr_prediction = predict_labels(w_rr, tx_test)\n",
    "acc_rr = accuracy_ratio(rr_prediction, y_test)\n",
    "print('Accuracy ratio = %.3f'%acc_rr)\n",
    "print('Test loss = %.3f'%compute_loss(y_test, tx_test, w_rr))\n",
    "print('Train loss = %.3f'%loss_rr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T15:18:22.322364Z",
     "start_time": "2019-10-18T15:18:22.309384Z"
    }
   },
   "source": [
    "np.savetxt('../data/w_rr.dat', w_rr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T18:30:07.069000Z",
     "start_time": "2019-10-18T18:30:07.055291Z"
    }
   },
   "outputs": [],
   "source": [
    "def ridge_regression_demo(x, y, degree, ratio, seed):\n",
    "    \"\"\"ridge regression demo.\"\"\"\n",
    "    # define parameter\n",
    "    lambdas = np.logspace(-5, 3, 50)\n",
    "    # split the data, and return train and test data\n",
    "    x_train, y_train, x_test, y_test = split_data(x, y, ratio, seed)\n",
    "    #Clean\n",
    "    if clean:\n",
    "        y_train, x_train = clean_data(y_train, x_train)\n",
    "        y_test, x_test = clean_data(y_test, x_test)\n",
    "    # form train and test data with offset column\n",
    "    x_train_std = standardize_features(x_train)[0]\n",
    "    x_test_std = standardize_features(x_test)[0]\n",
    "    tx_train=build_poly(x_train_std, degree)\n",
    "    tx_test=build_poly(x_test_std, degree)\n",
    "    rmse_tr = []\n",
    "    rmse_te = []\n",
    "    accuracies = []\n",
    "    for ind, lambda_ in enumerate(lambdas):\n",
    "        # calcualte weight through least square.\n",
    "        w_train, loss_train = ridge_regression(y_train, tx_train, lambda_)\n",
    "        rmse_tr.append(np.sqrt(2*loss_train))\n",
    "        rmse_te.append(np.sqrt(2*compute_loss(y_test, tx_test, w_train, kind = 'mse')))\n",
    "        accuracies.append(accuracy_ratio(predict_labels(w_train, tx_test), y_test))\n",
    "        print(\"proportion={p}, degree={d}, lambda={l:.3e}, Training RMSE={tr:.3f}, Testing RMSE={te:.3f}, Accuracy={ac:.3f}\".format(\n",
    "               p=ratio, d=degree, l=lambda_, tr=rmse_tr[ind], te=rmse_te[ind], ac=accuracies[ind]))\n",
    "        \n",
    "    # Plot the obtained results\n",
    "    plot_train_test(rmse_tr, rmse_te, lambdas, degree)\n",
    "    plt.figure()\n",
    "    plt.semilogx(lambdas,accuracies, marker='o')\n",
    "def plot_train_test(train_errors, test_errors, lambdas, degree):\n",
    "    \"\"\"\n",
    "    train_errors, test_errors and lambas should be list (of the same size) the respective train error and test error for a given lambda,\n",
    "    * lambda[0] = 1\n",
    "    * train_errors[0] = RMSE of a ridge regression on the train set\n",
    "    * test_errors[0] = RMSE of the parameter found by ridge regression applied on the test set\n",
    "    \n",
    "    degree is just used for the title of the plot.\n",
    "    \"\"\"\n",
    "    plt.semilogx(lambdas, train_errors, color='b', marker='*', label=\"Train error\")\n",
    "    plt.semilogx(lambdas, test_errors, color='r', marker='*', label=\"Test error\")\n",
    "    plt.xlabel(\"lambda\")\n",
    "    plt.ylabel(\"RMSE\")\n",
    "    plt.title(\"Ridge regression for polynomial degree \" + str(degree))\n",
    "    leg = plt.legend(loc=1, shadow=True)\n",
    "    leg.draw_frame(False)\n",
    "    plt.savefig(\"../results/ridge_regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T18:30:11.749743Z",
     "start_time": "2019-10-18T18:30:08.141908Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "seed = 42\n",
    "degree = 2\n",
    "split_ratio = 0.9\n",
    "ridge_regression_demo(x, y, degree, split_ratio, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T18:30:20.729742Z",
     "start_time": "2019-10-18T18:30:20.721124Z"
    }
   },
   "outputs": [],
   "source": [
    "def cross_validation(y, x, k_indices, k, lambda_, degree):\n",
    "    \"\"\"return the loss of ridge regression.\"\"\"\n",
    "    # get k'th subgroup in test, others in train\n",
    "    id_test = k_indices[k]\n",
    "    id_train = k_indices[~(np.arange(k_indices.shape[0]) == k)].reshape(-1)\n",
    "    x_test = x[id_test]\n",
    "    x_train = x[id_train]\n",
    "    y_test = y[id_test]\n",
    "    y_train = y[id_train]\n",
    "    #Clean\n",
    "    if clean:\n",
    "        y_train, x_train = clean_data(y_train, x_train)\n",
    "        y_test, x_test = clean_data(y_test, x_test)\n",
    "    # Standardize\n",
    "    x_train_std = standardize(x_train)[0]\n",
    "    x_test_std = standardize(x_test)[0]\n",
    "    # Define feature matrix\n",
    "    tx_train = build_poly(x_train_std, degree)\n",
    "    tx_test = build_poly(x_test_std, degree)\n",
    "    # ridge regression\n",
    "    weight, loss_tr = ridge_regression(y_train, tx_train, lambda_)\n",
    "    # calculate the loss for train and test data\n",
    "    loss_te = compute_loss(y_test, tx_test, weight)\n",
    "    accuracy = accuracy_ratio(predict_labels(weight, tx_test), y_test)\n",
    "\n",
    "    return loss_tr, loss_te, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T18:31:38.710840Z",
     "start_time": "2019-10-18T18:30:26.506760Z"
    }
   },
   "outputs": [],
   "source": [
    "def cross_validation_visualization(lambds, mse_tr, mse_te):\n",
    "    \"\"\"visualization the curves of mse_tr and mse_te.\"\"\"\n",
    "    best_l_err = lambds[np.argmin(mse_te)]\n",
    "    print('Best lambda from error: %.2e'%best_l_err)\n",
    "    plt.semilogx(lambds, mse_tr, marker=\".\", color='b', label='train error')\n",
    "    plt.semilogx(lambds, mse_te, marker=\".\", color='r', label='test error')\n",
    "    plt.axvline(best_l_err, c = 'g', label = '$\\lambda^*_{rmse}=%.1e$'%best_l_err, ls = ':')\n",
    "    plt.xlabel(\"lambda\")\n",
    "    plt.ylabel(\"rmse\")\n",
    "    plt.title(\"cross validation\")\n",
    "    plt.legend(loc=0)\n",
    "    plt.grid(True)\n",
    "    plt.savefig(\"../results/cross_validation\")\n",
    "def cross_validation_visualization_accuracy(lambdas, accuracies):\n",
    "    \"\"\"visualization the curves of mse_tr and mse_te.\"\"\"\n",
    "    plt.semilogx(lambdas, accuracies, lw =2, marker = '*', label = 'Accuracy ratio')\n",
    "    best_l_acc = lambdas[np.argmax(accuracies)]\n",
    "    plt.axvline(best_l_acc, c= 'k', label = '$\\lambda^*_{acc}=%.1e$'%best_l_acc, ls = ':')\n",
    "    print('Best lambda from accuracy: %.2e'%best_l_acc)\n",
    "    plt.xlabel(\"lambda\")\n",
    "    plt.ylabel(\"accuracy\")\n",
    "    plt.title(\"cross validation\")\n",
    "    plt.legend(loc=0)\n",
    "    plt.grid(True)\n",
    "    plt.savefig(\"../results/cross_validation_accuracies\")\n",
    "def cross_validation_demo():\n",
    "    seed = 42\n",
    "    degree = 2\n",
    "    k_fold = 4\n",
    "    lambdas = np.logspace(-7, 3, 30)\n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    # define lists to store the loss of training data and test data\n",
    "    rmse_tr = []\n",
    "    rmse_te = []\n",
    "    std_tr = []\n",
    "    std_te = []\n",
    "    accuracies = []\n",
    "    # cross validation\n",
    "    for ind, lambda_ in enumerate(lambdas):\n",
    "        x_validation = np.array([cross_validation(y, x, k_indices, k, lambda_, degree) for k in range(k_fold)])\n",
    "        rmse_tr.append(np.mean(np.sqrt(2 * x_validation[:, 0])))\n",
    "        rmse_te.append(np.mean(np.sqrt(2 * x_validation[:, 1])))\n",
    "        std_tr.append(np.std(np.sqrt(2 * x_validation[:, 0])))\n",
    "        std_te.append(np.std(np.sqrt(2 * x_validation[:, 1])))\n",
    "        accuracies.append(np.mean(x_validation[:,2]))\n",
    "    cross_validation_visualization_accuracy(lambdas, accuracies)\n",
    "    plt.figure()\n",
    "    cross_validation_visualization(lambdas, rmse_tr, rmse_te)\n",
    "\n",
    "cross_validation_demo()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Bias-Variance test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T12:53:17.453735Z",
     "start_time": "2019-10-17T12:50:27.446578Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def bias_variance_decomposition_visualization(degrees, rmse_tr, rmse_te):\n",
    "    \"\"\"visualize the bias variance decomposition.\"\"\"\n",
    "    rmse_tr_mean = np.expand_dims(np.mean(rmse_tr, axis=0), axis=0)\n",
    "    rmse_te_mean = np.expand_dims(np.mean(rmse_te, axis=0), axis=0)\n",
    "    print(rmse_te_mean, rmse_tr_mean)\n",
    "    plt.plot(degrees,\n",
    "             rmse_tr.T,\n",
    "             'b',\n",
    "             linestyle=\"-\",\n",
    "             color=([0.7, 0.7, 1]),\n",
    "             label='train',\n",
    "             linewidth=0.3)\n",
    "    plt.plot(degrees,\n",
    "             rmse_te.T,\n",
    "             'r',\n",
    "             linestyle=\"-\",\n",
    "             color=[1, 0.7, 0.7],\n",
    "             label='test',\n",
    "             linewidth=0.3)\n",
    "    plt.plot(degrees,\n",
    "             rmse_tr_mean.T,\n",
    "             'b',\n",
    "             linestyle=\"-\",\n",
    "             label='train',\n",
    "             linewidth=3)\n",
    "    plt.plot(degrees,\n",
    "             rmse_te_mean.T,\n",
    "             'r',\n",
    "             linestyle=\"-\",\n",
    "             label='test',\n",
    "             linewidth=3)\n",
    "    plt.ylim(0.7, 1)\n",
    "    plt.xlabel(\"degree\")\n",
    "    plt.ylabel(\"error\")\n",
    "    plt.title(\"Bias-Variance Decomposition\")\n",
    "    plt.savefig(\"bias_variance\")\n",
    "\n",
    "\n",
    "def bias_variance_demo():\n",
    "    \"\"\"The entry.\"\"\"\n",
    "    # define parameters\n",
    "    seeds = range(100)\n",
    "    ratio_train = 0.5\n",
    "    degrees = range(1, 8)\n",
    "    # define list to store the variable\n",
    "    rmse_tr = np.empty((len(seeds), len(degrees)))\n",
    "    rmse_te = np.empty((len(seeds), len(degrees)))\n",
    "    for index_seed, seed in enumerate(seeds):\n",
    "        np.random.seed(seed)\n",
    "        # split data with a specific seed\n",
    "        x_train, y_train, x_test, y_test = split_data(x, y, ratio_train, seed)\n",
    "        x_train_std = standardize(x_train)[0]\n",
    "        x_test_std = standardize(x_test)[0]\n",
    "        for index_degrees, degree in enumerate(degrees):\n",
    "            tx_train = build_poly(x_train_std, degree)\n",
    "            tx_test = build_poly(x_test_std, degree)\n",
    "            weight, loss_tr = ridge_regression(y_train, tx_train, 1.89e-05 )\n",
    "            loss_te = compute_loss(y_test, tx_test, weight, kind='mse')\n",
    "            rmse_tr[index_seed, index_degrees] = np.sqrt(2 * loss_tr)\n",
    "            rmse_te[index_seed, index_degrees] = np.sqrt(2 * loss_te)\n",
    "    bias_variance_decomposition_visualization(degrees, rmse_tr, rmse_te)\n",
    "\n",
    "\n",
    "bias_variance_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T12:54:53.029395Z",
     "start_time": "2019-10-17T12:54:47.011563Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(id_out_test.shape)\n",
    "x_out_test_std = standardize_features(x_out_test)\n",
    "x_out = x_out_test_std[0]\n",
    "tx_out = build_poly(x_out, 2)\n",
    "\n",
    "create_csv_submission(id_out_test, predict_labels(w_rr, tx_out) , '../results/rr_pred.csv')\n",
    "create_csv_submission(id_out_test, predict_labels(w_gd, tx_out) , '../results/gd_pred_accel.csv')\n",
    "create_csv_submission(id_out_test, predict_labels(w_lsq, tx_out) , '../results/lsq_pred.csv')\n",
    "#create_csv_submission(id_out_test, predict_labels(w_sgd, tx_out) , '../results/sgd_pred_noadapt.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T18:32:32.794589Z",
     "start_time": "2019-10-18T18:32:32.777297Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_labels_log(weights, data):\n",
    "    \"\"\"Generates class predictions given weights, and a test data matrix\"\"\"\n",
    "    y_pred = np.dot(data, weights)\n",
    "    y_pred[np.where(y_pred <= 0.5)] = -1\n",
    "    y_pred[np.where(y_pred > 0.5)] = 1\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T18:32:38.319778Z",
     "start_time": "2019-10-18T18:32:38.223130Z"
    }
   },
   "outputs": [],
   "source": [
    "y_train_log = np.copy(y_train)\n",
    "y_train_log[y_train == -1] = 0\n",
    "\n",
    "y_test_log = np.copy(y_test)\n",
    "y_test_log[y_test == -1] = 0\n",
    "\n",
    "w_init = np.array([0] * tx_train.shape[1])\n",
    "max_iter = 400\n",
    "gamma = 1e-6\n",
    "w_lrgd, loss_lrgd = logistic_regression(y_train_log,\n",
    "                                        tx_train,\n",
    "                                        w_init,\n",
    "                                        max_iter,\n",
    "                                        gamma,\n",
    "                                        pr=True,\n",
    "                                        adapt_gamma=False,\n",
    "                                       accel=True)\n",
    "\n",
    "lrgd_prediction = predict_labels_log(w_lrgd, tx_test)\n",
    "print(tx_test.dot(w_lrgd))\n",
    "acc_lrgd = accuracy_ratio(lrgd_prediction, y_test)\n",
    "\n",
    "print('Accuracy ratio = %.3f' % acc_lrgd)\n",
    "print('Test loss = %.3f' % compute_loss_logistic(y_test_log, tx_test, w_lrgd))\n",
    "print('Train loss = %.3f' % loss_lrgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T18:33:13.633702Z",
     "start_time": "2019-10-18T18:32:41.270424Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lambda_ = 500\n",
    "gamma = 1e-7\n",
    "w_rlrgd, loss_rlrgd = reg_logistic_regression(y_train_log,\n",
    "                                              tx_train,\n",
    "                                              lambda_,\n",
    "                                              w_init,\n",
    "                                              max_iter,\n",
    "                                              gamma,\n",
    "                                              pr=True,\n",
    "                                              adapt_gamma=False, \n",
    "                                              accel=False)\n",
    "rlrgd_prediction = predict_labels_log(w_rlrgd, tx_test)\n",
    "acc_rlrgd = accuracy_ratio(rlrgd_prediction, y_test)\n",
    "print('Accuracy ratio = %.3f' % acc_rlrgd)\n",
    "print('Test loss = %.3f' % compute_loss_logistic(y_test_log, tx_test, w_rlrgd))\n",
    "print('Train loss = %.3f' % loss_rlrgd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T17:51:14.763658Z",
     "start_time": "2019-10-18T17:51:14.741402Z"
    }
   },
   "source": [
    "def cross_validation(y, x, k_indices, k, lambda_, degree):\n",
    "    \"\"\"return the loss of ridge regression.\"\"\"\n",
    "    gamma = 1e-8\n",
    "    # get k'th subgroup in test, others in train\n",
    "    id_test = k_indices[k]\n",
    "    id_train = k_indices[~(np.arange(k_indices.shape[0]) == k)].reshape(-1)\n",
    "    x_test = x[id_test]\n",
    "    x_train = x[id_train]\n",
    "    y_test = y[id_test]\n",
    "    y_train = y[id_train]\n",
    "    #Clean\n",
    "    if clean:\n",
    "        y_train, x_train = clean_data(y_train, x_train)\n",
    "        y_test, x_test = clean_data(y_test, x_test)\n",
    "    # Standardize\n",
    "    x_train_std = standardize(x_train)[0]\n",
    "    x_test_std = standardize(x_test)[0]\n",
    "    # Define feature matrix\n",
    "    tx_train = build_poly(x_train_std, degree)\n",
    "    tx_test = build_poly(x_test_std, degree)\n",
    "    y_train_log = np.copy(y_train)\n",
    "    y_train_log[y_train == -1] = 0\n",
    "\n",
    "    y_test_log = np.copy(y_test)\n",
    "    y_test_log[y_test == -1] = 0\n",
    "    # logistic regression\n",
    "    weight, loss_tr = reg_logistic_regression(y_train_log,\n",
    "                                              tx_train,\n",
    "                                              lambda_,\n",
    "                                              w_init,\n",
    "                                              max_iter,\n",
    "                                              gamma,\n",
    "                                              pr=True,\n",
    "                                              adapt_gamma=False)\n",
    "    # calculate the loss for train and test data\n",
    "    loss_te = compute_loss_logistic(y_test, tx_test, weight)\n",
    "    accuracy = accuracy_ratio(predict_labels(weight, tx_test), y_test)\n",
    "\n",
    "    return loss_tr, loss_te, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T17:57:34.213341Z",
     "start_time": "2019-10-18T17:51:15.297351Z"
    }
   },
   "outputs": [],
   "source": [
    "cross_validation_demo()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
