{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T18:08:14.387486Z",
     "start_time": "2019-10-18T18:08:14.153089Z"
    }
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "from implementations import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T18:08:33.728339Z",
     "start_time": "2019-10-18T18:08:14.389345Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read train_y=LABELS, train_x=FEATURES and train_id=EVENT_IDS from dataset.\n",
    "subsamp = True\n",
    "y, x, id_ = load_csv_data('../data/train.csv', sub_sample=subsamp)\n",
    "y_out_test, x_out_test, id_out_test = load_csv_data('../data/test.csv', sub_sample=subsamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T18:28:52.668100Z",
     "start_time": "2019-10-18T18:28:51.985091Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3500, 31), (1500, 31))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean=True\n",
    "degree = 1\n",
    "feature_expansion = False\n",
    "x_train, y_train, x_test, y_test = split_data(x, y, ratio=0.7, seed = 42)\n",
    "if clean:\n",
    "    # Clean data\n",
    "    y_train, x_train = clean_data(y_train, x_train)\n",
    "    y_test, x_test = clean_data(y_test, x_test)\n",
    "# Standardize data\n",
    "x_train_std = standardize_features(x_train)\n",
    "x_test_std = standardize_features(x_test)\n",
    "x_train = x_train_std[0]\n",
    "x_test = x_test_std[0]\n",
    "# Build data matrix\n",
    "tx_train = build_poly(x_train, degree)\n",
    "tx_test = build_poly(x_test, degree)\n",
    "tx_train.shape, tx_test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T18:29:24.387727Z",
     "start_time": "2019-10-18T18:29:03.744434Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "w_init = np.array([0] * tx_train.shape[1])\n",
    "max_iter = 400\n",
    "gamma = 0.0009\n",
    "w_gd, loss_gd = least_squares_GD(y_train,\n",
    "                                 tx_train,\n",
    "                                 w_init,\n",
    "                                 max_iter,\n",
    "                                 gamma,\n",
    "                                 pr=True,\n",
    "                                 adapt_gamma=False,\n",
    "                                 kind='mse',\n",
    "                                accel=True)\n",
    "gd_prediction = predict_labels(w_gd, tx_test)\n",
    "acc_gd = accuracy_ratio(gd_prediction, y_test)\n",
    "print('Accuracy ratio = %.3f' % acc_gd)\n",
    "print('Test loss = %.3f' % compute_loss(y_test, tx_test, w_gd))\n",
    "print('Train loss = %.3f' % loss_gd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T12:37:03.486835Z",
     "start_time": "2019-10-17T12:37:03.480662Z"
    }
   },
   "source": [
    "np.savetxt('../data/w_gd_acc.dat', w_gd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T17:25:23.973086Z",
     "start_time": "2019-10-18T17:25:21.157754Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "w_init = np.array([0] * tx_train.shape[1])\n",
    "max_iter = 5000\n",
    "gamma = 1e-4\n",
    "batch_size = 1\n",
    "\n",
    "w_sgd, loss_sgd = least_squares_SGD(y_train,\n",
    "                                    tx_train,\n",
    "                                    w_init,\n",
    "                                    batch_size,\n",
    "                                    max_iter,\n",
    "                                    gamma,\n",
    "                                    pr=True,\n",
    "                                    adapt_gamma=False,\n",
    "                                    choose_best=True)\n",
    "sgd_prediction = predict_labels(w_sgd, tx_test)\n",
    "acc_sgd = accuracy_ratio(sgd_prediction, y_test)\n",
    "print('Accuracy ratio = %.2f' % acc_sgd)\n",
    "print('Test loss = %.2e' % compute_loss(y_test, tx_test, w_sgd))\n",
    "print('Train loss = %.2e' % loss_sgd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T18:29:58.126363Z",
     "start_time": "2019-10-18T18:29:58.071738Z"
    }
   },
   "outputs": [],
   "source": [
    "w_lsq, loss_lsq = least_squares(y_train, tx_train)\n",
    "lsq_prediction = predict_labels(w_lsq, tx_test)\n",
    "acc_lsq = accuracy_ratio(lsq_prediction, y_test)\n",
    "print('Accuracy ratio = %.2f' % acc_lsq)\n",
    "print('Train loss = %.2f' % loss_lsq)\n",
    "print('Test loss = %.2e' % compute_loss(y_test, tx_test, w_lsq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T12:45:30.271461Z",
     "start_time": "2019-10-17T12:45:30.267909Z"
    }
   },
   "source": [
    "np.savetxt('../data/w_lsq.dat', w_lsq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T18:31:55.175315Z",
     "start_time": "2019-10-18T18:31:55.126195Z"
    }
   },
   "outputs": [],
   "source": [
    "lambda_ = 3.3e-2\n",
    "w_rr, loss_rr = ridge_regression(y_train, tx_train, lambda_)\n",
    "rr_prediction = predict_labels(w_rr, tx_test)\n",
    "acc_rr = accuracy_ratio(rr_prediction, y_test)\n",
    "print('Accuracy ratio = %.3f'%acc_rr)\n",
    "print('Test loss = %.3f'%compute_loss(y_test, tx_test, w_rr))\n",
    "print('Train loss = %.3f'%loss_rr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T15:18:22.322364Z",
     "start_time": "2019-10-18T15:18:22.309384Z"
    }
   },
   "source": [
    "np.savetxt('../data/w_rr.dat', w_rr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T18:30:07.069000Z",
     "start_time": "2019-10-18T18:30:07.055291Z"
    }
   },
   "outputs": [],
   "source": [
    "def ridge_regression_demo(x, y, degree, ratio, seed):\n",
    "    \"\"\"ridge regression demo.\"\"\"\n",
    "    # define parameter\n",
    "    lambdas = np.logspace(-5, 3, 50)\n",
    "    # split the data, and return train and test data\n",
    "    x_train, y_train, x_test, y_test = split_data(x, y, ratio, seed)\n",
    "    #Clean\n",
    "    if clean:\n",
    "        y_train, x_train = clean_data(y_train, x_train)\n",
    "        y_test, x_test = clean_data(y_test, x_test)\n",
    "    # form train and test data with offset column\n",
    "    x_train_std = standardize_features(x_train)[0]\n",
    "    x_test_std = standardize_features(x_test)[0]\n",
    "    tx_train=build_poly(x_train_std, degree)\n",
    "    tx_test=build_poly(x_test_std, degree)\n",
    "    rmse_tr = []\n",
    "    rmse_te = []\n",
    "    accuracies = []\n",
    "    for ind, lambda_ in enumerate(lambdas):\n",
    "        # calcualte weight through least square.\n",
    "        w_train, loss_train = ridge_regression(y_train, tx_train, lambda_)\n",
    "        rmse_tr.append(np.sqrt(2*loss_train))\n",
    "        rmse_te.append(np.sqrt(2*compute_loss(y_test, tx_test, w_train, kind = 'mse')))\n",
    "        accuracies.append(accuracy_ratio(predict_labels(w_train, tx_test), y_test))\n",
    "        print(\"proportion={p}, degree={d}, lambda={l:.3e}, Training RMSE={tr:.3f}, Testing RMSE={te:.3f}, Accuracy={ac:.3f}\".format(\n",
    "               p=ratio, d=degree, l=lambda_, tr=rmse_tr[ind], te=rmse_te[ind], ac=accuracies[ind]))\n",
    "        \n",
    "    # Plot the obtained results\n",
    "    plot_train_test(rmse_tr, rmse_te, lambdas, degree)\n",
    "    plt.figure()\n",
    "    plt.semilogx(lambdas,accuracies, marker='o')\n",
    "def plot_train_test(train_errors, test_errors, lambdas, degree):\n",
    "    \"\"\"\n",
    "    train_errors, test_errors and lambas should be list (of the same size) the respective train error and test error for a given lambda,\n",
    "    * lambda[0] = 1\n",
    "    * train_errors[0] = RMSE of a ridge regression on the train set\n",
    "    * test_errors[0] = RMSE of the parameter found by ridge regression applied on the test set\n",
    "    \n",
    "    degree is just used for the title of the plot.\n",
    "    \"\"\"\n",
    "    plt.semilogx(lambdas, train_errors, color='b', marker='*', label=\"Train error\")\n",
    "    plt.semilogx(lambdas, test_errors, color='r', marker='*', label=\"Test error\")\n",
    "    plt.xlabel(\"lambda\")\n",
    "    plt.ylabel(\"RMSE\")\n",
    "    plt.title(\"Ridge regression for polynomial degree \" + str(degree))\n",
    "    leg = plt.legend(loc=1, shadow=True)\n",
    "    leg.draw_frame(False)\n",
    "    plt.savefig(\"../results/ridge_regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T18:30:11.749743Z",
     "start_time": "2019-10-18T18:30:08.141908Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "seed = 42\n",
    "degree = 2\n",
    "split_ratio = 0.9\n",
    "ridge_regression_demo(x, y, degree, split_ratio, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T18:30:20.729742Z",
     "start_time": "2019-10-18T18:30:20.721124Z"
    }
   },
   "outputs": [],
   "source": [
    "def cross_validation(y, x, k_indices, k, lambda_, degree):\n",
    "    \"\"\"return the loss of ridge regression.\"\"\"\n",
    "    # get k'th subgroup in test, others in train\n",
    "    id_test = k_indices[k]\n",
    "    id_train = k_indices[~(np.arange(k_indices.shape[0]) == k)].reshape(-1)\n",
    "    x_test = x[id_test]\n",
    "    x_train = x[id_train]\n",
    "    y_test = y[id_test]\n",
    "    y_train = y[id_train]\n",
    "    #Clean\n",
    "    if clean:\n",
    "        y_train, x_train = clean_data(y_train, x_train)\n",
    "        y_test, x_test = clean_data(y_test, x_test)\n",
    "    # Standardize\n",
    "    x_train_std = standardize(x_train)[0]\n",
    "    x_test_std = standardize(x_test)[0]\n",
    "    # Define feature matrix\n",
    "    tx_train = build_poly(x_train_std, degree)\n",
    "    tx_test = build_poly(x_test_std, degree)\n",
    "    # ridge regression\n",
    "    weight, loss_tr = ridge_regression(y_train, tx_train, lambda_)\n",
    "    # calculate the loss for train and test data\n",
    "    loss_te = compute_loss(y_test, tx_test, weight)\n",
    "    accuracy = accuracy_ratio(predict_labels(weight, tx_test), y_test)\n",
    "\n",
    "    return loss_tr, loss_te, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T18:31:38.710840Z",
     "start_time": "2019-10-18T18:30:26.506760Z"
    }
   },
   "outputs": [],
   "source": [
    "def cross_validation_visualization(lambds, mse_tr, mse_te):\n",
    "    \"\"\"visualization the curves of mse_tr and mse_te.\"\"\"\n",
    "    best_l_err = lambds[np.argmin(mse_te)]\n",
    "    print('Best lambda from error: %.2e'%best_l_err)\n",
    "    plt.semilogx(lambds, mse_tr, marker=\".\", color='b', label='train error')\n",
    "    plt.semilogx(lambds, mse_te, marker=\".\", color='r', label='test error')\n",
    "    plt.axvline(best_l_err, c = 'g', label = '$\\lambda^*_{rmse}=%.1e$'%best_l_err, ls = ':')\n",
    "    plt.xlabel(\"lambda\")\n",
    "    plt.ylabel(\"rmse\")\n",
    "    plt.title(\"cross validation\")\n",
    "    plt.legend(loc=0)\n",
    "    plt.grid(True)\n",
    "    plt.savefig(\"../results/cross_validation\")\n",
    "def cross_validation_visualization_accuracy(lambdas, accuracies):\n",
    "    \"\"\"visualization the curves of mse_tr and mse_te.\"\"\"\n",
    "    plt.semilogx(lambdas, accuracies, lw =2, marker = '*', label = 'Accuracy ratio')\n",
    "    best_l_acc = lambdas[np.argmax(accuracies)]\n",
    "    plt.axvline(best_l_acc, c= 'k', label = '$\\lambda^*_{acc}=%.1e$'%best_l_acc, ls = ':')\n",
    "    print('Best lambda from accuracy: %.2e'%best_l_acc)\n",
    "    plt.xlabel(\"lambda\")\n",
    "    plt.ylabel(\"accuracy\")\n",
    "    plt.title(\"cross validation\")\n",
    "    plt.legend(loc=0)\n",
    "    plt.grid(True)\n",
    "    plt.savefig(\"../results/cross_validation_accuracies\")\n",
    "def cross_validation_demo():\n",
    "    seed = 42\n",
    "    degree = 2\n",
    "    k_fold = 4\n",
    "    lambdas = np.logspace(-7, 3, 30)\n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    # define lists to store the loss of training data and test data\n",
    "    rmse_tr = []\n",
    "    rmse_te = []\n",
    "    std_tr = []\n",
    "    std_te = []\n",
    "    accuracies = []\n",
    "    # cross validation\n",
    "    for ind, lambda_ in enumerate(lambdas):\n",
    "        x_validation = np.array([cross_validation(y, x, k_indices, k, lambda_, degree) for k in range(k_fold)])\n",
    "        rmse_tr.append(np.mean(np.sqrt(2 * x_validation[:, 0])))\n",
    "        rmse_te.append(np.mean(np.sqrt(2 * x_validation[:, 1])))\n",
    "        std_tr.append(np.std(np.sqrt(2 * x_validation[:, 0])))\n",
    "        std_te.append(np.std(np.sqrt(2 * x_validation[:, 1])))\n",
    "        accuracies.append(np.mean(x_validation[:,2]))\n",
    "    cross_validation_visualization_accuracy(lambdas, accuracies)\n",
    "    plt.figure()\n",
    "    cross_validation_visualization(lambdas, rmse_tr, rmse_te)\n",
    "\n",
    "cross_validation_demo()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Bias-Variance test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T12:53:17.453735Z",
     "start_time": "2019-10-17T12:50:27.446578Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def bias_variance_decomposition_visualization(degrees, rmse_tr, rmse_te):\n",
    "    \"\"\"visualize the bias variance decomposition.\"\"\"\n",
    "    rmse_tr_mean = np.expand_dims(np.mean(rmse_tr, axis=0), axis=0)\n",
    "    rmse_te_mean = np.expand_dims(np.mean(rmse_te, axis=0), axis=0)\n",
    "    print(rmse_te_mean, rmse_tr_mean)\n",
    "    plt.plot(degrees,\n",
    "             rmse_tr.T,\n",
    "             'b',\n",
    "             linestyle=\"-\",\n",
    "             color=([0.7, 0.7, 1]),\n",
    "             label='train',\n",
    "             linewidth=0.3)\n",
    "    plt.plot(degrees,\n",
    "             rmse_te.T,\n",
    "             'r',\n",
    "             linestyle=\"-\",\n",
    "             color=[1, 0.7, 0.7],\n",
    "             label='test',\n",
    "             linewidth=0.3)\n",
    "    plt.plot(degrees,\n",
    "             rmse_tr_mean.T,\n",
    "             'b',\n",
    "             linestyle=\"-\",\n",
    "             label='train',\n",
    "             linewidth=3)\n",
    "    plt.plot(degrees,\n",
    "             rmse_te_mean.T,\n",
    "             'r',\n",
    "             linestyle=\"-\",\n",
    "             label='test',\n",
    "             linewidth=3)\n",
    "    plt.ylim(0.7, 1)\n",
    "    plt.xlabel(\"degree\")\n",
    "    plt.ylabel(\"error\")\n",
    "    plt.title(\"Bias-Variance Decomposition\")\n",
    "    plt.savefig(\"bias_variance\")\n",
    "\n",
    "\n",
    "def bias_variance_demo():\n",
    "    \"\"\"The entry.\"\"\"\n",
    "    # define parameters\n",
    "    seeds = range(100)\n",
    "    ratio_train = 0.5\n",
    "    degrees = range(1, 8)\n",
    "    # define list to store the variable\n",
    "    rmse_tr = np.empty((len(seeds), len(degrees)))\n",
    "    rmse_te = np.empty((len(seeds), len(degrees)))\n",
    "    for index_seed, seed in enumerate(seeds):\n",
    "        np.random.seed(seed)\n",
    "        # split data with a specific seed\n",
    "        x_train, y_train, x_test, y_test = split_data(x, y, ratio_train, seed)\n",
    "        x_train_std = standardize(x_train)[0]\n",
    "        x_test_std = standardize(x_test)[0]\n",
    "        for index_degrees, degree in enumerate(degrees):\n",
    "            tx_train = build_poly(x_train_std, degree)\n",
    "            tx_test = build_poly(x_test_std, degree)\n",
    "            weight, loss_tr = ridge_regression(y_train, tx_train, 1.89e-05 )\n",
    "            loss_te = compute_loss(y_test, tx_test, weight, kind='mse')\n",
    "            rmse_tr[index_seed, index_degrees] = np.sqrt(2 * loss_tr)\n",
    "            rmse_te[index_seed, index_degrees] = np.sqrt(2 * loss_te)\n",
    "    bias_variance_decomposition_visualization(degrees, rmse_tr, rmse_te)\n",
    "\n",
    "\n",
    "bias_variance_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T12:54:53.029395Z",
     "start_time": "2019-10-17T12:54:47.011563Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(id_out_test.shape)\n",
    "x_out_test_std = standardize_features(x_out_test)\n",
    "x_out = x_out_test_std[0]\n",
    "tx_out = build_poly(x_out, 2)\n",
    "\n",
    "create_csv_submission(id_out_test, predict_labels(w_rr, tx_out) , '../results/rr_pred.csv')\n",
    "create_csv_submission(id_out_test, predict_labels(w_gd, tx_out) , '../results/gd_pred_accel.csv')\n",
    "create_csv_submission(id_out_test, predict_labels(w_lsq, tx_out) , '../results/lsq_pred.csv')\n",
    "#create_csv_submission(id_out_test, predict_labels(w_sgd, tx_out) , '../results/sgd_pred_noadapt.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T18:32:32.794589Z",
     "start_time": "2019-10-18T18:32:32.777297Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_labels_log(weights, data):\n",
    "    \"\"\"Generates class predictions given weights, and a test data matrix\"\"\"\n",
    "    y_pred = np.dot(data, weights)\n",
    "    y_pred[np.where(y_pred <= 0.5)] = -1\n",
    "    y_pred[np.where(y_pred > 0.5)] = 1\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T18:32:38.319778Z",
     "start_time": "2019-10-18T18:32:38.223130Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression GD (0/9999): loss=2426.015131959809\n",
      "Logistic Regression GD (100/9999): loss=1914.207882028191\n",
      "Logistic Regression GD (200/9999): loss=1832.5493808593055\n",
      "Logistic Regression GD (300/9999): loss=1801.8249539615178\n",
      "Logistic Regression GD (400/9999): loss=1785.8975039922557\n",
      "Logistic Regression GD (500/9999): loss=1775.7978788111134\n",
      "Logistic Regression GD (600/9999): loss=1768.5212754140496\n",
      "Logistic Regression GD (700/9999): loss=1762.8593618143577\n",
      "Logistic Regression GD (800/9999): loss=1758.2478417340633\n",
      "Logistic Regression GD (900/9999): loss=1754.3857658843751\n",
      "Logistic Regression GD (1000/9999): loss=1751.0929348527388\n",
      "Logistic Regression GD (1100/9999): loss=1748.2506957106327\n",
      "Logistic Regression GD (1200/9999): loss=1745.7750192001572\n",
      "Logistic Regression GD (1300/9999): loss=1743.6031661407433\n",
      "Logistic Regression GD (1400/9999): loss=1741.6865163951472\n",
      "Logistic Regression GD (1500/9999): loss=1739.9863950761448\n",
      "Logistic Regression GD (1600/9999): loss=1738.4714603781651\n",
      "Logistic Regression GD (1700/9999): loss=1737.1159627076077\n",
      "Logistic Regression GD (1800/9999): loss=1735.8985233498706\n",
      "Logistic Regression GD (1900/9999): loss=1734.8012425506324\n",
      "Logistic Regression GD (2000/9999): loss=1733.8090279723908\n",
      "Logistic Regression GD (2100/9999): loss=1732.9090772838897\n",
      "Logistic Regression GD (2200/9999): loss=1732.090472444489\n",
      "Logistic Regression GD (2300/9999): loss=1731.3438572024734\n",
      "Logistic Regression GD (2400/9999): loss=1730.6611779352272\n",
      "Logistic Regression GD (2500/9999): loss=1730.035473522025\n",
      "Logistic Regression GD (2600/9999): loss=1729.4607036837563\n",
      "Logistic Regression GD (2700/9999): loss=1728.931607830906\n",
      "Logistic Regression GD (2800/9999): loss=1728.443588328347\n",
      "Logistic Regression GD (2900/9999): loss=1727.9926134536963\n",
      "Logistic Regression GD (3000/9999): loss=1727.575136347228\n",
      "Logistic Regression GD (3100/9999): loss=1727.1880270251588\n",
      "Logistic Regression GD (3200/9999): loss=1726.8285151218925\n",
      "Logistic Regression GD (3300/9999): loss=1726.4941414871935\n",
      "Logistic Regression GD (3400/9999): loss=1726.1827171245545\n",
      "Logistic Regression GD (3500/9999): loss=1725.8922882411464\n",
      "Logistic Regression GD (3600/9999): loss=1725.6211064054485\n",
      "Logistic Regression GD (3700/9999): loss=1725.3676029890564\n",
      "Logistic Regression GD (3800/9999): loss=1725.1303672142164\n",
      "Logistic Regression GD (3900/9999): loss=1724.9081272458463\n",
      "Logistic Regression GD (4000/9999): loss=1724.69973386203\n",
      "Logistic Regression GD (4100/9999): loss=1724.5041463146367\n",
      "Logistic Regression GD (4200/9999): loss=1724.3204200553826\n",
      "Logistic Regression GD (4300/9999): loss=1724.1476960549855\n",
      "Logistic Regression GD (4400/9999): loss=1723.9851914863207\n",
      "Logistic Regression GD (4500/9999): loss=1723.832191578281\n",
      "Logistic Regression GD (4600/9999): loss=1723.6880424768456\n",
      "Logistic Regression GD (4700/9999): loss=1723.5521449746764\n",
      "Logistic Regression GD (4800/9999): loss=1723.4239489913457\n",
      "Logistic Regression GD (4900/9999): loss=1723.3029487037147\n",
      "Logistic Regression GD (5000/9999): loss=1723.1886782406352\n",
      "Logistic Regression GD (5100/9999): loss=1723.080707868506\n",
      "Logistic Regression GD (5200/9999): loss=1722.978640604656\n",
      "Logistic Regression GD (5300/9999): loss=1722.8821092043665\n",
      "Logistic Regression GD (5400/9999): loss=1722.7907734748692\n",
      "Logistic Regression GD (5500/9999): loss=1722.7043178760373\n",
      "Logistic Regression GD (5600/9999): loss=1722.622449372944\n",
      "Logistic Regression GD (5700/9999): loss=1722.5448955101117\n",
      "Logistic Regression GD (5800/9999): loss=1722.4714026812715\n",
      "Logistic Regression GD (5900/9999): loss=1722.4017345718578\n",
      "Logistic Regression GD (6000/9999): loss=1722.3356707544103\n",
      "Logistic Regression GD (6100/9999): loss=1722.2730054195797\n",
      "Logistic Regression GD (6200/9999): loss=1722.2135462276053\n",
      "Logistic Regression GD (6300/9999): loss=1722.1571132670288\n",
      "Logistic Regression GD (6400/9999): loss=1722.1035381090228\n",
      "Logistic Regression GD (6500/9999): loss=1722.0526629471315\n",
      "Logistic Regression GD (6600/9999): loss=1722.0043398134399\n",
      "Logistic Regression GD (6700/9999): loss=1721.9584298632558\n",
      "Logistic Regression GD (6800/9999): loss=1721.9148027213134\n",
      "Logistic Regression GD (6900/9999): loss=1721.8733358833133\n",
      "Logistic Regression GD (7000/9999): loss=1721.8339141673198\n",
      "Logistic Regression GD (7100/9999): loss=1721.7964292101572\n",
      "Logistic Regression GD (7200/9999): loss=1721.760779004479\n",
      "Logistic Regression GD (7300/9999): loss=1721.7268674726681\n",
      "Logistic Regression GD (7400/9999): loss=1721.6946040741282\n",
      "Logistic Regression GD (7500/9999): loss=1721.6639034429118\n",
      "Logistic Regression GD (7600/9999): loss=1721.634685052936\n",
      "Logistic Regression GD (7700/9999): loss=1721.60687290833\n",
      "Logistic Regression GD (7800/9999): loss=1721.580395256713\n",
      "Logistic Regression GD (7900/9999): loss=1721.5551843234143\n",
      "Logistic Regression GD (8000/9999): loss=1721.531176064859\n",
      "Logistic Regression GD (8100/9999): loss=1721.5083099395006\n",
      "Logistic Regression GD (8200/9999): loss=1721.4865286948589\n",
      "Logistic Regression GD (8300/9999): loss=1721.4657781693402\n",
      "Logistic Regression GD (8400/9999): loss=1721.4460071076528\n",
      "Logistic Regression GD (8500/9999): loss=1721.4271669887391\n",
      "Logistic Regression GD (8600/9999): loss=1721.4092118652475\n",
      "Logistic Regression GD (8700/9999): loss=1721.392098213644\n",
      "Logistic Regression GD (8800/9999): loss=1721.37578479416\n",
      "Logistic Regression GD (8900/9999): loss=1721.3602325198306\n",
      "Logistic Regression GD (9000/9999): loss=1721.34540433395\n",
      "Logistic Regression GD (9100/9999): loss=1721.331265095326\n",
      "Logistic Regression GD (9200/9999): loss=1721.317781470763\n",
      "Logistic Regression GD (9300/9999): loss=1721.304921834263\n",
      "Logistic Regression GD (9400/9999): loss=1721.2926561724619\n",
      "Logistic Regression GD (9500/9999): loss=1721.2809559958637\n",
      "Logistic Regression GD (9600/9999): loss=1721.2697942554773\n",
      "Logistic Regression GD (9700/9999): loss=1721.2591452644774\n",
      "Logistic Regression GD (9800/9999): loss=1721.2489846245549\n",
      "Logistic Regression GD (9900/9999): loss=1721.2392891566342\n",
      "[-2.83214912 -0.27330489  1.80318504 ... -1.43978763 -1.14972804\n",
      " -3.05363866]\n",
      "Accuracy ratio = 0.735\n",
      "Test loss = 714.195\n",
      "Train loss = 1721.230\n"
     ]
    }
   ],
   "source": [
    "y_train_log = np.copy(y_train)\n",
    "y_train_log[y_train == -1] = 0\n",
    "\n",
    "y_test_log = np.copy(y_test)\n",
    "y_test_log[y_test == -1] = 0\n",
    "\n",
    "w_init = np.array([0] * tx_train.shape[1])\n",
    "max_iter = 10000\n",
    "gamma = 1e-5\n",
    "w_lrgd, loss_lrgd = logistic_regression(y_train_log,\n",
    "                                        tx_train,\n",
    "                                        w_init,\n",
    "                                        max_iter,\n",
    "                                        gamma,\n",
    "                                        pr=True,\n",
    "                                        adapt_gamma=False,\n",
    "                                       accel=False)\n",
    "\n",
    "lrgd_prediction = predict_labels_log(w_lrgd, tx_test)\n",
    "print(tx_test.dot(w_lrgd))\n",
    "acc_lrgd = accuracy_ratio(lrgd_prediction, y_test)\n",
    "\n",
    "print('Accuracy ratio = %.3f' % acc_lrgd)\n",
    "print('Test loss = %.3f' % compute_loss_logistic(y_test_log, tx_test, w_lrgd))\n",
    "print('Train loss = %.3f' % loss_lrgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T18:33:13.633702Z",
     "start_time": "2019-10-18T18:32:41.270424Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Regularized Logistic Regression GD (0/9999): loss=2426.015131959809\n",
      " Regularized Logistic Regression GD (100/9999): loss=2408.133345823764\n",
      " Regularized Logistic Regression GD (200/9999): loss=2391.3361359385167\n",
      " Regularized Logistic Regression GD (300/9999): loss=2375.5385439897823\n",
      " Regularized Logistic Regression GD (400/9999): loss=2360.662989292634\n",
      " Regularized Logistic Regression GD (500/9999): loss=2346.638717916484\n",
      " Regularized Logistic Regression GD (600/9999): loss=2333.401251467855\n",
      " Regularized Logistic Regression GD (700/9999): loss=2320.8918505729716\n",
      " Regularized Logistic Regression GD (800/9999): loss=2309.057003223274\n",
      " Regularized Logistic Regression GD (900/9999): loss=2297.8479442930825\n",
      " Regularized Logistic Regression GD (1000/9999): loss=2287.2202096143624\n",
      " Regularized Logistic Regression GD (1100/9999): loss=2277.1332258574753\n",
      " Regularized Logistic Regression GD (1200/9999): loss=2267.54993596787\n",
      " Regularized Logistic Regression GD (1300/9999): loss=2258.4364589054653\n",
      " Regularized Logistic Regression GD (1400/9999): loss=2249.7617818040485\n",
      " Regularized Logistic Regression GD (1500/9999): loss=2241.497482312313\n",
      " Regularized Logistic Regression GD (1600/9999): loss=2233.6174787170703\n",
      " Regularized Logistic Regression GD (1700/9999): loss=2226.0978054223037\n",
      " Regularized Logistic Regression GD (1800/9999): loss=2218.9164114196305\n",
      " Regularized Logistic Regression GD (1900/9999): loss=2212.052979503473\n",
      " Regularized Logistic Regression GD (2000/9999): loss=2205.488764134064\n",
      " Regularized Logistic Regression GD (2100/9999): loss=2199.206446016813\n",
      " Regularized Logistic Regression GD (2200/9999): loss=2193.190001636319\n",
      " Regularized Logistic Regression GD (2300/9999): loss=2187.424586150008\n",
      " Regularized Logistic Regression GD (2400/9999): loss=2181.896428205426\n",
      " Regularized Logistic Regression GD (2500/9999): loss=2176.592735393881\n",
      " Regularized Logistic Regression GD (2600/9999): loss=2171.5016091901157\n",
      " Regularized Logistic Regression GD (2700/9999): loss=2166.611968352532\n",
      " Regularized Logistic Regression GD (2800/9999): loss=2161.9134798713835\n",
      " Regularized Logistic Regression GD (2900/9999): loss=2157.396496653727\n",
      " Regularized Logistic Regression GD (3000/9999): loss=2153.05200122466\n",
      " Regularized Logistic Regression GD (3100/9999): loss=2148.871554805158\n",
      " Regularized Logistic Regression GD (3200/9999): loss=2144.8472511986874\n",
      " Regularized Logistic Regression GD (3300/9999): loss=2140.9716749824784\n",
      " Regularized Logistic Regression GD (3400/9999): loss=2137.2378635557734\n",
      " Regularized Logistic Regression GD (3500/9999): loss=2133.639272647344\n",
      " Regularized Logistic Regression GD (3600/9999): loss=2130.169744928713\n",
      " Regularized Logistic Regression GD (3700/9999): loss=2126.823481418603\n",
      " Regularized Logistic Regression GD (3800/9999): loss=2123.5950153986687\n",
      " Regularized Logistic Regression GD (3900/9999): loss=2120.4791885910913\n",
      " Regularized Logistic Regression GD (4000/9999): loss=2117.4711293756404\n",
      " Regularized Logistic Regression GD (4100/9999): loss=2114.566232847729\n",
      " Regularized Logistic Regression GD (4200/9999): loss=2111.7601425401253\n",
      " Regularized Logistic Regression GD (4300/9999): loss=2109.0487336497704\n",
      " Regularized Logistic Regression GD (4400/9999): loss=2106.428097627772\n",
      " Regularized Logistic Regression GD (4500/9999): loss=2103.8945280054086\n",
      " Regularized Logistic Regression GD (4600/9999): loss=2101.4445073420793\n",
      " Regularized Logistic Regression GD (4700/9999): loss=2099.0746951928\n",
      " Regularized Logistic Regression GD (4800/9999): loss=2096.781917003201\n",
      " Regularized Logistic Regression GD (4900/9999): loss=2094.5631538492335\n",
      " Regularized Logistic Regression GD (5000/9999): loss=2092.4155329469954\n",
      " Regularized Logistic Regression GD (5100/9999): loss=2090.336318865464\n",
      " Regularized Logistic Regression GD (5200/9999): loss=2088.322905381453\n",
      " Regularized Logistic Regression GD (5300/9999): loss=2086.3728079219904\n",
      " Regularized Logistic Regression GD (5400/9999): loss=2084.4836565445557\n",
      " Regularized Logistic Regression GD (5500/9999): loss=2082.6531894103164\n",
      " Regularized Logistic Regression GD (5600/9999): loss=2080.8792467097096\n",
      " Regularized Logistic Regression GD (5700/9999): loss=2079.1597650035114\n",
      " Regularized Logistic Regression GD (5800/9999): loss=2077.4927719459065\n",
      " Regularized Logistic Regression GD (5900/9999): loss=2075.876381359156\n",
      " Regularized Logistic Regression GD (6000/9999): loss=2074.308788632182\n",
      " Regularized Logistic Regression GD (6100/9999): loss=2072.788266417878\n",
      " Regularized Logistic Regression GD (6200/9999): loss=2071.313160606178\n",
      " Regularized Logistic Regression GD (6300/9999): loss=2069.881886551942\n",
      " Regularized Logistic Regression GD (6400/9999): loss=2068.4929255385186\n",
      " Regularized Logistic Regression GD (6500/9999): loss=2067.144821459505\n",
      " Regularized Logistic Regression GD (6600/9999): loss=2065.8361777026976\n",
      " Regularized Logistic Regression GD (6700/9999): loss=2064.5656542216043\n",
      " Regularized Logistic Regression GD (6800/9999): loss=2063.331964781054\n",
      " Regularized Logistic Regression GD (6900/9999): loss=2062.1338743646274\n",
      " Regularized Logistic Regression GD (7000/9999): loss=2060.970196732551\n",
      " Regularized Logistic Regression GD (7100/9999): loss=2059.839792119697\n",
      " Regularized Logistic Regression GD (7200/9999): loss=2058.741565064087\n",
      " Regularized Logistic Regression GD (7300/9999): loss=2057.674462357116\n",
      " Regularized Logistic Regression GD (7400/9999): loss=2056.6374711073636\n",
      " Regularized Logistic Regression GD (7500/9999): loss=2055.62961691051\n",
      " Regularized Logistic Regression GD (7600/9999): loss=2054.6499621184325\n",
      " Regularized Logistic Regression GD (7700/9999): loss=2053.697604201114\n",
      " Regularized Logistic Regression GD (7800/9999): loss=2052.7716741954223\n",
      " Regularized Logistic Regression GD (7900/9999): loss=2051.871335235316\n",
      " Regularized Logistic Regression GD (8000/9999): loss=2050.9957811583986\n",
      " Regularized Logistic Regression GD (8100/9999): loss=2050.144235184115\n",
      " Regularized Logistic Regression GD (8200/9999): loss=2049.3159486592363\n",
      " Regularized Logistic Regression GD (8300/9999): loss=2048.510199866582\n",
      " Regularized Logistic Regression GD (8400/9999): loss=2047.7262928932028\n",
      " Regularized Logistic Regression GD (8500/9999): loss=2046.9635565545368\n",
      " Regularized Logistic Regression GD (8600/9999): loss=2046.221343371259\n",
      " Regularized Logistic Regression GD (8700/9999): loss=2045.499028595796\n",
      " Regularized Logistic Regression GD (8800/9999): loss=2044.7960092856683\n",
      " Regularized Logistic Regression GD (8900/9999): loss=2044.1117034210104\n",
      " Regularized Logistic Regression GD (9000/9999): loss=2043.4455490638027\n",
      " Regularized Logistic Regression GD (9100/9999): loss=2042.797003556502\n",
      " Regularized Logistic Regression GD (9200/9999): loss=2042.165542757914\n",
      " Regularized Logistic Regression GD (9300/9999): loss=2041.5506603142787\n",
      " Regularized Logistic Regression GD (9400/9999): loss=2040.951866963686\n",
      " Regularized Logistic Regression GD (9500/9999): loss=2040.3686898720305\n",
      " Regularized Logistic Regression GD (9600/9999): loss=2039.8006719988605\n",
      " Regularized Logistic Regression GD (9700/9999): loss=2039.2473714915354\n",
      " Regularized Logistic Regression GD (9800/9999): loss=2038.7083611062403\n",
      " Regularized Logistic Regression GD (9900/9999): loss=2038.1832276544776\n",
      "Accuracy ratio = 0.715\n",
      "Test loss = 831.530\n",
      "Train loss = 2037.677\n"
     ]
    }
   ],
   "source": [
    "lambda_ = 500\n",
    "gamma = 1e-6\n",
    "w_rlrgd, loss_rlrgd = reg_logistic_regression(y_train_log,\n",
    "                                              tx_train,\n",
    "                                              lambda_,\n",
    "                                              w_init,\n",
    "                                              max_iter,\n",
    "                                              gamma,\n",
    "                                              pr=True,\n",
    "                                              adapt_gamma=False, \n",
    "                                              accel=False)\n",
    "rlrgd_prediction = predict_labels_log(w_rlrgd, tx_test)\n",
    "acc_rlrgd = accuracy_ratio(rlrgd_prediction, y_test)\n",
    "print('Accuracy ratio = %.3f' % acc_rlrgd)\n",
    "print('Test loss = %.3f' % compute_loss_logistic(y_test_log, tx_test, w_rlrgd))\n",
    "print('Train loss = %.3f' % loss_rlrgd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T17:51:14.763658Z",
     "start_time": "2019-10-18T17:51:14.741402Z"
    }
   },
   "source": [
    "def cross_validation(y, x, k_indices, k, lambda_, degree):\n",
    "    \"\"\"return the loss of ridge regression.\"\"\"\n",
    "    gamma = 1e-8\n",
    "    # get k'th subgroup in test, others in train\n",
    "    id_test = k_indices[k]\n",
    "    id_train = k_indices[~(np.arange(k_indices.shape[0]) == k)].reshape(-1)\n",
    "    x_test = x[id_test]\n",
    "    x_train = x[id_train]\n",
    "    y_test = y[id_test]\n",
    "    y_train = y[id_train]\n",
    "    #Clean\n",
    "    if clean:\n",
    "        y_train, x_train = clean_data(y_train, x_train)\n",
    "        y_test, x_test = clean_data(y_test, x_test)\n",
    "    # Standardize\n",
    "    x_train_std = standardize(x_train)[0]\n",
    "    x_test_std = standardize(x_test)[0]\n",
    "    # Define feature matrix\n",
    "    tx_train = build_poly(x_train_std, degree)\n",
    "    tx_test = build_poly(x_test_std, degree)\n",
    "    y_train_log = np.copy(y_train)\n",
    "    y_train_log[y_train == -1] = 0\n",
    "\n",
    "    y_test_log = np.copy(y_test)\n",
    "    y_test_log[y_test == -1] = 0\n",
    "    # logistic regression\n",
    "    weight, loss_tr = reg_logistic_regression(y_train_log,\n",
    "                                              tx_train,\n",
    "                                              lambda_,\n",
    "                                              w_init,\n",
    "                                              max_iter,\n",
    "                                              gamma,\n",
    "                                              pr=True,\n",
    "                                              adapt_gamma=False)\n",
    "    # calculate the loss for train and test data\n",
    "    loss_te = compute_loss_logistic(y_test, tx_test, weight)\n",
    "    accuracy = accuracy_ratio(predict_labels(weight, tx_test), y_test)\n",
    "\n",
    "    return loss_tr, loss_te, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T17:57:34.213341Z",
     "start_time": "2019-10-18T17:51:15.297351Z"
    }
   },
   "outputs": [],
   "source": [
    "cross_validation_demo()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression NEW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression GD (0/9999): loss=4852.030263919618\n",
      "Logistic Regression GD (100/9999): loss=3664.563818774271\n",
      "Logistic Regression GD (200/9999): loss=3571.647714808384\n",
      "Logistic Regression GD (300/9999): loss=3536.9764960579855\n",
      "Logistic Regression GD (400/9999): loss=3516.4534168234986\n",
      "Logistic Regression GD (500/9999): loss=3502.153903323066\n",
      "Logistic Regression GD (600/9999): loss=3491.524283641052\n",
      "Logistic Regression GD (700/9999): loss=3483.351763955165\n",
      "Logistic Regression GD (800/9999): loss=3476.925139340024\n",
      "Logistic Regression GD (900/9999): loss=3471.7820538336728\n",
      "Logistic Regression GD (1000/9999): loss=3467.605320631438\n",
      "Logistic Regression GD (1100/9999): loss=3464.1700510667542\n",
      "Logistic Regression GD (1200/9999): loss=3461.312973614231\n",
      "Logistic Regression GD (1300/9999): loss=3458.913273520771\n",
      "Logistic Regression GD (1400/9999): loss=3456.8800804434823\n",
      "Logistic Regression GD (1500/9999): loss=3455.144044624306\n",
      "Logistic Regression GD (1600/9999): loss=3453.6515334475093\n",
      "Logistic Regression GD (1700/9999): loss=3452.360557685204\n",
      "Logistic Regression GD (1800/9999): loss=3451.237865930999\n",
      "Logistic Regression GD (1900/9999): loss=3450.256842985074\n",
      "Logistic Regression GD (2000/9999): loss=3449.395970473458\n",
      "Logistic Regression GD (2100/9999): loss=3448.6376862169236\n",
      "Logistic Regression GD (2200/9999): loss=3447.967529960983\n",
      "Logistic Regression GD (2300/9999): loss=3447.373497095362\n",
      "Logistic Regression GD (2400/9999): loss=3446.8455450165147\n",
      "Logistic Regression GD (2500/9999): loss=3446.3752126005447\n",
      "Logistic Regression GD (2600/9999): loss=3445.955324256528\n",
      "Logistic Regression GD (2700/9999): loss=3445.5797577746466\n",
      "Logistic Regression GD (2800/9999): loss=3445.243260692062\n",
      "Logistic Regression GD (2900/9999): loss=3444.9413038551547\n",
      "Logistic Regression GD (3000/9999): loss=3444.669963722264\n",
      "Logistic Regression GD (3100/9999): loss=3444.425827043848\n",
      "Logistic Regression GD (3200/9999): loss=3444.2059130969674\n",
      "Logistic Regression GD (3300/9999): loss=3444.0076097923816\n",
      "Logistic Regression GD (3400/9999): loss=3443.8286208243453\n",
      "Logistic Regression GD (3500/9999): loss=3443.6669216729924\n",
      "Logistic Regression GD (3600/9999): loss=3443.5207227528954\n",
      "Logistic Regression GD (3700/9999): loss=3443.3884383693917\n",
      "Logistic Regression GD (3800/9999): loss=3443.2686604258733\n",
      "Logistic Regression GD (3900/9999): loss=3443.160136042206\n",
      "Logistic Regression GD (4000/9999): loss=3443.0617484124714\n",
      "Logistic Regression GD (4100/9999): loss=3442.972500361245\n",
      "Logistic Regression GD (4200/9999): loss=3442.8915001603605\n",
      "Logistic Regression GD (4300/9999): loss=3442.8179492491945\n",
      "Logistic Regression GD (4400/9999): loss=3442.7511315657625\n",
      "Logistic Regression GD (4500/9999): loss=3442.6904042473698\n",
      "Logistic Regression GD (4600/9999): loss=3442.6351895007456\n",
      "Logistic Regression GD (4700/9999): loss=3442.5849674749065\n",
      "Logistic Regression GD (4800/9999): loss=3442.5392699971026\n",
      "Logistic Regression GD (4900/9999): loss=3442.497675054272\n",
      "Logistic Regression GD (5000/9999): loss=3442.459801920581\n",
      "Logistic Regression GD (5100/9999): loss=3442.425306846647\n",
      "Logistic Regression GD (5200/9999): loss=3442.393879238473\n",
      "Logistic Regression GD (5300/9999): loss=3442.365238264455\n",
      "Logistic Regression GD (5400/9999): loss=3442.3391298375786\n",
      "Logistic Regression GD (5500/9999): loss=3442.315323927166\n",
      "Logistic Regression GD (5600/9999): loss=3442.2936121607604\n",
      "Logistic Regression GD (5700/9999): loss=3442.2738056819353\n",
      "Logistic Regression GD (5800/9999): loss=3442.2557332343113\n",
      "Logistic Regression GD (5900/9999): loss=3442.2392394458398\n",
      "Logistic Regression GD (6000/9999): loss=3442.224183290717\n",
      "Logistic Regression GD (6100/9999): loss=3442.210436709086\n",
      "Logistic Regression GD (6200/9999): loss=3442.197883367085\n",
      "Logistic Regression GD (6300/9999): loss=3442.1864175419514\n",
      "Logistic Regression GD (6400/9999): loss=3442.175943118679\n",
      "Logistic Regression GD (6500/9999): loss=3442.1663726862735\n",
      "Logistic Regression GD (6600/9999): loss=3442.15762672312\n",
      "Logistic Regression GD (6700/9999): loss=3442.14963286209\n",
      "Logistic Regression GD (6800/9999): loss=3442.1423252271225\n",
      "Logistic Regression GD (6900/9999): loss=3442.1356438339085\n",
      "Logistic Regression GD (7000/9999): loss=3442.1295340481593\n",
      "Logistic Regression GD (7100/9999): loss=3442.123946095604\n",
      "Logistic Regression GD (7200/9999): loss=3442.1188346185345\n",
      "Logistic Regression GD (7300/9999): loss=3442.1141582742457\n",
      "Logistic Regression GD (7400/9999): loss=3442.1098793712367\n",
      "Logistic Regression GD (7500/9999): loss=3442.1059635394313\n",
      "Logistic Regression GD (7600/9999): loss=3442.102379431127\n",
      "Logistic Regression GD (7700/9999): loss=3442.0990984496693\n",
      "Logistic Regression GD (7800/9999): loss=3442.096094503176\n",
      "Logistic Regression GD (7900/9999): loss=3442.093343780915\n",
      "Logistic Regression GD (8000/9999): loss=3442.0908245501605\n",
      "Logistic Regression GD (8100/9999): loss=3442.0885169716075\n",
      "Logistic Regression GD (8200/9999): loss=3442.0864029315644\n",
      "Logistic Regression GD (8300/9999): loss=3442.084465889364\n",
      "Logistic Regression GD (8400/9999): loss=3442.0826907385654\n",
      "Logistic Regression GD (8500/9999): loss=3442.081063680662\n",
      "Logistic Regression GD (8600/9999): loss=3442.0795721101263\n",
      "Logistic Regression GD (8700/9999): loss=3442.0782045097753\n",
      "Logistic Regression GD (8800/9999): loss=3442.0769503554543\n",
      "Logistic Regression GD (8900/9999): loss=3442.075800029247\n",
      "Logistic Regression GD (9000/9999): loss=3442.074744740379\n",
      "Logistic Regression GD (9100/9999): loss=3442.073776453157\n",
      "Logistic Regression GD (9200/9999): loss=3442.0728878212662\n",
      "Logistic Regression GD (9300/9999): loss=3442.0720721278904\n",
      "Logistic Regression GD (9400/9999): loss=3442.0713232310936\n",
      "Logistic Regression GD (9500/9999): loss=3442.0706355140237\n",
      "Logistic Regression GD (9600/9999): loss=3442.0700038394743\n",
      "Logistic Regression GD (9700/9999): loss=3442.0694235084447\n",
      "Logistic Regression GD (9800/9999): loss=3442.068890222307\n",
      "Logistic Regression GD (9900/9999): loss=3442.0684000482993\n",
      "Accuracy ratio = 0.762\n",
      "Test loss = 1426.924\n",
      "Train loss = 3442.068\n",
      "[-0.99884535 -0.08692351 -0.62048512 -1.15394778  0.03036878  0.21260695\n",
      "  0.04074648 -0.03643553  1.07960171 -0.05666659  0.27515725 -0.59567044\n",
      "  0.32096549  0.22169643  0.64416003 -0.01853318  0.00630399  0.94414138\n",
      "  0.06667784  0.06606847  0.24607475 -0.06326445 -0.27745569  0.08431296\n",
      " -0.09013968 -0.09429453 -0.01668369 -0.1948242  -0.00757388  0.00818886\n",
      " -0.00464285]\n"
     ]
    }
   ],
   "source": [
    "w_init = np.zeros(tx_train.shape[1])\n",
    "max_iter = 10000\n",
    "gamma = 1e-5\n",
    "w_lrgd, loss_lrgd = logistic_regression(y_train,\n",
    "                                        tx_train,\n",
    "                                        w_init,\n",
    "                                        max_iter,\n",
    "                                        gamma,\n",
    "                                        pr=True,\n",
    "                                        adapt_gamma=False,\n",
    "                                       accel=False,\n",
    "                                       new=True)\n",
    "\n",
    "lrgd_prediction = predict_labels(w_lrgd, tx_test)\n",
    "\n",
    "acc_lrgd = accuracy_ratio(lrgd_prediction, y_test)\n",
    "\n",
    "print('Accuracy ratio = %.3f' % acc_lrgd)\n",
    "print('Test loss = %.3f' % compute_loss_logistic_new(y_test, tx_test, w_lrgd))\n",
    "print('Train loss = %.3f' % loss_lrgd)\n",
    "print((w_lrgd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Regularized Logistic Regression GD (0/9999): loss=4852.030263919618\n",
      " Regularized Logistic Regression GD (100/9999): loss=3859.2908289969505\n",
      " Regularized Logistic Regression GD (200/9999): loss=3719.222063663206\n",
      " Regularized Logistic Regression GD (300/9999): loss=3672.6377450042305\n",
      " Regularized Logistic Regression GD (400/9999): loss=3651.231481922037\n",
      " Regularized Logistic Regression GD (500/9999): loss=3639.1260262798633\n",
      " Regularized Logistic Regression GD (600/9999): loss=3631.3165910994035\n",
      " Regularized Logistic Regression GD (700/9999): loss=3625.8706856435347\n",
      " Regularized Logistic Regression GD (800/9999): loss=3621.8983962573293\n",
      " Regularized Logistic Regression GD (900/9999): loss=3618.9227333069803\n",
      " Regularized Logistic Regression GD (1000/9999): loss=3616.6558960961597\n",
      " Regularized Logistic Regression GD (1100/9999): loss=3614.9091287201295\n",
      " Regularized Logistic Regression GD (1200/9999): loss=3613.551634247608\n",
      " Regularized Logistic Regression GD (1300/9999): loss=3612.489504866049\n",
      " Regularized Logistic Regression GD (1400/9999): loss=3611.653731953081\n",
      " Regularized Logistic Regression GD (1500/9999): loss=3610.992790597736\n",
      " Regularized Logistic Regression GD (1600/9999): loss=3610.4677628442996\n",
      " Regularized Logistic Regression GD (1700/9999): loss=3610.0489893878694\n",
      " Regularized Logistic Regression GD (1800/9999): loss=3609.7137013133743\n",
      " Regularized Logistic Regression GD (1900/9999): loss=3609.44430980777\n",
      " Regularized Logistic Regression GD (2000/9999): loss=3609.2271522900237\n",
      " Regularized Logistic Regression GD (2100/9999): loss=3609.0515625971775\n",
      " Regularized Logistic Regression GD (2200/9999): loss=3608.9091751834426\n",
      " Regularized Logistic Regression GD (2300/9999): loss=3608.7934004880763\n",
      " Regularized Logistic Regression GD (2400/9999): loss=3608.6990267753927\n",
      " Regularized Logistic Regression GD (2500/9999): loss=3608.621916199937\n",
      " Regularized Logistic Regression GD (2600/9999): loss=3608.558771569669\n",
      " Regularized Logistic Regression GD (2700/9999): loss=3608.506956484702\n",
      " Regularized Logistic Regression GD (2800/9999): loss=3608.464355999243\n",
      " Regularized Logistic Regression GD (2900/9999): loss=3608.429268207257\n",
      " Regularized Logistic Regression GD (3000/9999): loss=3608.400319539371\n",
      " Regularized Logistic Regression GD (3100/9999): loss=3608.37639832282\n",
      " Regularized Logistic Regression GD (3200/9999): loss=3608.356602468422\n",
      " Regularized Logistic Regression GD (3300/9999): loss=3608.340198130129\n",
      " Regularized Logistic Regression GD (3400/9999): loss=3608.326586920779\n",
      " Regularized Logistic Regression GD (3500/9999): loss=3608.3152798251526\n",
      " Regularized Logistic Regression GD (3600/9999): loss=3608.3058763747003\n",
      " Regularized Logistic Regression GD (3700/9999): loss=3608.2980479707558\n",
      " Regularized Logistic Regression GD (3800/9999): loss=3608.291524489838\n",
      " Regularized Logistic Regression GD (3900/9999): loss=3608.286083494328\n",
      " Regularized Logistic Regression GD (4000/9999): loss=3608.281541517946\n",
      " Regularized Logistic Regression GD (4100/9999): loss=3608.2777470086994\n",
      " Regularized Logistic Regression GD (4200/9999): loss=3608.274574599883\n",
      " Regularized Logistic Regression GD (4300/9999): loss=3608.2719204482473\n",
      " Regularized Logistic Regression GD (4400/9999): loss=3608.26969843209\n",
      " Regularized Logistic Regression GD (4500/9999): loss=3608.2678370440985\n",
      " Regularized Logistic Regression GD (4600/9999): loss=3608.266276846939\n",
      " Regularized Logistic Regression GD (4700/9999): loss=3608.2649683857426\n",
      " Regularized Logistic Regression GD (4800/9999): loss=3608.2638704724172\n",
      " Regularized Logistic Regression GD (4900/9999): loss=3608.262948773232\n",
      " Regularized Logistic Regression GD (5000/9999): loss=3608.262174644213\n",
      " Regularized Logistic Regression GD (5100/9999): loss=3608.2615241695066\n",
      " Regularized Logistic Regression GD (5200/9999): loss=3608.2609773662025\n",
      " Regularized Logistic Regression GD (5300/9999): loss=3608.2605175259837\n",
      " Regularized Logistic Regression GD (5400/9999): loss=3608.2601306694023\n",
      " Regularized Logistic Regression GD (5500/9999): loss=3608.2598050929664\n",
      " Regularized Logistic Regression GD (5600/9999): loss=3608.259530992904\n",
      " Regularized Logistic Regression GD (5700/9999): loss=3608.2593001522623\n",
      " Regularized Logistic Regression GD (5800/9999): loss=3608.2591056804854\n",
      " Regularized Logistic Regression GD (5900/9999): loss=3608.2589417964346\n",
      " Regularized Logistic Regression GD (6000/9999): loss=3608.25880364748\n",
      " Regularized Logistic Regression GD (6100/9999): loss=3608.2586871585536\n",
      " Regularized Logistic Regression GD (6200/9999): loss=3608.258588906086\n",
      " Regularized Logistic Regression GD (6300/9999): loss=3608.258506012675\n",
      " Regularized Logistic Regression GD (6400/9999): loss=3608.258436059017\n",
      " Regularized Logistic Regression GD (6500/9999): loss=3608.2583770102074\n",
      " Regularized Logistic Regression GD (6600/9999): loss=3608.2583271540475\n",
      " Regularized Logistic Regression GD (6700/9999): loss=3608.2582850493814\n",
      " Regularized Logistic Regression GD (6800/9999): loss=3608.258249482777\n",
      " Regularized Logistic Regression GD (6900/9999): loss=3608.2582194322254\n",
      " Regularized Logistic Regression GD (7000/9999): loss=3608.2581940366667\n",
      " Regularized Logistic Regression GD (7100/9999): loss=3608.258172570437\n",
      " Regularized Logistic Regression GD (7200/9999): loss=3608.258154421802\n",
      " Regularized Logistic Regression GD (7300/9999): loss=3608.2581390749197\n",
      " Regularized Logistic Regression GD (7400/9999): loss=3608.2581260946995\n",
      " Regularized Logistic Regression GD (7500/9999): loss=3608.2581151140594\n",
      " Regularized Logistic Regression GD (7600/9999): loss=3608.2581058232213\n",
      " Regularized Logistic Regression GD (7700/9999): loss=3608.2580979606987\n",
      " Regularized Logistic Regression GD (7800/9999): loss=3608.25809130572\n",
      " Regularized Logistic Regression GD (7900/9999): loss=3608.2580856718378\n",
      " Regularized Logistic Regression GD (8000/9999): loss=3608.2580809015626\n",
      " Regularized Logistic Regression GD (8100/9999): loss=3608.258076861837\n",
      " Regularized Logistic Regression GD (8200/9999): loss=3608.258073440218\n",
      " Regularized Logistic Regression GD (8300/9999): loss=3608.2580705416663\n",
      " Regularized Logistic Regression GD (8400/9999): loss=3608.2580680858305\n",
      " Regularized Logistic Regression GD (8500/9999): loss=3608.25806600477\n",
      " Regularized Logistic Regression GD (8600/9999): loss=3608.258064241025\n",
      " Regularized Logistic Regression GD (8700/9999): loss=3608.2580627459897\n",
      " Regularized Logistic Regression GD (8800/9999): loss=3608.258061478541\n",
      "Accuracy ratio = 0.761\n",
      "Test loss = 1461.346\n",
      "Train loss = 3608.258\n",
      "[-0.83714664 -0.13285127 -0.53978596 -0.39225452  0.06448877  0.17309301\n",
      "  0.05053845 -0.05583889  0.56134465 -0.06663147  0.14109101 -0.25924928\n",
      "  0.30227349  0.20073664  0.48412078 -0.01728894 -0.01148385  0.37694295\n",
      "  0.05485534  0.05520179  0.13613583 -0.04908318 -0.1287674   0.06525394\n",
      " -0.03897427 -0.08468006 -0.02373987 -0.14431697 -0.00121565  0.00492089\n",
      " -0.00991743]\n"
     ]
    }
   ],
   "source": [
    "lambda_ = 100\n",
    "gamma = 5e-6\n",
    "w_rlrgd, loss_rlrgd = reg_logistic_regression(y_train,\n",
    "                                              tx_train,\n",
    "                                              lambda_,\n",
    "                                              w_init,\n",
    "                                              max_iter,\n",
    "                                              gamma,\n",
    "                                              pr=True,\n",
    "                                              adapt_gamma=False, \n",
    "                                              accel=False,\n",
    "                                              new = True)\n",
    "rlrgd_prediction = predict_labels(w_rlrgd, tx_test)\n",
    "acc_rlrgd = accuracy_ratio(rlrgd_prediction, y_test)\n",
    "print('Accuracy ratio = %.3f' % acc_rlrgd)\n",
    "print('Test loss = %.3f' % compute_loss_logistic_new(y_test, tx_test, w_rlrgd))\n",
    "print('Train loss = %.3f' % loss_rlrgd)\n",
    "print((w_rlrgd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
